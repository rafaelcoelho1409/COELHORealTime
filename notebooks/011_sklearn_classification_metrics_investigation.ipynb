{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "list_all_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get all items from sklearn.metrics\n",
    "all_items = [name for name in dir(metrics) if not name.startswith('_')]\n",
    "print(\"All items in metrics module:\")\n",
    "print(all_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "categorize_items",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize items by type\n",
    "functions = []\n",
    "classes = []\n",
    "submodules = []\n",
    "\n",
    "for name in all_items:\n",
    "    obj = getattr(metrics, name)\n",
    "    if inspect.isfunction(obj):\n",
    "        functions.append(name)\n",
    "    elif inspect.isclass(obj):\n",
    "        classes.append(name)\n",
    "    elif inspect.ismodule(obj):\n",
    "        submodules.append(name)\n",
    "\n",
    "print(f\"Functions: {len(functions)}\")\n",
    "print(functions)\n",
    "print(f\"\\nClasses: {len(classes)}\")\n",
    "print(classes)\n",
    "print(f\"\\nSubmodules: {len(submodules)}\")\n",
    "print(submodules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classification_metrics_list",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Filter classification metrics suitable for TFD (binary classification)\n",
    "# Exclude regression metrics, clustering metrics, pairwise distances, etc.\n",
    "\n",
    "regression_metrics = {\n",
    "    'mean_absolute_error', 'mean_squared_error', 'mean_squared_log_error',\n",
    "    'median_absolute_error', 'r2_score', 'explained_variance_score',\n",
    "    'max_error', 'mean_absolute_percentage_error', 'mean_tweedie_deviance',\n",
    "    'mean_poisson_deviance', 'mean_gamma_deviance', 'mean_pinball_loss',\n",
    "    'd2_absolute_error_score', 'd2_pinball_score', 'd2_tweedie_score',\n",
    "    'root_mean_squared_error', 'root_mean_squared_log_error'\n",
    "}\n",
    "\n",
    "clustering_metrics = {\n",
    "    'adjusted_mutual_info_score', 'adjusted_rand_score', 'calinski_harabasz_score',\n",
    "    'completeness_score', 'davies_bouldin_score', 'fowlkes_mallows_score',\n",
    "    'homogeneity_completeness_v_measure', 'homogeneity_score', 'mutual_info_score',\n",
    "    'normalized_mutual_info_score', 'rand_score', 'silhouette_samples',\n",
    "    'silhouette_score', 'v_measure_score', 'consensus_score'\n",
    "}\n",
    "\n",
    "ranking_metrics = {\n",
    "    'coverage_error', 'dcg_score', 'label_ranking_average_precision_score',\n",
    "    'label_ranking_loss', 'ndcg_score'\n",
    "}\n",
    "\n",
    "pairwise_utils = {\n",
    "    'euclidean_distances', 'nan_euclidean_distances', 'pairwise_distances',\n",
    "    'pairwise_distances_argmin', 'pairwise_distances_argmin_min',\n",
    "    'pairwise_distances_chunked', 'pairwise_kernels', 'auc'\n",
    "}\n",
    "\n",
    "utility_functions = {\n",
    "    'check_scoring', 'get_scorer', 'get_scorer_names', 'make_scorer'\n",
    "}\n",
    "\n",
    "# Classification metrics functions\n",
    "classification_functions = [\n",
    "    name for name in functions\n",
    "    if name not in regression_metrics\n",
    "    and name not in clustering_metrics\n",
    "    and name not in ranking_metrics\n",
    "    and name not in pairwise_utils\n",
    "    and name not in utility_functions\n",
    "]\n",
    "\n",
    "print(f\"Classification metrics functions: {len(classification_functions)}\")\n",
    "print(classification_functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tfd_recommendations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPREHENSIVE METRICS CONFIGURATION FOR TRANSACTION FRAUD DETECTION (TFD)\n",
    "# =============================================================================\n",
    "#\n",
    "# TFD Characteristics:\n",
    "# - Binary classification (fraud=1, non-fraud=0)\n",
    "# - Highly imbalanced (~1-5% fraud rate)\n",
    "# - High cost of False Negatives (missed fraud = financial loss)\n",
    "# - Cost of False Positives (blocked legitimate = customer friction)\n",
    "#\n",
    "# Key Differences from River (Online ML):\n",
    "# - sklearn metrics are BATCH metrics (computed on entire dataset at once)\n",
    "# - sklearn metrics are FUNCTIONS, not classes (no .update() method)\n",
    "# - sklearn requires y_true and y_pred/y_score as input arrays\n",
    "# - Some metrics need predicted labels, others need probability scores\n",
    "#\n",
    "# =============================================================================\n",
    "# FINAL RECOMMENDED CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PRIMARY METRICS - Most important for fraud detection\n",
    "# These should be the main focus for model selection and evaluation\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "primary_metric_functions = {\n",
    "    # ROC-AUC: Best overall threshold-independent metric for imbalanced binary\n",
    "    # Uses probability scores, not predicted labels\n",
    "    \"roc_auc_score\": metrics.roc_auc_score,\n",
    "    \n",
    "    # Average Precision (PR-AUC): Area under precision-recall curve\n",
    "    # Better than ROC-AUC for highly imbalanced data\n",
    "    \"average_precision_score\": metrics.average_precision_score,\n",
    "    \n",
    "    # Recall: Fraud detection rate (minimize missed fraud)\n",
    "    # TP / (TP + FN) - How many actual frauds did we catch?\n",
    "    \"recall_score\": metrics.recall_score,\n",
    "    \n",
    "    # Precision: False alarm rate (customer experience)\n",
    "    # TP / (TP + FP) - Of predicted frauds, how many were actually fraud?\n",
    "    \"precision_score\": metrics.precision_score,\n",
    "    \n",
    "    # F1: Harmonic mean of Precision & Recall\n",
    "    # Best when you want balance between precision and recall\n",
    "    \"f1_score\": metrics.f1_score,\n",
    "    \n",
    "    # F-beta with beta=2: Weights Recall 2x more than Precision\n",
    "    # CRITICAL for fraud detection where missing fraud is costly\n",
    "    \"fbeta_score\": metrics.fbeta_score,\n",
    "}\n",
    "\n",
    "primary_metric_args = {\n",
    "    # ROC-AUC: No special args needed for binary classification\n",
    "    # Expects y_score (probabilities), not y_pred (labels)\n",
    "    \"roc_auc_score\": {},\n",
    "    \n",
    "    # Average Precision: pos_label=1 means fraud is positive class\n",
    "    # Expects y_score (probabilities)\n",
    "    \"average_precision_score\": {\n",
    "        \"pos_label\": 1,\n",
    "    },\n",
    "    \n",
    "    # Recall: binary classification with fraud as positive class\n",
    "    # Expects y_pred (labels)\n",
    "    \"recall_score\": {\n",
    "        \"pos_label\": 1,\n",
    "        \"average\": \"binary\",\n",
    "        \"zero_division\": 0.0,\n",
    "    },\n",
    "    \n",
    "    # Precision: binary classification with fraud as positive class\n",
    "    # Expects y_pred (labels)\n",
    "    \"precision_score\": {\n",
    "        \"pos_label\": 1,\n",
    "        \"average\": \"binary\",\n",
    "        \"zero_division\": 0.0,\n",
    "    },\n",
    "    \n",
    "    # F1: binary classification with fraud as positive class\n",
    "    # Expects y_pred (labels)\n",
    "    \"f1_score\": {\n",
    "        \"pos_label\": 1,\n",
    "        \"average\": \"binary\",\n",
    "        \"zero_division\": 0.0,\n",
    "    },\n",
    "    \n",
    "    # F-beta: beta=2 weights Recall 2x more than Precision\n",
    "    # For fraud: missing fraud (FN) is more costly than false alarm (FP)\n",
    "    # Expects y_pred (labels)\n",
    "    \"fbeta_score\": {\n",
    "        \"beta\": 2.0,  # Recall is 2x more important than Precision\n",
    "        \"pos_label\": 1,\n",
    "        \"average\": \"binary\",\n",
    "        \"zero_division\": 0.0,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Primary metrics defined:\")\n",
    "for name in primary_metric_functions:\n",
    "    print(f\"  - {name}: {primary_metric_args.get(name, {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary_metrics",
   "metadata": {},
   "outputs": [],
   "source": "# -----------------------------------------------------------------------------\n# SECONDARY METRICS - Good for monitoring and additional insights\n# These provide complementary information but shouldn't drive model selection\n# -----------------------------------------------------------------------------\n\nsecondary_metric_functions = {\n    # Accuracy: Overall correctness (TP + TN) / Total\n    # CAUTION: Misleading for imbalanced data when used alone!\n    # Include for: baseline comparison, sanity checks, stakeholder reporting\n    # With 3% fraud: predicting all non-fraud = 97% accuracy (useless!)\n    # ALWAYS show alongside balanced_accuracy and recall\n    \"accuracy_score\": metrics.accuracy_score,\n    \n    # Balanced Accuracy: Average of recall on each class\n    # = (TPR + TNR) / 2 = (Recall_fraud + Recall_non_fraud) / 2\n    # Better than accuracy for imbalanced data - penalizes ignoring minority\n    \"balanced_accuracy_score\": metrics.balanced_accuracy_score,\n    \n    # Matthews Correlation Coefficient: Most robust single metric\n    # Balanced measure, works well with imbalanced classes\n    # Range: [-1, +1], 0 = random, +1 = perfect, -1 = inverse\n    # Only metric that gives high score when all 4 confusion matrix categories are good\n    \"matthews_corrcoef\": metrics.matthews_corrcoef,\n    \n    # Cohen's Kappa: Agreement beyond chance\n    # Useful for comparing with baseline/random classifier\n    # Range: [-1, +1], 0 = no better than chance, +1 = perfect\n    \"cohen_kappa_score\": metrics.cohen_kappa_score,\n    \n    # Jaccard Score: Intersection over Union (IoU)\n    # TP / (TP + FP + FN) - stricter than F1\n    # Ignores TN, focuses only on positive class predictions\n    \"jaccard_score\": metrics.jaccard_score,\n}\n\nsecondary_metric_args = {\n    # Accuracy: normalize=True returns fraction [0, 1]\n    # sample_weight=None means equal weight for all samples\n    # For imbalanced data: use alongside balanced_accuracy, never alone!\n    \"accuracy_score\": {\n        \"normalize\": True,  # Return fraction (0.0 to 1.0), not count\n        # sample_weight: Can be set dynamically to correct for imbalance\n        # Example: weight fraud samples higher to penalize missing them\n    },\n    \n    # Balanced Accuracy: adjusted=False returns [0, 1], adjusted=True shifts to [-0.5, 1]\n    # adjusted=True: random classifier scores 0, adjusted=False: random scores ~0.5\n    \"balanced_accuracy_score\": {\n        \"adjusted\": False,  # Keep in [0, 1] range for interpretability\n    },\n    \n    # MCC: No special args, works on y_true vs y_pred\n    # Handles imbalanced data well by design\n    \"matthews_corrcoef\": {},\n    \n    # Cohen's Kappa: weights=None for unweighted agreement\n    # weights='linear' or 'quadratic' for ordinal classification\n    \"cohen_kappa_score\": {\n        \"weights\": None,  # Unweighted (linear/quadratic for ordinal data)\n    },\n    \n    # Jaccard: binary classification with fraud as positive class\n    \"jaccard_score\": {\n        \"pos_label\": 1,\n        \"average\": \"binary\",\n        \"zero_division\": 0.0,\n    },\n}\n\nprint(\"Secondary metrics defined:\")\nfor name in secondary_metric_functions:\n    print(f\"  - {name}: {secondary_metric_args.get(name, {})}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "probabilistic_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# PROBABILISTIC/CALIBRATION METRICS - For probability calibration monitoring\n",
    "# These measure how well-calibrated the predicted probabilities are\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "probabilistic_metric_functions = {\n",
    "    # Log Loss (Cross-Entropy): Penalizes confident wrong predictions\n",
    "    # Lower is better, heavily penalizes confident mistakes\n",
    "    \"log_loss\": metrics.log_loss,\n",
    "    \n",
    "    # Brier Score: Mean squared error of probability predictions\n",
    "    # Lower is better, range [0, 1] for binary with scale_by_half=True\n",
    "    \"brier_score_loss\": metrics.brier_score_loss,\n",
    "    \n",
    "    # D^2 Log Loss Score: Fraction of log loss explained\n",
    "    # Similar to R^2, but for log loss; higher is better\n",
    "    \"d2_log_loss_score\": metrics.d2_log_loss_score,\n",
    "    \n",
    "    # D^2 Brier Score: Fraction of Brier score explained\n",
    "    # Similar to R^2, but for Brier score; higher is better\n",
    "    # Best=1.0, can be negative if worse than null model\n",
    "    \"d2_brier_score\": metrics.d2_brier_score,\n",
    "}\n",
    "\n",
    "probabilistic_metric_args = {\n",
    "    # Log Loss: normalize=True returns mean loss per sample\n",
    "    \"log_loss\": {\n",
    "        \"normalize\": True,\n",
    "    },\n",
    "    \n",
    "    # Brier Score: pos_label=1 means fraud is positive class\n",
    "    \"brier_score_loss\": {\n",
    "        \"pos_label\": 1,\n",
    "    },\n",
    "    \n",
    "    # D^2 Log Loss: No special args\n",
    "    \"d2_log_loss_score\": {},\n",
    "    \n",
    "    # D^2 Brier Score: pos_label=1 for fraud class\n",
    "    \"d2_brier_score\": {\n",
    "        \"pos_label\": 1,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Probabilistic/Calibration metrics defined:\")\n",
    "for name in probabilistic_metric_functions:\n",
    "    print(f\"  - {name}: {probabilistic_metric_args.get(name, {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# ANALYSIS/REPORTING METRICS - For detailed analysis and threshold tuning\n",
    "# These return multiple values or structured outputs\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "analysis_metric_functions = {\n",
    "    # Confusion Matrix: Foundation for many other metrics\n",
    "    # Returns 2x2 matrix: [[TN, FP], [FN, TP]]\n",
    "    \"confusion_matrix\": metrics.confusion_matrix,\n",
    "    \n",
    "    # NEW IN SKLEARN 1.8! Confusion Matrix at Thresholds\n",
    "    # Returns TN, FP, FN, TP arrays for each threshold\n",
    "    # CRITICAL for threshold optimization in fraud detection\n",
    "    \"confusion_matrix_at_thresholds\": metrics.confusion_matrix_at_thresholds,\n",
    "    \n",
    "    # Classification Report: Text summary of P, R, F1 per class\n",
    "    # Can return dict with output_dict=True\n",
    "    \"classification_report\": metrics.classification_report,\n",
    "    \n",
    "    # Precision-Recall Curve: For threshold analysis\n",
    "    # Returns (precision, recall, thresholds)\n",
    "    \"precision_recall_curve\": metrics.precision_recall_curve,\n",
    "    \n",
    "    # ROC Curve: For threshold analysis\n",
    "    # Returns (fpr, tpr, thresholds)\n",
    "    \"roc_curve\": metrics.roc_curve,\n",
    "    \n",
    "    # DET Curve: Detection Error Tradeoff\n",
    "    # Returns (fpr, fnr, thresholds) - plots FNR vs FPR\n",
    "    # Useful for fraud: visualize false alarm vs missed fraud tradeoff\n",
    "    \"det_curve\": metrics.det_curve,\n",
    "    \n",
    "    # Class Likelihood Ratios: LR+, LR- for diagnostic testing\n",
    "    # Returns (positive_lr, negative_lr)\n",
    "    \"class_likelihood_ratios\": metrics.class_likelihood_ratios,\n",
    "    \n",
    "    # Precision-Recall-FScore-Support: All in one\n",
    "    # Returns (precision, recall, fbeta, support) arrays\n",
    "    \"precision_recall_fscore_support\": metrics.precision_recall_fscore_support,\n",
    "    \n",
    "    # AUC: General utility to compute area under any curve\n",
    "    \"auc\": metrics.auc,\n",
    "}\n",
    "\n",
    "analysis_metric_args = {\n",
    "    # Confusion Matrix: labels=[0, 1] ensures consistent ordering\n",
    "    # normalize='true' normalizes over actual (row-wise)\n",
    "    \"confusion_matrix\": {\n",
    "        \"labels\": [0, 1],  # [non-fraud, fraud]\n",
    "        \"normalize\": None,  # Return raw counts; use 'true'/'pred'/'all' for proportions\n",
    "    },\n",
    "    \n",
    "    # Confusion Matrix at Thresholds: pos_label=1 for fraud\n",
    "    # Returns (tns, fps, fns, tps, thresholds) arrays\n",
    "    \"confusion_matrix_at_thresholds\": {\n",
    "        \"pos_label\": 1,  # Fraud is positive class\n",
    "    },\n",
    "    \n",
    "    # Classification Report: output_dict=True for programmatic access\n",
    "    \"classification_report\": {\n",
    "        \"target_names\": [\"Non-Fraud\", \"Fraud\"],\n",
    "        \"output_dict\": True,  # Return dict instead of string\n",
    "        \"zero_division\": 0.0,\n",
    "    },\n",
    "    \n",
    "    # Precision-Recall Curve: pos_label=1 for fraud class\n",
    "    \"precision_recall_curve\": {\n",
    "        \"pos_label\": 1,\n",
    "    },\n",
    "    \n",
    "    # ROC Curve: pos_label=1 for fraud class\n",
    "    \"roc_curve\": {\n",
    "        \"pos_label\": 1,\n",
    "        \"drop_intermediate\": True,  # Reduce points for efficiency\n",
    "    },\n",
    "    \n",
    "    # DET Curve: pos_label=1 for fraud class\n",
    "    \"det_curve\": {\n",
    "        \"pos_label\": 1,\n",
    "        \"drop_intermediate\": True,  # Reduce points for efficiency\n",
    "    },\n",
    "    \n",
    "    # Class Likelihood Ratios: labels=[non-fraud, fraud] ordering\n",
    "    \"class_likelihood_ratios\": {\n",
    "        \"labels\": [0, 1],  # [negative_class, positive_class]\n",
    "    },\n",
    "    \n",
    "    # Precision-Recall-FScore-Support: beta=1.0 for F1\n",
    "    \"precision_recall_fscore_support\": {\n",
    "        \"beta\": 1.0,\n",
    "        \"pos_label\": 1,\n",
    "        \"average\": \"binary\",\n",
    "        \"zero_division\": 0.0,\n",
    "    },\n",
    "    \n",
    "    # AUC: No default args, takes x and y arrays directly\n",
    "    \"auc\": {},\n",
    "}\n",
    "\n",
    "print(\"Analysis/Reporting metrics defined:\")\n",
    "for name in analysis_metric_functions:\n",
    "    print(f\"  - {name}: {analysis_metric_args.get(name, {})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "display_classes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# VISUALIZATION CLASSES - For plotting metrics\n",
    "# These are classes that create matplotlib visualizations\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "display_classes = {\n",
    "    # Confusion Matrix Display: Heatmap of confusion matrix\n",
    "    \"ConfusionMatrixDisplay\": metrics.ConfusionMatrixDisplay,\n",
    "    \n",
    "    # ROC Curve Display: Plot ROC curve with AUC\n",
    "    \"RocCurveDisplay\": metrics.RocCurveDisplay,\n",
    "    \n",
    "    # Precision-Recall Display: Plot PR curve with AP\n",
    "    \"PrecisionRecallDisplay\": metrics.PrecisionRecallDisplay,\n",
    "    \n",
    "    # DET Curve Display: Detection Error Tradeoff curve\n",
    "    \"DetCurveDisplay\": metrics.DetCurveDisplay,\n",
    "}\n",
    "\n",
    "print(\"Display classes for visualization:\")\n",
    "for name in display_classes:\n",
    "    print(f\"  - {name}\")\n",
    "    print(f\"    Methods: from_estimator(), from_predictions(), plot()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excluded_metrics",
   "metadata": {},
   "outputs": [],
   "source": "# -----------------------------------------------------------------------------\n# EXCLUDED METRICS - Less relevant for TFD binary classification\n# -----------------------------------------------------------------------------\n\nexcluded_metrics = {\n    # top_k_accuracy_score: For multiclass ranking, not binary\n    \"top_k_accuracy_score\": \"For multiclass ranking scenarios, not binary classification\",\n    \n    # hamming_loss: More relevant for multilabel classification\n    \"hamming_loss\": \"More relevant for multilabel classification\",\n    \n    # zero_one_loss: Equivalent to 1 - accuracy, same issues\n    # If you need error rate, just compute 1 - accuracy_score\n    \"zero_one_loss\": \"Equivalent to 1 - accuracy, redundant\",\n    \n    # hinge_loss: Specific to SVM margin-based classifiers\n    # Only meaningful for linear SVMs, not tree-based models like XGBoost/CatBoost\n    \"hinge_loss\": \"Specific to SVM margin-based classifiers\",\n    \n    # multilabel_confusion_matrix: For multilabel, not binary\n    \"multilabel_confusion_matrix\": \"For multilabel classification scenarios\",\n}\n\nprint(\"Excluded metrics and reasons:\")\nfor name, reason in excluded_metrics.items():\n    print(f\"  - {name}: {reason}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usage_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# USAGE EXAMPLE - How to use these metrics in practice\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Simulated predictions\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "fraud_rate = 0.03  # 3% fraud rate (imbalanced)\n",
    "\n",
    "# True labels\n",
    "y_true = np.random.choice([0, 1], size=n_samples, p=[1-fraud_rate, fraud_rate])\n",
    "\n",
    "# Simulated model probabilities (fraud probability)\n",
    "# Good model: higher probs for actual fraud, lower for non-fraud\n",
    "y_proba = np.where(\n",
    "    y_true == 1,\n",
    "    np.random.beta(5, 2, size=n_samples),  # Fraud: skewed high\n",
    "    np.random.beta(2, 5, size=n_samples),  # Non-fraud: skewed low\n",
    ")\n",
    "\n",
    "# Predicted labels at threshold 0.5\n",
    "threshold = 0.5\n",
    "y_pred = (y_proba >= threshold).astype(int)\n",
    "\n",
    "print(f\"Dataset: {n_samples} samples, {y_true.sum()} fraud ({100*y_true.mean():.1f}%)\")\n",
    "print(f\"Predictions: {y_pred.sum()} predicted fraud ({100*y_pred.mean():.1f}%)\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary_metrics_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PRIMARY metrics\n",
    "print(\"=\" * 60)\n",
    "print(\"PRIMARY METRICS (Most important for fraud detection)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ROC-AUC (uses probabilities)\n",
    "roc_auc = metrics.roc_auc_score(y_true, y_proba, **primary_metric_args[\"roc_auc_score\"])\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Average Precision (uses probabilities)\n",
    "avg_precision = metrics.average_precision_score(y_true, y_proba, **primary_metric_args[\"average_precision_score\"])\n",
    "print(f\"Average Precision (PR-AUC): {avg_precision:.4f}\")\n",
    "\n",
    "# Recall (uses labels)\n",
    "recall = metrics.recall_score(y_true, y_pred, **primary_metric_args[\"recall_score\"])\n",
    "print(f\"Recall (Fraud Detection Rate): {recall:.4f}\")\n",
    "\n",
    "# Precision (uses labels)\n",
    "precision = metrics.precision_score(y_true, y_pred, **primary_metric_args[\"precision_score\"])\n",
    "print(f\"Precision (True Fraud Rate): {precision:.4f}\")\n",
    "\n",
    "# F1 Score (uses labels)\n",
    "f1 = metrics.f1_score(y_true, y_pred, **primary_metric_args[\"f1_score\"])\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# F2 Score (uses labels) - Recall weighted 2x\n",
    "f2 = metrics.fbeta_score(y_true, y_pred, **primary_metric_args[\"fbeta_score\"])\n",
    "print(f\"F2 Score (Recall-weighted): {f2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary_metrics_example",
   "metadata": {},
   "outputs": [],
   "source": "# Compute SECONDARY metrics\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SECONDARY METRICS (Additional monitoring)\")\nprint(\"=\" * 60)\n\n# Accuracy (with caution label)\naccuracy = metrics.accuracy_score(y_true, y_pred, **secondary_metric_args[\"accuracy_score\"])\nprint(f\"Accuracy: {accuracy:.4f}  ⚠️ (misleading alone for imbalanced data)\")\n\n# Balanced Accuracy (preferred over accuracy for imbalanced)\nbal_acc = metrics.balanced_accuracy_score(y_true, y_pred, **secondary_metric_args[\"balanced_accuracy_score\"])\nprint(f\"Balanced Accuracy: {bal_acc:.4f}  ✓ (use this instead of accuracy)\")\n\n# Show the difference to illustrate the imbalance problem\nprint(f\"\\n  → Gap (Accuracy - Balanced Acc): {accuracy - bal_acc:.4f}\")\nprint(f\"  → Large gap indicates class imbalance affecting accuracy\")\n\n# Matthews Correlation Coefficient\nmcc = metrics.matthews_corrcoef(y_true, y_pred, **secondary_metric_args[\"matthews_corrcoef\"])\nprint(f\"\\nMatthews Correlation Coefficient: {mcc:.4f}\")\n\n# Cohen's Kappa\nkappa = metrics.cohen_kappa_score(y_true, y_pred, **secondary_metric_args[\"cohen_kappa_score\"])\nprint(f\"Cohen's Kappa: {kappa:.4f}\")\n\n# Jaccard Score\njaccard = metrics.jaccard_score(y_true, y_pred, **secondary_metric_args[\"jaccard_score\"])\nprint(f\"Jaccard Score: {jaccard:.4f}\")\n\n# Demonstrate sample_weight for accuracy (correcting imbalance)\nprint(\"\\n\" + \"-\" * 60)\nprint(\"ACCURACY WITH SAMPLE WEIGHTS (Correcting for Imbalance)\")\nprint(\"-\" * 60)\n\n# Compute class weights inversely proportional to class frequency\nn_fraud = y_true.sum()\nn_non_fraud = len(y_true) - n_fraud\nweight_fraud = len(y_true) / (2 * n_fraud) if n_fraud > 0 else 1\nweight_non_fraud = len(y_true) / (2 * n_non_fraud) if n_non_fraud > 0 else 1\n\n# Create sample weights array\nsample_weights = np.where(y_true == 1, weight_fraud, weight_non_fraud)\n\n# Compute weighted accuracy\nweighted_accuracy = metrics.accuracy_score(y_true, y_pred, sample_weight=sample_weights)\nprint(f\"Unweighted Accuracy: {accuracy:.4f}\")\nprint(f\"Weighted Accuracy:   {weighted_accuracy:.4f}\")\nprint(f\"Balanced Accuracy:   {bal_acc:.4f}\")\nprint(f\"\\n  → Weighted accuracy ≈ Balanced accuracy when using inverse class weights\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "probabilistic_metrics_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PROBABILISTIC/CALIBRATION metrics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROBABILISTIC METRICS (Calibration monitoring)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Log Loss\n",
    "logloss = metrics.log_loss(y_true, y_proba, **probabilistic_metric_args[\"log_loss\"])\n",
    "print(f\"Log Loss: {logloss:.4f}\")\n",
    "\n",
    "# Brier Score\n",
    "brier = metrics.brier_score_loss(y_true, y_proba, **probabilistic_metric_args[\"brier_score_loss\"])\n",
    "print(f\"Brier Score: {brier:.4f}\")\n",
    "\n",
    "# D^2 Log Loss Score\n",
    "d2_logloss = metrics.d2_log_loss_score(y_true, y_proba, **probabilistic_metric_args[\"d2_log_loss_score\"])\n",
    "print(f\"D^2 Log Loss Score: {d2_logloss:.4f}\")\n",
    "\n",
    "# D^2 Brier Score\n",
    "d2_brier = metrics.d2_brier_score(y_true, y_proba, **probabilistic_metric_args[\"d2_brier_score\"])\n",
    "print(f\"D^2 Brier Score: {d2_brier:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_metrics_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ANALYSIS metrics\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ANALYSIS METRICS (Detailed reporting)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = metrics.confusion_matrix(y_true, y_pred, **analysis_metric_args[\"confusion_matrix\"])\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                 Predicted\")\n",
    "print(f\"              Non-Fraud  Fraud\")\n",
    "print(f\"Actual Non-Fraud  {tn:5d}    {fp:5d}\")\n",
    "print(f\"       Fraud      {fn:5d}    {tp:5d}\")\n",
    "\n",
    "# Classification Report\n",
    "print(f\"\\nClassification Report:\")\n",
    "report = metrics.classification_report(y_true, y_pred, **analysis_metric_args[\"classification_report\"])\n",
    "for cls, vals in report.items():\n",
    "    if isinstance(vals, dict):\n",
    "        print(f\"  {cls}: P={vals['precision']:.3f}, R={vals['recall']:.3f}, F1={vals['f1-score']:.3f}\")\n",
    "\n",
    "# Class Likelihood Ratios\n",
    "lr_pos, lr_neg = metrics.class_likelihood_ratios(y_true, y_pred, **analysis_metric_args[\"class_likelihood_ratios\"])\n",
    "print(f\"\\nClass Likelihood Ratios:\")\n",
    "print(f\"  LR+ (Positive): {lr_pos:.4f}\")\n",
    "print(f\"  LR- (Negative): {lr_neg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pixwglolvm8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NEW METRICS DEMONSTRATION\n",
    "# confusion_matrix_at_thresholds and det_curve are especially useful for TFD\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEW SKLEARN 1.8 METRICS (Threshold Optimization)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Confusion Matrix at Thresholds - CRITICAL for threshold optimization\n",
    "tns, fps, fns, tps, thresholds = metrics.confusion_matrix_at_thresholds(\n",
    "    y_true, y_proba, **analysis_metric_args[\"confusion_matrix_at_thresholds\"]\n",
    ")\n",
    "print(f\"\\nConfusion Matrix at Thresholds (first 5 thresholds):\")\n",
    "print(f\"{'Threshold':>10} | {'TN':>6} | {'FP':>6} | {'FN':>6} | {'TP':>6} | {'Recall':>8} | {'Precision':>10}\")\n",
    "print(\"-\" * 75)\n",
    "for i in range(min(5, len(thresholds))):\n",
    "    recall_i = tps[i] / (tps[i] + fns[i]) if (tps[i] + fns[i]) > 0 else 0\n",
    "    precision_i = tps[i] / (tps[i] + fps[i]) if (tps[i] + fps[i]) > 0 else 0\n",
    "    print(f\"{thresholds[i]:>10.4f} | {tns[i]:>6.0f} | {fps[i]:>6.0f} | {fns[i]:>6.0f} | {tps[i]:>6.0f} | {recall_i:>8.4f} | {precision_i:>10.4f}\")\n",
    "\n",
    "# DET Curve - Detection Error Tradeoff\n",
    "fpr_det, fnr_det, thresholds_det = metrics.det_curve(\n",
    "    y_true, y_proba, **analysis_metric_args[\"det_curve\"]\n",
    ")\n",
    "print(f\"\\nDET Curve (Detection Error Tradeoff):\")\n",
    "print(f\"  FPR (False Positive Rate) range: {fpr_det.min():.4f} - {fpr_det.max():.4f}\")\n",
    "print(f\"  FNR (False Negative Rate) range: {fnr_det.min():.4f} - {fnr_det.max():.4f}\")\n",
    "print(f\"  Number of thresholds: {len(thresholds_det)}\")\n",
    "\n",
    "# Show a few threshold points\n",
    "print(f\"\\nSample DET curve points (FPR vs FNR tradeoff):\")\n",
    "print(f\"{'Threshold':>10} | {'FPR':>8} | {'FNR':>8}\")\n",
    "print(\"-\" * 35)\n",
    "sample_indices = [0, len(thresholds_det)//4, len(thresholds_det)//2, -1]\n",
    "for i in sample_indices:\n",
    "    if i < len(thresholds_det):\n",
    "        print(f\"{thresholds_det[i]:>10.4f} | {fpr_det[i]:>8.4f} | {fnr_det[i]:>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated_config",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONSOLIDATED CONFIGURATION FOR TFD BATCH ML\n# =============================================================================\n\n# All scalar metrics in one dictionary (for easy iteration)\nscalar_metric_functions = {\n    **primary_metric_functions,\n    **secondary_metric_functions,\n    **probabilistic_metric_functions,\n}\n\nscalar_metric_args = {\n    **primary_metric_args,\n    **secondary_metric_args,\n    **probabilistic_metric_args,\n}\n\n# Metrics that require probability scores (y_proba)\nproba_metrics = {\n    \"roc_auc_score\",\n    \"average_precision_score\",\n    \"log_loss\",\n    \"brier_score_loss\",\n    \"d2_log_loss_score\",\n    \"d2_brier_score\",\n}\n\n# Metrics that require predicted labels (y_pred)\nlabel_metrics = {\n    \"accuracy_score\",  # Added: include for baseline/sanity checks\n    \"recall_score\",\n    \"precision_score\",\n    \"f1_score\",\n    \"fbeta_score\",\n    \"balanced_accuracy_score\",\n    \"matthews_corrcoef\",\n    \"cohen_kappa_score\",\n    \"jaccard_score\",\n}\n\n# Curve/threshold metrics (return arrays, not scalars)\ncurve_metrics = {\n    \"roc_curve\",\n    \"precision_recall_curve\",\n    \"det_curve\",\n    \"confusion_matrix_at_thresholds\",\n}\n\nprint(\"Consolidated configuration:\")\nprint(f\"  Total scalar metrics: {len(scalar_metric_functions)}\")\nprint(f\"  Probability-based: {len(proba_metrics)}\")\nprint(f\"  Label-based: {len(label_metrics)}\")\nprint(f\"  Curve/threshold metrics: {len(curve_metrics)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_function",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# HELPER FUNCTION FOR COMPUTING ALL METRICS\n# =============================================================================\n\ndef compute_all_classification_metrics(y_true, y_pred, y_proba):\n    \"\"\"\n    Compute all recommended classification metrics for TFD.\n    \n    Parameters:\n    -----------\n    y_true : array-like\n        True binary labels (0 = non-fraud, 1 = fraud)\n    y_pred : array-like\n        Predicted binary labels\n    y_proba : array-like\n        Predicted probability of fraud (positive class)\n        \n    Returns:\n    --------\n    dict : Dictionary with all computed metrics\n    \"\"\"\n    results = {}\n    \n    # PRIMARY METRICS\n    results[\"roc_auc\"] = metrics.roc_auc_score(y_true, y_proba)\n    results[\"average_precision\"] = metrics.average_precision_score(y_true, y_proba, pos_label=1)\n    results[\"recall\"] = metrics.recall_score(y_true, y_pred, pos_label=1, zero_division=0.0)\n    results[\"precision\"] = metrics.precision_score(y_true, y_pred, pos_label=1, zero_division=0.0)\n    results[\"f1\"] = metrics.f1_score(y_true, y_pred, pos_label=1, zero_division=0.0)\n    results[\"f2\"] = metrics.fbeta_score(y_true, y_pred, beta=2.0, pos_label=1, zero_division=0.0)\n    \n    # SECONDARY METRICS\n    results[\"accuracy\"] = metrics.accuracy_score(y_true, y_pred)  # Added: for baseline/reporting\n    results[\"balanced_accuracy\"] = metrics.balanced_accuracy_score(y_true, y_pred)\n    results[\"mcc\"] = metrics.matthews_corrcoef(y_true, y_pred)\n    results[\"cohen_kappa\"] = metrics.cohen_kappa_score(y_true, y_pred)\n    results[\"jaccard\"] = metrics.jaccard_score(y_true, y_pred, pos_label=1, zero_division=0.0)\n    \n    # PROBABILISTIC METRICS\n    results[\"log_loss\"] = metrics.log_loss(y_true, y_proba)\n    results[\"brier_score\"] = metrics.brier_score_loss(y_true, y_proba, pos_label=1)\n    results[\"d2_log_loss\"] = metrics.d2_log_loss_score(y_true, y_proba)\n    results[\"d2_brier\"] = metrics.d2_brier_score(y_true, y_proba, pos_label=1)\n    \n    # CONFUSION MATRIX COMPONENTS\n    cm = metrics.confusion_matrix(y_true, y_pred, labels=[0, 1])\n    tn, fp, fn, tp = cm.ravel()\n    results[\"true_negatives\"] = int(tn)\n    results[\"false_positives\"] = int(fp)\n    results[\"false_negatives\"] = int(fn)\n    results[\"true_positives\"] = int(tp)\n    \n    return results\n\n# Test the helper function\nall_metrics = compute_all_classification_metrics(y_true, y_pred, y_proba)\nprint(\"All metrics computed:\")\nfor name, value in all_metrics.items():\n    if isinstance(value, float):\n        print(f\"  {name}: {value:.4f}\")\n    else:\n        print(f\"  {name}: {value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "key_insights",
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# KEY INSIGHTS FOR TFD BATCH ML\n# =============================================================================\n#\n# 1. PRIMARY METRICS (Use for model selection):\n#    - ROC-AUC: Best overall threshold-independent metric\n#    - Average Precision (PR-AUC): Better for highly imbalanced data\n#    - F2 Score: When catching fraud is more important than false alarms\n#\n# 2. ACCURACY vs BALANCED ACCURACY:\n#    - accuracy_score: Include for baseline comparison and stakeholder reporting\n#    - ⚠️ NEVER use accuracy alone for imbalanced data!\n#    - With 3% fraud rate, predicting all non-fraud = 97% accuracy (useless!)\n#    - ALWAYS show balanced_accuracy alongside accuracy\n#    - Use sample_weight parameter to correct for class imbalance\n#    - Large gap (accuracy - balanced_accuracy) indicates imbalance problem\n#\n# 3. THRESHOLD SELECTION (NEW IN SKLEARN 1.8):\n#    - Default threshold (0.5) is rarely optimal for imbalanced data\n#    - Use confusion_matrix_at_thresholds for optimal cutoff analysis\n#    - Use precision_recall_curve to find optimal threshold\n#    - Consider business cost of FN vs FP when choosing threshold\n#\n# 4. PROBABILITY CALIBRATION:\n#    - Use brier_score_loss and log_loss to assess calibration\n#    - NEW: d2_brier_score measures fraction of Brier explained (like R²)\n#    - Well-calibrated probabilities are crucial for threshold tuning\n#\n# 5. ERROR TRADEOFF ANALYSIS:\n#    - NEW: det_curve shows FPR vs FNR tradeoff directly\n#    - For fraud: FPR = false alarms, FNR = missed fraud\n#    - Use DetCurveDisplay for visualization\n#\n# 6. BATCH vs ONLINE:\n#    - sklearn metrics compute on full dataset at once\n#    - For online/streaming, use River library instead\n#    - Batch metrics are more stable but require all data upfront\n#\n# 7. METRIC INTERPRETATION FOR TFD:\n#    - High Recall: Catching most fraud (minimizing FN)\n#    - High Precision: Few false alarms (minimizing FP)\n#    - F2 > F1: Indicates model favors recall (fraud detection)\n#    - MCC close to 1: Strong balanced performance\n#    - Large (Accuracy - Balanced Accuracy) gap: Class imbalance issue\n#\n# 8. METRICS SUMMARY:\n#    - Primary: 6 (roc_auc, avg_precision, recall, precision, f1, f2)\n#    - Secondary: 5 (accuracy, balanced_acc, mcc, cohen_kappa, jaccard)\n#    - Probabilistic: 4 (log_loss, brier, d2_log_loss, d2_brier)\n#    - Analysis/Curve: 9 (confusion_matrix, cm_at_thresholds, classification_report,\n#                         pr_curve, roc_curve, det_curve, class_lr, prfs, auc)\n#    - Visualization: 4 (ConfusionMatrixDisplay, RocCurveDisplay,\n#                        PrecisionRecallDisplay, DetCurveDisplay)\n#\n# =============================================================================\nprint(\"Key insights documented above. Review before using in production.\")\nprint()\nprint(\"TOTAL METRICS INVESTIGATED:\")\nprint(f\"  - Primary: 6 (roc_auc, avg_precision, recall, precision, f1, f2)\")\nprint(f\"  - Secondary: 5 (accuracy, balanced_acc, mcc, cohen_kappa, jaccard)\")\nprint(f\"  - Probabilistic: 4 (log_loss, brier, d2_log_loss, d2_brier)\")\nprint(f\"  - Analysis/Curve: 9 (confusion_matrix, cm_at_thresholds, classification_report,\")\nprint(f\"                       pr_curve, roc_curve, det_curve, class_lr, prfs, auc)\")\nprint(f\"  - Visualization: 4 (ConfusionMatrixDisplay, RocCurveDisplay,\")\nprint(f\"                      PrecisionRecallDisplay, DetCurveDisplay)\")\nprint()\nprint(\"TOTAL: 28 metrics/functions + 4 display classes = 32 items\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}