{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7af8c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import (\n",
    "    KafkaConsumer,\n",
    "    TopicPartition\n",
    ")\n",
    "from kafka.errors import NoBrokersAvailable\n",
    "from typing import Any, Dict, Hashable, Optional, List\n",
    "from river import (\n",
    "    base,\n",
    "    compose,\n",
    "    metrics,\n",
    "    drift,\n",
    "    forest,\n",
    "    cluster,\n",
    "    preprocessing,\n",
    "    time_series,\n",
    "    linear_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f949875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAFKA_HOST = \"localhost\"\n",
    "KAFKA_PORT = 9092  # Change to 9093 if skaffold port-forward uses that port\n",
    "KAFKA_BROKERS = f'{KAFKA_HOST}:{KAFKA_PORT}'\n",
    "PROJECT_NAME = \"Transaction Fraud Detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84a077a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accuracy',\n",
       " 'AdjustedMutualInfo',\n",
       " 'AdjustedRand',\n",
       " 'BalancedAccuracy',\n",
       " 'ClassificationReport',\n",
       " 'CohenKappa',\n",
       " 'Completeness',\n",
       " 'ConfusionMatrix',\n",
       " 'CrossEntropy',\n",
       " 'F1',\n",
       " 'FBeta',\n",
       " 'FowlkesMallows',\n",
       " 'GeometricMean',\n",
       " 'Homogeneity',\n",
       " 'Jaccard',\n",
       " 'LogLoss',\n",
       " 'MAE',\n",
       " 'MAPE',\n",
       " 'MCC',\n",
       " 'MSE',\n",
       " 'MacroF1',\n",
       " 'MacroFBeta',\n",
       " 'MacroJaccard',\n",
       " 'MacroPrecision',\n",
       " 'MacroRecall',\n",
       " 'MicroF1',\n",
       " 'MicroFBeta',\n",
       " 'MicroJaccard',\n",
       " 'MicroPrecision',\n",
       " 'MicroRecall',\n",
       " 'MultiFBeta',\n",
       " 'MutualInfo',\n",
       " 'NormalizedMutualInfo',\n",
       " 'Precision',\n",
       " 'R2',\n",
       " 'RMSE',\n",
       " 'RMSLE',\n",
       " 'ROCAUC',\n",
       " 'Rand',\n",
       " 'Recall',\n",
       " 'RollingROCAUC',\n",
       " 'SMAPE',\n",
       " 'Silhouette',\n",
       " 'VBeta',\n",
       " 'WeightedF1',\n",
       " 'WeightedFBeta',\n",
       " 'WeightedJaccard',\n",
       " 'WeightedPrecision',\n",
       " 'WeightedRecall',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'accuracy',\n",
       " 'annotations',\n",
       " 'balanced_accuracy',\n",
       " 'base',\n",
       " 'confusion',\n",
       " 'cross_entropy',\n",
       " 'efficient_rollingrocauc',\n",
       " 'expected_mutual_info',\n",
       " 'fbeta',\n",
       " 'fowlkes_mallows',\n",
       " 'geometric_mean',\n",
       " 'jaccard',\n",
       " 'kappa',\n",
       " 'log_loss',\n",
       " 'mae',\n",
       " 'mape',\n",
       " 'mcc',\n",
       " 'mse',\n",
       " 'multioutput',\n",
       " 'mutual_info',\n",
       " 'precision',\n",
       " 'r2',\n",
       " 'rand',\n",
       " 'recall',\n",
       " 'report',\n",
       " 'roc_auc',\n",
       " 'rolling_roc_auc',\n",
       " 'silhouette',\n",
       " 'smape',\n",
       " 'vbeta']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b54725ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consumer(project_name, max_retries: int = 5, retry_delay: float = 5.0, start_offset: Optional[int] = None):\n",
    "    \"\"\"Create and return Kafka consumer with manual partition assignment.\n",
    "    \n",
    "    Args:\n",
    "        project_name: Name of the project for topic selection.\n",
    "        max_retries: Maximum connection retry attempts.\n",
    "        retry_delay: Delay between retries in seconds.\n",
    "        start_offset: If provided, seek to this offset + 1 to continue from last processed.\n",
    "                     If None, seeks to the earliest available offset (handles log compaction).\n",
    "    \n",
    "    Note: Uses manual partition assignment instead of group-based subscription\n",
    "    due to Kafka 4.0 compatibility issues with kafka-python's consumer group protocol.\n",
    "    \"\"\"\n",
    "    consumer_name_dict = {\n",
    "        \"Transaction Fraud Detection\": \"transaction_fraud_detection\",\n",
    "        \"Estimated Time of Arrival\": \"estimated_time_of_arrival\",\n",
    "        \"E-Commerce Customer Interactions\": \"e_commerce_customer_interactions\",\n",
    "        \"Sales Forecasting\": \"sales_forecasting\"\n",
    "    }\n",
    "    KAFKA_TOPIC = consumer_name_dict[project_name]\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Create consumer without topic subscription (manual assignment)\n",
    "            consumer = KafkaConsumer(\n",
    "                bootstrap_servers = KAFKA_BROKERS,\n",
    "                value_deserializer = lambda v: json.loads(v.decode('utf-8')),\n",
    "                consumer_timeout_ms = 1000,  # 1 second timeout for graceful shutdown checks\n",
    "                api_version = (3, 7),  # Force API version for Kafka 4.0 compatibility\n",
    "            )\n",
    "            \n",
    "            # Manually assign partition 0 of the topic\n",
    "            tp = TopicPartition(KAFKA_TOPIC, 0)\n",
    "            consumer.assign([tp])\n",
    "            \n",
    "            # Seek to appropriate offset\n",
    "            if start_offset is not None:\n",
    "                # Continue from the next message after last processed\n",
    "                next_offset = start_offset + 1\n",
    "                consumer.seek(tp, next_offset)\n",
    "                print(f\"Kafka consumer seeking to offset {next_offset} (continuing from {start_offset})\")\n",
    "            else:\n",
    "                # No stored offset - seek to earliest AVAILABLE offset\n",
    "                # Using seek_to_beginning + poll + position as workaround for Kafka 4.0\n",
    "                # (beginning_offsets() blocks indefinitely with kafka-python + Kafka 4.0)\n",
    "                consumer.seek_to_beginning(tp)\n",
    "                consumer.poll(timeout_ms=100)  # Trigger the seek\n",
    "                begin_offset = consumer.position(tp)\n",
    "                consumer.seek(tp, begin_offset)  # Ensure we're at the detected position\n",
    "                print(f\"Kafka consumer seeking to earliest available offset {begin_offset}\")\n",
    "            \n",
    "            print(f\"Kafka consumer created for {project_name} (manual assignment)\")\n",
    "            return consumer\n",
    "            \n",
    "        except NoBrokersAvailable as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"Kafka not available for {project_name}, retrying in {retry_delay}s... (attempt {attempt + 1}/{max_retries})\")\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                print(f\"Failed to connect to Kafka for {project_name} after {max_retries} attempts. Continuing without consumer.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating Kafka consumer for {project_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5fe69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOrdinalEncoder:\n",
    "    \"\"\"\n",
    "    An incremental ordinal encoder that is picklable and processes dictionaries.\n",
    "    Assigns a unique integer ID to each unique category encountered for each feature.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Dictionary to store mappings for each feature.\n",
    "        # Keys are feature names (from input dictionary), values are dictionaries\n",
    "        # mapping category value to integer ID for that feature.\n",
    "        self._feature_mappings: Dict[Hashable, Dict[Any, int]] = {}\n",
    "        # Dictionary to store the next available integer ID for each feature.\n",
    "        # Keys are feature names, values are integers.\n",
    "        self._feature_next_ids: Dict[Hashable, int] = {}\n",
    "    def learn_one(self, x: Dict[Hashable, Any]):\n",
    "        \"\"\"\n",
    "        Learns categories from a single sample dictionary.\n",
    "        Iterates through the dictionary's items and learns each category value\n",
    "        for its corresponding feature.\n",
    "        Args:\n",
    "            x: A dictionary representing a single sample.\n",
    "               Keys are feature names, values are feature values.\n",
    "               Assumes categorical features are present in this dictionary.\n",
    "        \"\"\"\n",
    "        for feature_name, category_value in x.items():\n",
    "            # Ensure the category value is hashable (dictionaries/lists are not)\n",
    "            # You might need more sophisticated type checking or handling\n",
    "            # if your input dictionaries contain complex unhashable types\n",
    "            if not isinstance(category_value, Hashable):\n",
    "                 print(f\"Warning: Skipping unhashable value for feature '{feature_name}': {category_value}\")\n",
    "                 continue # Skip this feature for learning\n",
    "            # If this is the first time we see this feature, initialize its mapping and counter\n",
    "            if feature_name not in self._feature_mappings:\n",
    "                self._feature_mappings[feature_name] = {}\n",
    "                self._feature_next_ids[feature_name] = 0\n",
    "            # Get the mapping and counter for this specific feature\n",
    "            feature_map = self._feature_mappings[feature_name]\n",
    "            feature_next_id = self._feature_next_ids[feature_name]\n",
    "            # Check if the category value is already in the mapping for this feature\n",
    "            if category_value not in feature_map:\n",
    "                # If it's a new category for this feature, assign the next available ID\n",
    "                feature_map[category_value] = feature_next_id\n",
    "                # Increment the counter for the next new category for this feature\n",
    "                self._feature_next_ids[feature_name] += 1\n",
    "    def transform_one(self, x: Dict[Hashable, Any]) -> Dict[Hashable, int]:\n",
    "        \"\"\"\n",
    "        Transforms categorical features in a single sample dictionary into integer IDs.\n",
    "        Args:\n",
    "            x: A dictionary representing a single sample.\n",
    "               Keys are feature names, values are feature values.\n",
    "        Returns:\n",
    "            A new dictionary containing the transformed integer IDs for the\n",
    "            categorical features that the encoder has seen. Features not\n",
    "            seen by the encoder are excluded from the output dictionary.\n",
    "        Raises:\n",
    "            KeyError: If a feature is seen but a specific category value\n",
    "                      within that feature has not been seen during learning.\n",
    "                      You might want to add logic here to handle unseen categories\n",
    "                      (e.g., return a default value like -1 or NaN for that feature).\n",
    "        \"\"\"\n",
    "        transformed_sample: Dict[Hashable, int] = {}\n",
    "        for feature_name, category_value in x.items():\n",
    "            # Only attempt to transform features that the encoder has seen\n",
    "            if feature_name in self._feature_mappings:\n",
    "                feature_map = self._feature_mappings[feature_name]\n",
    "                # Check if the category value for this feature has been seen\n",
    "                if category_value in feature_map:\n",
    "                    # Transform the category value using the feature's mapping\n",
    "                    transformed_sample[feature_name] = feature_map[category_value]\n",
    "                else:\n",
    "                    # Handle unseen category values for a known feature\n",
    "                    # By default, this will raise a KeyError as per the docstring.\n",
    "                    # Example: return a placeholder value instead of raising error:\n",
    "                    # transformed_sample[feature_name] = -1 # Or some other indicator\n",
    "                    # print(f\"Warning: Unseen category '{category_value}' for feature '{feature_name}' during transform.\")\n",
    "                    # Or raise the error explicitly:\n",
    "                    raise KeyError(f\"Unseen category '{category_value}' for feature '{feature_name}' during transform.\")\n",
    "            # Features not in self._feature_mappings are ignored in the output.\n",
    "            # If you need to include them (e.g., original numerical features),\n",
    "            # you would copy them over here. This encoder only outputs encoded features.\n",
    "        return transformed_sample\n",
    "    def get_feature_mappings(self) -> Dict[Hashable, Dict[Any, int]]:\n",
    "        \"\"\"Returns the current mappings for all features.\"\"\"\n",
    "        return self._feature_mappings\n",
    "    def get_feature_next_ids(self) -> Dict[Hashable, int]:\n",
    "        \"\"\"Returns the next available IDs for all features.\"\"\"\n",
    "        return self._feature_next_ids\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"String representation of the encoder.\"\"\"\n",
    "        num_features = len(self._feature_mappings)\n",
    "        feature_details = \", \".join([f\"{name}: {len(mapping)} categories\" for name, mapping in self._feature_mappings.items()])\n",
    "        return f\"CustomPicklableOrdinalEncoder(features={num_features} [{feature_details}])\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed1f9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictImputer(base.Transformer):\n",
    "    \"\"\"\n",
    "    Imputes missing values (None or missing keys) for specified features in a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    on\n",
    "        List of feature names to impute.\n",
    "    fill_value\n",
    "        The value to use for imputation.\n",
    "    \"\"\"\n",
    "    def __init__(self, on: list, fill_value):\n",
    "        self.on = on\n",
    "        self.fill_value = fill_value\n",
    "    def transform_one(self, x: dict):\n",
    "        x_transformed = x.copy()\n",
    "        for feature in self.on:\n",
    "            if x_transformed.get(feature) is None:\n",
    "                x_transformed[feature] = self.fill_value\n",
    "        return x_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e13a8e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_device_info(x):\n",
    "    x_ = x['device_info']\n",
    "    return {\n",
    "        'os': x_['os'],\n",
    "        'browser': x_['browser'],\n",
    "    }\n",
    "\n",
    "def extract_timestamp_info(x):\n",
    "    x_ = dt.datetime.strptime(\n",
    "        x['timestamp'],\n",
    "        \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "    return {\n",
    "        'year': x_.year,\n",
    "        'month': x_.month,\n",
    "        'day': x_.day,\n",
    "        'hour': x_.hour,\n",
    "        'minute': x_.minute,\n",
    "        'second': x_.second\n",
    "    }\n",
    "\n",
    "def extract_coordinates(x):\n",
    "    x_ = x['location']\n",
    "    return {\n",
    "        'lat': x_['lat'],\n",
    "        'lon': x_['lon'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d85a701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(x, encoders, project_name):\n",
    "    \"\"\"Process a single sample for River incremental learning.\"\"\"\n",
    "    if project_name == \"Transaction Fraud Detection\":\n",
    "        pipe1 = compose.Select(\n",
    "            \"amount\",\n",
    "            \"account_age_days\",\n",
    "            \"cvv_provided\",\n",
    "            \"billing_address_match\"\n",
    "        )\n",
    "        pipe1.learn_one(x)\n",
    "        x1 = pipe1.transform_one(x)\n",
    "        pipe2 = compose.Select(\n",
    "            \"currency\",\n",
    "            \"merchant_id\",\n",
    "            \"payment_method\",\n",
    "            \"product_category\",\n",
    "            \"transaction_type\",\n",
    "        )\n",
    "        pipe2.learn_one(x)\n",
    "        x_pipe_2 = pipe2.transform_one(x)\n",
    "        pipe3a = compose.Select(\"device_info\")\n",
    "        pipe3a.learn_one(x)\n",
    "        x_pipe_3 = pipe3a.transform_one(x)\n",
    "        pipe3b = compose.FuncTransformer(extract_device_info)\n",
    "        pipe3b.learn_one(x_pipe_3)\n",
    "        x_pipe_3 = pipe3b.transform_one(x_pipe_3)\n",
    "        pipe4a = compose.Select(\"timestamp\")\n",
    "        pipe4a.learn_one(x)\n",
    "        x_pipe_4 = pipe4a.transform_one(x)\n",
    "        pipe4b = compose.FuncTransformer(extract_timestamp_info)\n",
    "        pipe4b.learn_one(x_pipe_4)\n",
    "        x_pipe_4 = pipe4b.transform_one(x_pipe_4)\n",
    "        x_to_encode = x_pipe_2 | x_pipe_3 | x_pipe_4\n",
    "        encoders[\"ordinal_encoder\"].learn_one(x_to_encode)\n",
    "        x2 = encoders[\"ordinal_encoder\"].transform_one(x_to_encode)\n",
    "        return x1 | x2, {\"ordinal_encoder\": encoders[\"ordinal_encoder\"]}\n",
    "    elif project_name == \"Estimated Time of Arrival\":\n",
    "        pipe1 = compose.Select(\n",
    "            'estimated_distance_km',\n",
    "            'temperature_celsius',\n",
    "            'hour_of_day',\n",
    "            'driver_rating',\n",
    "            'initial_estimated_travel_time_seconds',\n",
    "            'debug_traffic_factor',\n",
    "            'debug_weather_factor',\n",
    "            'debug_incident_delay_seconds',\n",
    "            'debug_driver_factor'\n",
    "        )\n",
    "        pipe1.learn_one(x)\n",
    "        x1 = pipe1.transform_one(x)\n",
    "        pipe2 = compose.Select(\n",
    "            'driver_id',\n",
    "            'vehicle_id',\n",
    "            'weather',\n",
    "            'vehicle_type'\n",
    "        )\n",
    "        pipe2.learn_one(x)\n",
    "        x_pipe_2 = pipe2.transform_one(x)\n",
    "        pipe3a = compose.Select(\n",
    "            \"timestamp\",\n",
    "        )\n",
    "        pipe3a.learn_one(x)\n",
    "        x_pipe_3 = pipe3a.transform_one(x)\n",
    "        pipe3b = compose.FuncTransformer(\n",
    "            extract_timestamp_info,\n",
    "        )\n",
    "        pipe3b.learn_one(x_pipe_3)\n",
    "        x_pipe_3 = pipe3b.transform_one(x_pipe_3)\n",
    "        x_to_encode = x_pipe_2 | x_pipe_3\n",
    "        encoders[\"ordinal_encoder\"].learn_one(x_to_encode)\n",
    "        x2 = encoders[\"ordinal_encoder\"].transform_one(x_to_encode)\n",
    "        return x1 | x2, {\n",
    "            \"ordinal_encoder\": encoders[\"ordinal_encoder\"]\n",
    "        }\n",
    "    elif project_name == \"E-Commerce Customer Interactions\":\n",
    "        pipe1 = compose.Select(\n",
    "            'price',\n",
    "            'quantity',\n",
    "            'session_event_sequence',\n",
    "            'time_on_page_seconds'\n",
    "        )\n",
    "        pipe1.learn_one(x)\n",
    "        x1 = pipe1.transform_one(x)\n",
    "        pipe2 = compose.Select(\n",
    "            'event_type',\n",
    "            'product_category',\n",
    "            'product_id',\n",
    "            'referrer_url',\n",
    "        )\n",
    "        pipe2.learn_one(x)\n",
    "        x_pipe_2 = pipe2.transform_one(x)\n",
    "        pipe3a = compose.Select(\n",
    "            \"device_info\"\n",
    "        )\n",
    "        pipe3a.learn_one(x)\n",
    "        x_pipe_3 = pipe3a.transform_one(x)\n",
    "        pipe3b = compose.FuncTransformer(\n",
    "            extract_device_info,\n",
    "        )\n",
    "        pipe3b.learn_one(x_pipe_3)\n",
    "        x_pipe_3 = pipe3b.transform_one(x_pipe_3)\n",
    "        pipe4a = compose.Select(\n",
    "            \"timestamp\",\n",
    "        )\n",
    "        pipe4a.learn_one(x)\n",
    "        x_pipe_4 = pipe4a.transform_one(x)\n",
    "        pipe4b = compose.FuncTransformer(\n",
    "            extract_timestamp_info,\n",
    "        )\n",
    "        pipe4b.learn_one(x_pipe_4)\n",
    "        x_pipe_4 = pipe4b.transform_one(x_pipe_4)\n",
    "        pipe5a = compose.Select(\n",
    "            \"location\",\n",
    "        )\n",
    "        pipe5a.learn_one(x)\n",
    "        x_pipe_5 = pipe5a.transform_one(x)\n",
    "        pipe5b = compose.FuncTransformer(\n",
    "            extract_coordinates,\n",
    "        )\n",
    "        pipe5b.learn_one(x_pipe_5)\n",
    "        x_pipe_5 = pipe5b.transform_one(x_pipe_5)\n",
    "        x_to_prep = x1 | x_pipe_2 | x_pipe_3 | x_pipe_4 | x_pipe_5\n",
    "        x_to_prep = DictImputer(\n",
    "            fill_value = False, \n",
    "            on = list(x_to_prep.keys())).transform_one(\n",
    "                x_to_prep)\n",
    "        numerical_features = [\n",
    "            'price',\n",
    "            'session_event_sequence',\n",
    "            'time_on_page_seconds',\n",
    "            'quantity'\n",
    "        ]\n",
    "        categorical_features = [\n",
    "            'event_type',\n",
    "            'product_category',\n",
    "            'product_id',\n",
    "            'referrer_url',\n",
    "            'os',\n",
    "            'browser',\n",
    "            'year',\n",
    "            'month',\n",
    "            'day',\n",
    "            'hour',\n",
    "            'minute',\n",
    "            'second'\n",
    "        ]\n",
    "        num_pipe = compose.Select(*numerical_features)\n",
    "        num_pipe.learn_one(x_to_prep)\n",
    "        x_num = num_pipe.transform_one(x_to_prep)\n",
    "        cat_pipe = compose.Select(*categorical_features)\n",
    "        cat_pipe.learn_one(x_to_prep)\n",
    "        x_cat = cat_pipe.transform_one(x_to_prep)\n",
    "        encoders[\"standard_scaler\"].learn_one(x_num)\n",
    "        x_scaled = encoders[\"standard_scaler\"].transform_one(x_num)\n",
    "        encoders[\"feature_hasher\"].learn_one(x_cat)\n",
    "        x_hashed = encoders[\"feature_hasher\"].transform_one(x_cat)\n",
    "        return x_scaled | x_hashed, {\n",
    "            \"standard_scaler\": encoders[\"standard_scaler\"], \n",
    "            \"feature_hasher\": encoders[\"feature_hasher\"]\n",
    "        }\n",
    "    elif project_name == \"Sales Forecasting\":\n",
    "        pipe1 = compose.Select(\n",
    "            'concept_drift_stage',\n",
    "            'day_of_week',\n",
    "            'is_holiday',\n",
    "            'is_promotion_active',\n",
    "            'month',\n",
    "            #'total_sales_amount',\n",
    "            'unit_price'\n",
    "        )\n",
    "        pipe1.learn_one(x)\n",
    "        x1 = pipe1.transform_one(x)\n",
    "        pipe2a = compose.Select(\n",
    "            \"timestamp\",\n",
    "        )\n",
    "        pipe2a.learn_one(x)\n",
    "        x_pipe_2 = pipe2a.transform_one(x)\n",
    "        pipe2b = compose.FuncTransformer(\n",
    "            extract_timestamp_info,\n",
    "        )\n",
    "        pipe2b.learn_one(x_pipe_2)\n",
    "        x2 = pipe2b.transform_one(x_pipe_2)\n",
    "        pipe3a = compose.Select(\n",
    "            'product_id',\n",
    "            'promotion_id',\n",
    "            'store_id'\n",
    "        )\n",
    "        pipe3a.learn_one(x)\n",
    "        x3 = pipe3a.transform_one(x)\n",
    "        x_to_process = x1 | x2 | x3\n",
    "        numerical_features = [\n",
    "            'unit_price',\n",
    "            #'total_sales_amount',\n",
    "        ]\n",
    "        categorical_features = [\n",
    "            'is_promotion_active',\n",
    "            'is_holiday',\n",
    "            'day_of_week',\n",
    "            'concept_drift_stage',\n",
    "            'year',\n",
    "            'month',\n",
    "            'day',\n",
    "            #'hour',\n",
    "            #'minute',\n",
    "            #'second',\n",
    "            'product_id',\n",
    "            'promotion_id',\n",
    "            'store_id',\n",
    "        ]\n",
    "        pipe_num = compose.Select(*numerical_features)\n",
    "        pipe_num.learn_one(x_to_process)\n",
    "        x_num = pipe_num.transform_one(x_to_process)\n",
    "        pipe_cat = compose.Select(*categorical_features)\n",
    "        pipe_cat.learn_one(x_to_process)\n",
    "        x_cat = pipe_cat.transform_one(x_to_process)\n",
    "        encoders[\"standard_scaler\"].learn_one(x_num)\n",
    "        x_num = encoders[\"standard_scaler\"].transform_one(x_num)\n",
    "        encoders[\"one_hot_encoder\"].learn_one(x_cat)\n",
    "        x_cat = encoders[\"one_hot_encoder\"].transform_one(x_cat)\n",
    "        return x_num | x_cat, {\n",
    "            \"one_hot_encoder\": encoders[\"one_hot_encoder\"],\n",
    "            \"standard_scaler\": encoders[\"standard_scaler\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c308dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_default_model(project_name):\n",
    "    \"\"\"Create default model based on project type.\n",
    "\n",
    "    Models are configured based on River ML documentation and best practices.\n",
    "    All parameters are documented with their River ML defaults and rationale.\n",
    "\n",
    "    See: https://riverml.xyz/latest/\n",
    "    \"\"\"\n",
    "    if project_name == \"Transaction Fraud Detection\":\n",
    "        # =================================================================\n",
    "        # ARFClassifier - Adaptive Random Forest Classifier\n",
    "        # For fraud detection with concept drift handling\n",
    "        # =================================================================\n",
    "        # OLD CONFIGURATION:\n",
    "        # return forest.ARFClassifier(\n",
    "        #     n_models = 10,\n",
    "        #     drift_detector = drift.ADWIN(),\n",
    "        #     warning_detector = drift.ADWIN(),\n",
    "        #     metric = metrics.ROCAUC(),\n",
    "        #     max_features = \"sqrt\",\n",
    "        #     lambda_value = 6,\n",
    "        #     seed = 42\n",
    "        # )\n",
    "\n",
    "        # CONFIGURATION based on River ML documentation:\n",
    "        # Reference: https://riverml.xyz/latest/api/forest/ARFClassifier/\n",
    "        # Reference: https://riverml.xyz/latest/examples/imbalanced-learning/\n",
    "        #\n",
    "        # - n_models=10: Default number of trees in ensemble\n",
    "        # - max_features=\"sqrt\": Default, sqrt of features per split\n",
    "        # - lambda_value=6: Default Leveraging Bagging parameter\n",
    "        # - metric=ROCAUC(): RECOMMENDED by River for imbalanced fraud detection\n",
    "        #   (River's imbalanced-learning guide uses ROCAUC for fraud detection)\n",
    "        # - disable_weighted_vote=False: Enable weighted voting for better accuracy\n",
    "        # - drift_detector ADWIN(delta=0.002): Default sensitivity (0.002)\n",
    "        # - warning_detector ADWIN(delta=0.01): Default warning sensitivity\n",
    "        # - grace_period=50: Default observations between split attempts\n",
    "        # - max_depth=None: Default, unlimited tree depth\n",
    "        # - split_criterion=\"info_gain\": Default, information gain criterion\n",
    "        # - delta=0.01: Default allowed error in split decision\n",
    "        # - tau=0.05: Default tie-breaking threshold\n",
    "        # - leaf_prediction=\"nba\": Default, Naive Bayes Adaptive\n",
    "        # - nb_threshold=0: Default, enable NB immediately\n",
    "        # - binary_split=False: Default, allow multi-way splits\n",
    "        # - min_branch_fraction=0.01: Default minimum data per branch\n",
    "        # - max_share_to_split=0.99: Default majority class proportion\n",
    "        # - max_size=100.0: Default max memory in MiB\n",
    "        # - memory_estimate_period=2000000: Default instances between memory checks\n",
    "        # - merit_preprune=True: Default merit-based pre-pruning\n",
    "        return forest.ARFClassifier(\n",
    "            n_models = 10,\n",
    "            max_features = \"sqrt\",\n",
    "            lambda_value = 6,\n",
    "            metric = metrics.ROCAUC(),\n",
    "            disable_weighted_vote = False,\n",
    "            drift_detector = drift.ADWIN(delta = 0.002),\n",
    "            warning_detector = drift.ADWIN(delta = 0.01),\n",
    "            grace_period = 50,\n",
    "            max_depth = None,\n",
    "            split_criterion = \"info_gain\",\n",
    "            delta = 0.01,\n",
    "            tau = 0.05,\n",
    "            leaf_prediction = \"nba\",\n",
    "            nb_threshold = 0,\n",
    "            nominal_attributes = None,\n",
    "            binary_split = False,\n",
    "            min_branch_fraction = 0.01,\n",
    "            max_share_to_split = 0.99,\n",
    "            max_size = 100.0,\n",
    "            memory_estimate_period = 2000000,\n",
    "            stop_mem_management = False,\n",
    "            remove_poor_attrs = False,\n",
    "            merit_preprune = True,\n",
    "            seed = 42,\n",
    "        )\n",
    "    elif project_name == \"Estimated Time of Arrival\":\n",
    "        # =================================================================\n",
    "        # ARFRegressor - Adaptive Random Forest Regressor\n",
    "        # For ETA prediction with continuous drift handling\n",
    "        # =================================================================\n",
    "        # OLD CONFIGURATION:\n",
    "        # return forest.ARFRegressor(\n",
    "        #     n_models = 10,\n",
    "        #     drift_detector = drift.ADWIN(),\n",
    "        #     warning_detector = drift.ADWIN(),\n",
    "        #     metric = metrics.RMSE(),\n",
    "        #     max_features = \"sqrt\",\n",
    "        #     lambda_value = 6,\n",
    "        #     seed = 42\n",
    "        # )\n",
    "\n",
    "        # CONFIGURATION based on River ML documentation:\n",
    "        # Reference: https://riverml.xyz/latest/api/forest/ARFRegressor/\n",
    "        #\n",
    "        # - n_models=10: Default number of trees\n",
    "        # - max_features=\"sqrt\": Default feature selection\n",
    "        # - aggregation_method=\"median\": Default, robust to outliers\n",
    "        # - lambda_value=6: Default Leveraging Bagging parameter\n",
    "        # - metric=MAE(): Using MAE as it's common for ETA prediction\n",
    "        # - disable_weighted_vote=True: Default for regressor\n",
    "        # - drift_detector ADWIN(delta=0.002): Default sensitivity\n",
    "        # - warning_detector ADWIN(delta=0.01): Default warning sensitivity\n",
    "        # - grace_period=50: Default observations between split attempts\n",
    "        # - max_depth=None: Default unlimited depth\n",
    "        # - delta=0.01: Default allowed error\n",
    "        # - tau=0.05: Default tie-breaking threshold\n",
    "        # - leaf_prediction=\"adaptive\": Default, dynamically chooses mean/model\n",
    "        # - model_selector_decay=0.95: Default decay for leaf model selection\n",
    "        # - min_samples_split=5: Default minimum samples for split\n",
    "        # - binary_split=False: Default multi-way splits\n",
    "        # - max_size=500.0: Default max memory in MiB\n",
    "        return forest.ARFRegressor(\n",
    "            n_models=10,\n",
    "            max_features=\"sqrt\",\n",
    "            aggregation_method=\"median\",\n",
    "            lambda_value=6,\n",
    "            metric=metrics.MAE(),\n",
    "            disable_weighted_vote=True,\n",
    "            drift_detector=drift.ADWIN(delta=0.002),\n",
    "            warning_detector=drift.ADWIN(delta=0.01),\n",
    "            grace_period=50,\n",
    "            max_depth=None,\n",
    "            delta=0.01,\n",
    "            tau=0.05,\n",
    "            leaf_prediction=\"adaptive\",\n",
    "            leaf_model=None,\n",
    "            model_selector_decay=0.95,\n",
    "            min_samples_split=5,\n",
    "            binary_split=False,\n",
    "            max_size=500.0,\n",
    "            memory_estimate_period=2000000,\n",
    "            nominal_attributes=None,\n",
    "            seed=42,\n",
    "        )\n",
    "    elif project_name == \"E-Commerce Customer Interactions\":\n",
    "        # =================================================================\n",
    "        # DBSTREAM - Density-Based Stream Clustering\n",
    "        # For customer behavior clustering with arbitrary shapes\n",
    "        # =================================================================\n",
    "        # OLD CONFIGURATION:\n",
    "        # return cluster.DBSTREAM(\n",
    "        #     clustering_threshold = 1.0,\n",
    "        #     fading_factor = 0.01,\n",
    "        #     cleanup_interval = 2,\n",
    "        # )\n",
    "\n",
    "        # CONFIGURATION based on River ML documentation example:\n",
    "        # Reference: https://riverml.xyz/latest/api/cluster/DBSTREAM/\n",
    "        #\n",
    "        # The River documentation provides this exact example configuration:\n",
    "        # - clustering_threshold=1.5: Micro-cluster radius\n",
    "        # - fading_factor=0.05: Historical data importance (must be > 0)\n",
    "        # - cleanup_interval=4: Time between cleanup processes\n",
    "        # - intersection_factor=0.5: Cluster overlap ratio for connectivity\n",
    "        # - minimum_weight=1.0: Threshold for non-noisy cluster classification\n",
    "        return cluster.DBSTREAM(\n",
    "            clustering_threshold=1.5,\n",
    "            fading_factor=0.05,\n",
    "            cleanup_interval=4,\n",
    "            intersection_factor=0.5,\n",
    "            minimum_weight=1.0,\n",
    "        )\n",
    "    elif project_name == \"Sales Forecasting\":\n",
    "        # =================================================================\n",
    "        # SNARIMAX - Seasonal Non-linear Auto-Regressive Integrated\n",
    "        # Moving Average with eXogenous inputs\n",
    "        # For sales forecasting with weekly seasonality\n",
    "        # =================================================================\n",
    "        # OLD CONFIGURATION:\n",
    "        # regressor_snarimax = linear_model.PARegressor(\n",
    "        #     C = 0.01,\n",
    "        #     mode = 1)\n",
    "        # return time_series.SNARIMAX(\n",
    "        #     p = 2,\n",
    "        #     d = 1,\n",
    "        #     q = 1,\n",
    "        #     m = 7,\n",
    "        #     sp = 1,\n",
    "        #     sd = 0,\n",
    "        #     sq = 1,\n",
    "        #     regressor = regressor_snarimax\n",
    "        # )\n",
    "\n",
    "        # CONFIGURATION based on River ML documentation:\n",
    "        # Reference: https://riverml.xyz/latest/api/time-series/SNARIMAX/\n",
    "        # Reference: https://riverml.xyz/latest/api/linear-model/PARegressor/\n",
    "        #\n",
    "        # SNARIMAX parameters for weekly sales data:\n",
    "        # - p=7: Past 7 days of target values (full week)\n",
    "        # - d=1: First-order differencing for trend removal\n",
    "        # - q=2: Past error terms for noise handling\n",
    "        # - m=7: Weekly seasonality period\n",
    "        # - sp=1: Seasonal autoregressive order\n",
    "        # - sd=1: Seasonal differencing (recommended for seasonal data)\n",
    "        # - sq=1: Seasonal moving average order\n",
    "        #\n",
    "        # PARegressor parameters (defaults from River docs):\n",
    "        # - C=1.0: Default regularization strength\n",
    "        # - mode=1: Default algorithm mode\n",
    "        # - eps=0.1: Default tolerance parameter\n",
    "        # - learn_intercept=True: Default bias learning\n",
    "        regressor_snarimax = linear_model.PARegressor(\n",
    "            C=1.0,\n",
    "            mode=1,\n",
    "            eps=0.1,\n",
    "            learn_intercept=True,\n",
    "        )\n",
    "        return time_series.SNARIMAX(\n",
    "            p=7,\n",
    "            d=1,\n",
    "            q=2,\n",
    "            m=7,\n",
    "            sp=1,\n",
    "            sd=1,\n",
    "            sq=1,\n",
    "            regressor=regressor_snarimax,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown project: {project_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb00c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_default_encoders(project_name):\n",
    "    \"\"\"Create default encoders based on project type.\"\"\"\n",
    "    if project_name in [\"Transaction Fraud Detection\", \"Estimated Time of Arrival\"]:\n",
    "        return {\"ordinal_encoder\": CustomOrdinalEncoder()}\n",
    "    elif project_name == \"E-Commerce Customer Interactions\":\n",
    "        return {\n",
    "            \"standard_scaler\": preprocessing.StandardScaler(),\n",
    "            \"feature_hasher\": preprocessing.FeatureHasher()\n",
    "        }\n",
    "    elif project_name == \"Sales Forecasting\":\n",
    "        return {\n",
    "            \"one_hot_encoder\": preprocessing.OneHotEncoder(),\n",
    "            \"standard_scaler\": preprocessing.StandardScaler(),\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown project: {project_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d930c716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kafka consumer seeking to offset 0 (continuing from -1)\n",
      "Kafka consumer created for Transaction Fraud Detection (manual assignment)\n"
     ]
    }
   ],
   "source": [
    "consumer = create_consumer(PROJECT_NAME, start_offset = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "yhnkc3lxt3j",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing connection to localhost:9092...\n",
      "Creating consumer...\n",
      "Consumer created!\n",
      "Partition assigned\n",
      "Seeking to offset 670000...\n",
      "Seek complete\n",
      "Polling...\n",
      "Poll 1: 0 partitions, 0 msgs\n",
      "Poll 2: 0 partitions, 0 msgs\n",
      "Poll 3: 0 partitions, 0 msgs\n",
      "Poll 4: 0 partitions, 0 msgs\n",
      "Poll 5: 0 partitions, 0 msgs\n"
     ]
    }
   ],
   "source": [
    "# Test Kafka connectivity - hardcoded offset (known valid: 467546-674409)\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "import json\n",
    "\n",
    "print(f\"Testing connection to {KAFKA_BROKERS}...\")\n",
    "\n",
    "try:\n",
    "    print(\"Creating consumer...\")\n",
    "    consumer = KafkaConsumer(\n",
    "        bootstrap_servers=KAFKA_BROKERS,\n",
    "        value_deserializer=lambda v: json.loads(v.decode('utf-8')),\n",
    "        api_version=(3, 7),\n",
    "    )\n",
    "    print(\"Consumer created!\")\n",
    "    \n",
    "    tp = TopicPartition('transaction_fraud_detection', 0)\n",
    "    consumer.assign([tp])\n",
    "    print(\"Partition assigned\")\n",
    "    \n",
    "    # Use known valid offset from earlier check\n",
    "    # Topic range: 467546 to ~676000\n",
    "    OFFSET = 670000\n",
    "    print(f\"Seeking to offset {OFFSET}...\")\n",
    "    consumer.seek(tp, OFFSET)\n",
    "    print(\"Seek complete\")\n",
    "    \n",
    "    print(\"Polling...\")\n",
    "    for i in range(5):\n",
    "        records = consumer.poll(timeout_ms=3000, max_records=5)\n",
    "        print(f\"Poll {i+1}: {len(records)} partitions, {sum(len(m) for m in records.values())} msgs\")\n",
    "        if records:\n",
    "            for p, msgs in records.items():\n",
    "                for m in msgs[:2]:\n",
    "                    print(f\"  offset={m.offset} keys={list(m.value.keys())[:3]}\")\n",
    "            print(\"SUCCESS!\")\n",
    "            break\n",
    "    \n",
    "    consumer.close()\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8aaa8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classification_metrics_dict = {\n",
    "    \"Accuracy\": metrics.Accuracy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ddc5d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total messages collected: 0\n"
     ]
    }
   ],
   "source": [
    "# Get only 1000 messages from consumer\n",
    "MAX_MESSAGES = 1000\n",
    "message_count = 0\n",
    "messages = []\n",
    "\n",
    "for message in consumer:\n",
    "    messages.append(message.value)\n",
    "    message_count += 1\n",
    "    if message_count % 100 == 0:\n",
    "        print(f\"Processed {message_count} messages...\")\n",
    "    if message_count >= MAX_MESSAGES:\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal messages collected: {len(messages)}\")\n",
    "if messages:\n",
    "    print(f\"First message sample: {messages[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc71dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
