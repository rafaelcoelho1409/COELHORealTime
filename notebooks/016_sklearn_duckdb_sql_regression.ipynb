{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 011 - Sklearn DuckDB SQL Regression (LightGBM + XGBoost)\n",
    "\n",
    "This notebook demonstrates **complete batch ML training** for ETA (Estimated Time of Arrival) regression using:\n",
    "\n",
    "1. **DuckDB SQL** for data loading and preprocessing from Delta Lake\n",
    "2. **LightGBM** as primary model (fastest, highly accurate for regression)\n",
    "3. **XGBoost** as fallback model\n",
    "4. **All sklearn regression metrics** for comprehensive evaluation\n",
    "5. **YellowBrick** for regression visualizations (coming next)\n",
    "\n",
    "## Model Selection Rationale\n",
    "\n",
    "Based on extensive research (2024-2025 benchmarks):\n",
    "\n",
    "| Model | Training Speed | Accuracy | Best For |\n",
    "|-------|----------------|----------|----------|\n",
    "| **LightGBM** | 7x faster than XGBoost | Lowest MAPE | Large datasets, real-time |\n",
    "| CatBoost | 3-4x slower than LightGBM | Best default | Categorical features |\n",
    "| XGBoost | Slowest | Very good | Fine-grained control |\n",
    "\n",
    "**LightGBM is chosen** for ETA prediction because:\n",
    "- Fastest training and inference (critical for real-time ETA updates)\n",
    "- Excellent regression performance (lowest MAPE in benchmarks)\n",
    "- Memory-efficient histogram-based algorithm\n",
    "- Industry standard: Uber, Lyft, DiDi use gradient boosting for ETA\n",
    "\n",
    "## Target Variable\n",
    "\n",
    "- **`simulated_actual_travel_time_seconds`**: Actual travel time in seconds\n",
    "- This is a continuous regression target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_HOST = \"localhost\"\n",
    "MINIO_PORT = \"9000\"\n",
    "MINIO_ENDPOINT = f\"{MINIO_HOST}:{MINIO_PORT}\"\n",
    "MINIO_ACCESS_KEY = \"minioadmin\"\n",
    "MINIO_SECRET_KEY = \"minioadmin123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_PATHS = {\n",
    "    \"Transaction Fraud Detection\": \"s3://lakehouse/delta/transaction_fraud_detection\",\n",
    "    \"Estimated Time of Arrival\": \"s3://lakehouse/delta/estimated_time_of_arrival\",\n",
    "    \"E-Commerce Customer Interactions\": \"s3://lakehouse/delta/e_commerce_customer_interactions\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB extensions loaded and S3 secret configured\n"
     ]
    }
   ],
   "source": [
    "# Disable AWS EC2 metadata service lookup (prevents 169.254.169.254 errors)\n",
    "os.environ[\"AWS_EC2_METADATA_DISABLED\"] = \"true\"\n",
    "\n",
    "# Create connection (in-memory database)\n",
    "conn = duckdb.connect()\n",
    "\n",
    "# Install and load required extensions\n",
    "conn.execute(\"INSTALL delta; LOAD delta;\")\n",
    "conn.execute(\"INSTALL httpfs; LOAD httpfs;\")\n",
    "\n",
    "# Create a secret for S3/MinIO credentials\n",
    "conn.execute(f\"\"\"\n",
    "    CREATE SECRET minio_secret (\n",
    "        TYPE S3,\n",
    "        KEY_ID '{MINIO_ACCESS_KEY}',\n",
    "        SECRET '{MINIO_SECRET_KEY}',\n",
    "        REGION 'us-east-1',\n",
    "        ENDPOINT '{MINIO_ENDPOINT}',\n",
    "        URL_STYLE 'path',\n",
    "        USE_SSL false\n",
    "    );\n",
    "\"\"\")\n",
    "print(\"DuckDB extensions loaded and S3 secret configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Definitions\n",
    "\n",
    "Define features upfront for LightGBM's native categorical handling.\n",
    "\n",
    "### ETA Features Overview\n",
    "\n",
    "| Category | Features |\n",
    "|----------|----------|\n",
    "| **Numerical** | estimated_distance_km, temperature_celsius, driver_rating, hour_of_day, initial_estimated_travel_time_seconds, debug_traffic_factor, debug_weather_factor, debug_incident_delay_seconds, debug_driver_factor |\n",
    "| **Categorical** | trip_id, driver_id, vehicle_id, origin, destination, weather, day_of_week, vehicle_type |\n",
    "| **Temporal** | year, month, day, hour, minute, second (extracted from timestamp) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features: 9\n",
      "Categorical features: 14\n",
      "Categorical indices: [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n"
     ]
    }
   ],
   "source": [
    "# Feature definitions for Estimated Time of Arrival\n",
    "ETA_NUMERICAL_FEATURES = [\n",
    "    \"estimated_distance_km\",\n",
    "    \"temperature_celsius\",\n",
    "    \"driver_rating\",\n",
    "    \"hour_of_day\",\n",
    "    \"initial_estimated_travel_time_seconds\",\n",
    "    \"debug_traffic_factor\",\n",
    "    \"debug_weather_factor\",\n",
    "    \"debug_incident_delay_seconds\",\n",
    "    \"debug_driver_factor\",\n",
    "]\n",
    "\n",
    "ETA_CATEGORICAL_FEATURES = [\n",
    "    # IDs (high cardinality - label encoded)\n",
    "    \"trip_id\",\n",
    "    \"driver_id\",\n",
    "    \"vehicle_id\",\n",
    "    # Locations\n",
    "    \"origin\",\n",
    "    \"destination\",\n",
    "    # Context\n",
    "    \"weather\",\n",
    "    \"day_of_week\",\n",
    "    \"vehicle_type\",\n",
    "    # Temporal (extracted from timestamp)\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    \"second\",\n",
    "]\n",
    "\n",
    "ETA_ALL_FEATURES = ETA_NUMERICAL_FEATURES + ETA_CATEGORICAL_FEATURES\n",
    "\n",
    "# Categorical feature indices for LightGBM (position in feature list)\n",
    "ETA_CAT_FEATURE_INDICES = list(range(\n",
    "    len(ETA_NUMERICAL_FEATURES),\n",
    "    len(ETA_ALL_FEATURES)\n",
    "))\n",
    "\n",
    "# Categorical feature names for LightGBM\n",
    "ETA_CAT_FEATURE_NAMES = ETA_CATEGORICAL_FEATURES\n",
    "\n",
    "print(f\"Numerical features: {len(ETA_NUMERICAL_FEATURES)}\")\n",
    "print(f\"Categorical features: {len(ETA_CATEGORICAL_FEATURES)}\")\n",
    "print(f\"Categorical indices: {ETA_CAT_FEATURE_INDICES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB SQL Preprocessing\n",
    "\n",
    "All categorical features are **label-encoded in SQL** using `DENSE_RANK() - 1`.\n",
    "\n",
    "This produces numeric data compatible with:\n",
    "- LightGBM (pass `categorical_feature` for native handling)\n",
    "- XGBoost (works directly with integers)\n",
    "- YellowBrick (requires numeric data)\n",
    "- All sklearn tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_duckdb_sql(\n",
    "    delta_path: str,\n",
    "    sample_frac: float | None = None,\n",
    "    max_rows: int | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and preprocess ETA data using pure DuckDB SQL.\n",
    "    \n",
    "    All categorical features are label-encoded using DENSE_RANK() - 1.\n",
    "    This produces numeric data compatible with:\n",
    "    - LightGBM (pass categorical_feature for native handling)\n",
    "    - XGBoost (works directly with integers)\n",
    "    - YellowBrick (requires numeric data)\n",
    "    - All sklearn tools\n",
    "    \n",
    "    Args:\n",
    "        delta_path: Path to Delta Lake table\n",
    "        sample_frac: Optional fraction of data to sample (0.0-1.0)\n",
    "        max_rows: Optional maximum number of rows to load\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with preprocessed features (all numeric) and target\n",
    "    \"\"\"\n",
    "    # Single query: All features numeric, categoricals label-encoded\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        -- Numerical features (unchanged)\n",
    "        estimated_distance_km,\n",
    "        temperature_celsius,\n",
    "        driver_rating,\n",
    "        hour_of_day,\n",
    "        initial_estimated_travel_time_seconds,\n",
    "        debug_traffic_factor,\n",
    "        debug_weather_factor,\n",
    "        debug_incident_delay_seconds,\n",
    "        debug_driver_factor,\n",
    "\n",
    "        -- Categorical features: Label encoded with DENSE_RANK() - 1\n",
    "        -- This produces 0-indexed integers compatible with all ML tools\n",
    "        DENSE_RANK() OVER (ORDER BY trip_id) - 1 AS trip_id,\n",
    "        DENSE_RANK() OVER (ORDER BY driver_id) - 1 AS driver_id,\n",
    "        DENSE_RANK() OVER (ORDER BY vehicle_id) - 1 AS vehicle_id,\n",
    "        DENSE_RANK() OVER (ORDER BY origin) - 1 AS origin,\n",
    "        DENSE_RANK() OVER (ORDER BY destination) - 1 AS destination,\n",
    "        DENSE_RANK() OVER (ORDER BY weather) - 1 AS weather,\n",
    "        DENSE_RANK() OVER (ORDER BY day_of_week) - 1 AS day_of_week,\n",
    "        DENSE_RANK() OVER (ORDER BY vehicle_type) - 1 AS vehicle_type,\n",
    "\n",
    "        -- Timestamp components (already integers)\n",
    "        CAST(date_part('year', CAST(timestamp AS TIMESTAMP)) AS INTEGER) AS year,\n",
    "        CAST(date_part('month', CAST(timestamp AS TIMESTAMP)) AS INTEGER) AS month,\n",
    "        CAST(date_part('day', CAST(timestamp AS TIMESTAMP)) AS INTEGER) AS day,\n",
    "        CAST(date_part('hour', CAST(timestamp AS TIMESTAMP)) AS INTEGER) AS hour,\n",
    "        CAST(date_part('minute', CAST(timestamp AS TIMESTAMP)) AS INTEGER) AS minute,\n",
    "        CAST(date_part('second', CAST(timestamp AS TIMESTAMP)) AS INTEGER) AS second,\n",
    "\n",
    "        -- Target (continuous - travel time in seconds)\n",
    "        simulated_actual_travel_time_seconds\n",
    "\n",
    "    FROM delta_scan('{delta_path}')\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading ETA data with DuckDB SQL (all features numeric)...\")\n",
    "\n",
    "    # Add sampling clause\n",
    "    if sample_frac is not None and 0 < sample_frac < 1:\n",
    "        query += f\" USING SAMPLE {sample_frac * 100}%\"\n",
    "        print(f\"  Sampling: {sample_frac * 100}%\")\n",
    "\n",
    "    # Add limit clause\n",
    "    if max_rows is not None:\n",
    "        query += f\" LIMIT {max_rows}\"\n",
    "        print(f\"  Max rows: {max_rows}\")\n",
    "\n",
    "    df = conn.execute(query).df()\n",
    "    print(f\"  Loaded {len(df):,} rows with {len(df.columns)} columns\")\n",
    "    print(f\"  All features numeric: {df.select_dtypes(include=['number']).shape[1]}/{len(df.columns)} columns\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ETA data with DuckDB SQL (all features numeric)...\n",
      "  Max rows: 10000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c2750d56ab44fe8b58feb6eba7bb86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 10,000 rows with 24 columns\n",
      "  All features numeric: 24/24 columns\n"
     ]
    }
   ],
   "source": [
    "# Set model type for training\n",
    "MODEL_TYPE = \"lightgbm\"  # \"lightgbm\" or \"xgboost\"\n",
    "\n",
    "# Load data from Delta Lake\n",
    "df = load_data_duckdb_sql(\n",
    "    DELTA_PATHS[\"Estimated Time of Arrival\"],\n",
    "    max_rows=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   estimated_distance_km                  10000 non-null  float64\n",
      " 1   temperature_celsius                    10000 non-null  float64\n",
      " 2   driver_rating                          10000 non-null  float64\n",
      " 3   hour_of_day                            10000 non-null  int32  \n",
      " 4   initial_estimated_travel_time_seconds  10000 non-null  int32  \n",
      " 5   debug_traffic_factor                   10000 non-null  float64\n",
      " 6   debug_weather_factor                   10000 non-null  float64\n",
      " 7   debug_incident_delay_seconds           10000 non-null  int32  \n",
      " 8   debug_driver_factor                    10000 non-null  float64\n",
      " 9   trip_id                                10000 non-null  int64  \n",
      " 10  driver_id                              10000 non-null  int64  \n",
      " 11  vehicle_id                             10000 non-null  int64  \n",
      " 12  origin                                 10000 non-null  int64  \n",
      " 13  destination                            10000 non-null  int64  \n",
      " 14  weather                                10000 non-null  int64  \n",
      " 15  day_of_week                            10000 non-null  int64  \n",
      " 16  vehicle_type                           10000 non-null  int64  \n",
      " 17  year                                   10000 non-null  int32  \n",
      " 18  month                                  10000 non-null  int32  \n",
      " 19  day                                    10000 non-null  int32  \n",
      " 20  hour                                   10000 non-null  int32  \n",
      " 21  minute                                 10000 non-null  int32  \n",
      " 22  second                                 10000 non-null  int32  \n",
      " 23  simulated_actual_travel_time_seconds   10000 non-null  int32  \n",
      "dtypes: float64(6), int32(10), int64(8)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Variable Statistics (simulated_actual_travel_time_seconds):\n",
      "count    10000.000000\n",
      "mean      4547.659900\n",
      "std       2491.924058\n",
      "min         60.000000\n",
      "25%       2667.000000\n",
      "50%       4231.000000\n",
      "75%       6047.000000\n",
      "max      19545.000000\n",
      "Name: simulated_actual_travel_time_seconds, dtype: float64\n",
      "\n",
      "Target range: 60 - 19545 seconds\n",
      "Target range in minutes: 1.0 - 325.8 minutes\n"
     ]
    }
   ],
   "source": [
    "# Check target variable distribution\n",
    "print(\"Target Variable Statistics (simulated_actual_travel_time_seconds):\")\n",
    "print(df[\"simulated_actual_travel_time_seconds\"].describe())\n",
    "print(f\"\\nTarget range: {df['simulated_actual_travel_time_seconds'].min():.0f} - {df['simulated_actual_travel_time_seconds'].max():.0f} seconds\")\n",
    "print(f\"Target range in minutes: {df['simulated_actual_travel_time_seconds'].min()/60:.1f} - {df['simulated_actual_travel_time_seconds'].max()/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Batch Data\n",
    "\n",
    "Split features/target and prepare for training.\n",
    "\n",
    "Data is **already all-numeric** from DuckDB SQL preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_data_duckdb(\n",
    "    df: pd.DataFrame,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process batch data for model training.\n",
    "    \n",
    "    Data is already all-numeric from load_data_duckdb_sql().\n",
    "    Works with both LightGBM (pass categorical_feature) and XGBoost.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame from load_data_duckdb_sql() (all numeric)\n",
    "        test_size: Fraction for test set\n",
    "        random_state: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Split features and target\n",
    "    y = df[\"simulated_actual_travel_time_seconds\"]\n",
    "    X = df.drop(\"simulated_actual_travel_time_seconds\", axis=1)\n",
    "    \n",
    "    print(f\"Features: {len(X.columns)} total ({len(ETA_NUMERICAL_FEATURES)} numeric, {len(ETA_CATEGORICAL_FEATURES)} label-encoded)\")\n",
    "    print(f\"All features are numeric - compatible with YellowBrick and sklearn tools\")\n",
    "    \n",
    "    # Train/test split (no stratification for regression)\n",
    "    print(f\"Splitting data: {1-test_size:.0%} train, {test_size:.0%} test...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    \n",
    "    print(f\"  Training set: {len(X_train):,} samples\")\n",
    "    print(f\"  Test set: {len(X_test):,} samples\")\n",
    "    \n",
    "    # Target statistics\n",
    "    print(f\"  Target mean (train): {y_train.mean():.1f} seconds ({y_train.mean()/60:.1f} minutes)\")\n",
    "    print(f\"  Target std (train): {y_train.std():.1f} seconds\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 23 total (9 numeric, 14 label-encoded)\n",
      "All features are numeric - compatible with YellowBrick and sklearn tools\n",
      "Splitting data: 80% train, 20% test...\n",
      "  Training set: 8,000 samples\n",
      "  Test set: 2,000 samples\n",
      "  Target mean (train): 4554.3 seconds (75.9 minutes)\n",
      "  Target std (train): 2487.3 seconds\n"
     ]
    }
   ],
   "source": [
    "# Process data - same for both model types (data is already numeric)\n",
    "X_train, X_test, y_train, y_test = process_batch_data_duckdb(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimated_distance_km</th>\n",
       "      <th>temperature_celsius</th>\n",
       "      <th>driver_rating</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>initial_estimated_travel_time_seconds</th>\n",
       "      <th>debug_traffic_factor</th>\n",
       "      <th>debug_weather_factor</th>\n",
       "      <th>debug_incident_delay_seconds</th>\n",
       "      <th>debug_driver_factor</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>...</th>\n",
       "      <th>destination</th>\n",
       "      <th>weather</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9254</th>\n",
       "      <td>22.52</td>\n",
       "      <td>29.8</td>\n",
       "      <td>4.3</td>\n",
       "      <td>8</td>\n",
       "      <td>2162</td>\n",
       "      <td>1.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>773153</td>\n",
       "      <td>...</td>\n",
       "      <td>313136</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2026</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>68.93</td>\n",
       "      <td>22.6</td>\n",
       "      <td>3.7</td>\n",
       "      <td>7</td>\n",
       "      <td>6426</td>\n",
       "      <td>1.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>226377</td>\n",
       "      <td>...</td>\n",
       "      <td>322853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2026</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>19.99</td>\n",
       "      <td>29.6</td>\n",
       "      <td>4.2</td>\n",
       "      <td>13</td>\n",
       "      <td>1887</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>650530</td>\n",
       "      <td>...</td>\n",
       "      <td>340954</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2026</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>41.43</td>\n",
       "      <td>21.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>7</td>\n",
       "      <td>3609</td>\n",
       "      <td>1.55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.04</td>\n",
       "      <td>28751</td>\n",
       "      <td>...</td>\n",
       "      <td>408974</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2026</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6669</th>\n",
       "      <td>49.61</td>\n",
       "      <td>19.7</td>\n",
       "      <td>4.3</td>\n",
       "      <td>3</td>\n",
       "      <td>4028</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.01</td>\n",
       "      <td>376446</td>\n",
       "      <td>...</td>\n",
       "      <td>15812</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2026</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      estimated_distance_km  temperature_celsius  driver_rating  hour_of_day  \\\n",
       "9254                  22.52                 29.8            4.3            8   \n",
       "1561                  68.93                 22.6            3.7            7   \n",
       "1670                  19.99                 29.6            4.2           13   \n",
       "6087                  41.43                 21.1            3.7            7   \n",
       "6669                  49.61                 19.7            4.3            3   \n",
       "\n",
       "      initial_estimated_travel_time_seconds  debug_traffic_factor  \\\n",
       "9254                                   2162                  1.41   \n",
       "1561                                   6426                  1.58   \n",
       "1670                                   1887                  1.37   \n",
       "6087                                   3609                  1.55   \n",
       "6669                                   4028                  1.30   \n",
       "\n",
       "      debug_weather_factor  debug_incident_delay_seconds  debug_driver_factor  \\\n",
       "9254                   1.0                             0                 1.01   \n",
       "1561                   1.0                             0                 1.04   \n",
       "1670                   1.0                             0                 1.01   \n",
       "6087                   1.0                             0                 1.04   \n",
       "6669                   1.0                             0                 1.01   \n",
       "\n",
       "      trip_id  ...  destination  weather  day_of_week  vehicle_type  year  \\\n",
       "9254   773153  ...       313136        4            0             0  2026   \n",
       "1561   226377  ...       322853        0            0             0  2026   \n",
       "1670   650530  ...       340954        0            0             0  2026   \n",
       "6087    28751  ...       408974        0            0             0  2026   \n",
       "6669   376446  ...        15812        0            0             0  2026   \n",
       "\n",
       "      month  day  hour  minute  second  \n",
       "9254      1   19     8      56       2  \n",
       "1561      1   19     7      49      58  \n",
       "1670      1   19    13      25      52  \n",
       "6087      1   19     7      31      14  \n",
       "6669      1   19     3      41       8  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "estimated_distance_km                    float64\n",
       "temperature_celsius                      float64\n",
       "driver_rating                            float64\n",
       "hour_of_day                                int32\n",
       "initial_estimated_travel_time_seconds      int32\n",
       "debug_traffic_factor                     float64\n",
       "debug_weather_factor                     float64\n",
       "debug_incident_delay_seconds               int32\n",
       "debug_driver_factor                      float64\n",
       "trip_id                                    int64\n",
       "driver_id                                  int64\n",
       "vehicle_id                                 int64\n",
       "origin                                     int64\n",
       "destination                                int64\n",
       "weather                                    int64\n",
       "day_of_week                                int64\n",
       "vehicle_type                               int64\n",
       "year                                       int32\n",
       "month                                      int32\n",
       "day                                        int32\n",
       "hour                                       int32\n",
       "minute                                     int32\n",
       "second                                     int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation (LightGBM Primary, XGBoost Fallback)\n",
    "\n",
    "### LightGBM (Primary)\n",
    "Optimal for ETA prediction:\n",
    "- Fastest training (7x faster than XGBoost)\n",
    "- Lowest MAPE in regression benchmarks\n",
    "- Efficient histogram-based algorithm\n",
    "- Native categorical feature support\n",
    "\n",
    "### XGBoost (Fallback)\n",
    "For comparison and YellowBrick compatibility:\n",
    "- Well-documented, mature library\n",
    "- Strong community support\n",
    "\n",
    "### Optimal Hyperparameters (Research-Based)\n",
    "\n",
    "Based on 2024-2025 benchmarks for ETA prediction:\n",
    "\n",
    "| Parameter | LightGBM | XGBoost | Rationale |\n",
    "|-----------|----------|---------|------------|\n",
    "| learning_rate | 0.05 | 0.05 | Balance speed/accuracy |\n",
    "| n_estimators | 1000 | 1000 | With early stopping |\n",
    "| max_depth | 10 | 8 | LightGBM handles deeper |\n",
    "| num_leaves | 100 | - | < 2^max_depth |\n",
    "| subsample | 0.8 | 0.8 | Prevent overfitting |\n",
    "| colsample_bytree | 0.8 | 0.8 | Feature sampling |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def create_batch_model(\n",
    "    model_type: Literal[\"lightgbm\", \"xgboost\"] = \"lightgbm\",\n",
    "    cat_feature_names: list[str] | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create regressor optimized for ETA prediction.\n",
    "    \n",
    "    Args:\n",
    "        model_type: \"lightgbm\" (primary) or \"xgboost\" (fallback)\n",
    "        cat_feature_names: Names of categorical features (for LightGBM)\n",
    "    \n",
    "    Returns:\n",
    "        Configured regressor ready for training\n",
    "    \n",
    "    References:\n",
    "        - LightGBM docs: https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "        - XGBoost docs: https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "        - ETA research: LightGBM achieves lowest MAPE in travel time prediction\n",
    "    \"\"\"\n",
    "    if model_type == \"lightgbm\":\n",
    "        print(f\"Creating LGBMRegressor (primary model)\")\n",
    "        if cat_feature_names:\n",
    "            print(f\"  Categorical features: {cat_feature_names}\")\n",
    "        \n",
    "        # Optimized LightGBM parameters for ETA prediction\n",
    "        model = LGBMRegressor(\n",
    "            # Core parameters\n",
    "            n_estimators=1000,              # Max trees; early stopping finds optimal\n",
    "            learning_rate=0.05,             # Good balance for large datasets\n",
    "            max_depth=10,                   # LightGBM handles deeper trees well\n",
    "            num_leaves=100,                 # Should be < 2^max_depth (1024)\n",
    "            \n",
    "            # Regularization\n",
    "            min_child_samples=20,           # Minimum samples in leaf\n",
    "            reg_alpha=0.1,                  # L1 regularization\n",
    "            reg_lambda=0.1,                 # L2 regularization\n",
    "            min_gain_to_split=0.01,         # Minimum gain to make a split\n",
    "            \n",
    "            # Sampling\n",
    "            subsample=0.8,                  # Row sampling\n",
    "            colsample_bytree=0.8,           # Column sampling\n",
    "            subsample_freq=1,               # Frequency of subsample\n",
    "            \n",
    "            # Objective & Metric\n",
    "            objective='regression',\n",
    "            metric='rmse',\n",
    "            \n",
    "            # Boosting\n",
    "            boosting_type='gbdt',\n",
    "            \n",
    "            # Performance\n",
    "            n_jobs=-1,                      # Use all CPU cores\n",
    "            random_state=42,\n",
    "            verbose=1,                       # 1 = info level logging\n",
    "        )\n",
    "    \n",
    "    elif model_type == \"xgboost\":\n",
    "        print(f\"Creating XGBRegressor (fallback model)\")\n",
    "        \n",
    "        # Optimized XGBoost parameters for ETA prediction\n",
    "        model = XGBRegressor(\n",
    "            # Core parameters\n",
    "            n_estimators=1000,              # Max trees; early stopping finds optimal\n",
    "            learning_rate=0.05,             # Good balance\n",
    "            max_depth=8,                    # XGBoost prefers shallower\n",
    "            \n",
    "            # Regularization\n",
    "            min_child_weight=5,             # Minimum sum of instance weight\n",
    "            gamma=0.1,                      # Minimum loss reduction\n",
    "            reg_alpha=0.1,                  # L1 regularization\n",
    "            reg_lambda=1.0,                 # L2 regularization\n",
    "            \n",
    "            # Sampling\n",
    "            subsample=0.8,                  # Row sampling\n",
    "            colsample_bytree=0.8,           # Column sampling\n",
    "            \n",
    "            # Objective\n",
    "            objective='reg:squarederror',\n",
    "            \n",
    "            # Performance\n",
    "            tree_method='hist',             # Faster histogram-based\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LGBMRegressor (primary model)\n",
      "  Categorical features: ['trip_id', 'driver_id', 'vehicle_id', 'origin', 'destination', 'weather', 'day_of_week', 'vehicle_type', 'year', 'month', 'day', 'hour', 'minute', 'second']\n"
     ]
    }
   ],
   "source": [
    "# Create model using MODEL_TYPE defined earlier\n",
    "model = create_batch_model(\n",
    "    model_type=MODEL_TYPE,\n",
    "    cat_feature_names=ETA_CAT_FEATURE_NAMES if MODEL_TYPE == \"lightgbm\" else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Training with all-numeric features (label-encoded in DuckDB SQL).\n",
    "\n",
    "**LightGBM**: Pass `categorical_feature` parameter for native categorical handling.\n",
    "**XGBoost**: Works directly with integer-encoded categoricals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM with early stopping...\n",
      "Logging every iteration...\n",
      "\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.334921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3237\n",
      "[LightGBM] [Info] Number of data points in the train set: 8000, number of used features: 19\n",
      "[LightGBM] [Warning] min_gain_to_split is set=0.01, min_split_gain=0.0 will be ignored. Current value: min_gain_to_split=0.01\n",
      "[LightGBM] [Info] Start training from score 4554.302250\n",
      "[1]\tvalidation's rmse: 2398.31\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[2]\tvalidation's rmse: 2283.23\n"
     ]
    }
   ],
   "source": [
    "# Train model based on type\n",
    "if MODEL_TYPE == \"lightgbm\":\n",
    "    # LightGBM training with native categorical support and early stopping\n",
    "    from lightgbm import early_stopping, log_evaluation\n",
    "    \n",
    "    print(\"Training LightGBM with early stopping...\")\n",
    "    print(\"Logging every iteration...\")\n",
    "    print()\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_names=[\"validation\"],\n",
    "        categorical_feature=ETA_CAT_FEATURE_NAMES,\n",
    "        callbacks=[\n",
    "            early_stopping(stopping_rounds=50, verbose=True),\n",
    "            log_evaluation(period=1),  # Log EVERY iteration\n",
    "            #log_evaluation(period=100)\n",
    "        ],\n",
    "    )\n",
    "    print()\n",
    "    print(f\"Best iteration: {model.best_iteration_}\")\n",
    "    print(f\"Best score (RMSE): {model.best_score_['validation']['rmse']:.4f}\")\n",
    "else:\n",
    "    # XGBoost training with early stopping\n",
    "    print(\"Training XGBoost with early stopping...\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=1,  # Log every iteration\n",
    "    )\n",
    "    print(f\"Best iteration: {model.best_iteration}\")\n",
    "    print(f\"Best score: {model.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Compare LightGBM (primary) vs XGBoost (fallback) on this dataset.\n",
    "\n",
    "Run the notebook twice with `MODEL_TYPE = \"lightgbm\"` and `MODEL_TYPE = \"xgboost\"` to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# PRIMARY METRICS - Most important for ETA prediction\n",
    "# These are the industry-standard metrics for travel time estimation\n",
    "# -----------------------------------------------------------------------------\n",
    "primary_metric_functions = {\n",
    "    # MAE: Mean Absolute Error - average absolute difference in seconds\n",
    "    # Most interpretable: \"On average, predictions are off by X seconds\"\n",
    "    \"mean_absolute_error\": metrics.mean_absolute_error,\n",
    "    \n",
    "    # RMSE: Root Mean Squared Error - penalizes large errors more\n",
    "    # Critical for ETA: large errors are worse than many small ones\n",
    "    \"root_mean_squared_error\": metrics.root_mean_squared_error,\n",
    "    \n",
    "    # MAPE: Mean Absolute Percentage Error - relative error\n",
    "    # Industry standard for ETA: \"Predictions are off by X%\"\n",
    "    \"mean_absolute_percentage_error\": metrics.mean_absolute_percentage_error,\n",
    "    \n",
    "    # R2: Coefficient of Determination - explained variance ratio\n",
    "    # How much variance in travel time is explained by the model\n",
    "    \"r2_score\": metrics.r2_score,\n",
    "}\n",
    "primary_metric_args = {\n",
    "    \"mean_absolute_error\": {},\n",
    "    \"root_mean_squared_error\": {},\n",
    "    \"mean_absolute_percentage_error\": {},\n",
    "    \"r2_score\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# SECONDARY METRICS - Additional insights for regression\n",
    "# -----------------------------------------------------------------------------\n",
    "secondary_metric_functions = {\n",
    "    # MSE: Mean Squared Error - used internally for optimization\n",
    "    \"mean_squared_error\": metrics.mean_squared_error,\n",
    "    \n",
    "    # Median Absolute Error - robust to outliers\n",
    "    # Useful when there are extreme travel times (accidents, etc.)\n",
    "    \"median_absolute_error\": metrics.median_absolute_error,\n",
    "    \n",
    "    # Max Error - worst case prediction\n",
    "    # Important for ETA: what's the worst prediction we could make?\n",
    "    \"max_error\": metrics.max_error,\n",
    "    \n",
    "    # Explained Variance Score - similar to R2 but doesn't center\n",
    "    \"explained_variance_score\": metrics.explained_variance_score,\n",
    "}\n",
    "secondary_metric_args = {\n",
    "    \"mean_squared_error\": {},\n",
    "    \"median_absolute_error\": {},\n",
    "    \"max_error\": {},\n",
    "    \"explained_variance_score\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# LOGARITHMIC METRICS - For positive targets (travel time > 0)\n",
    "# Useful when relative errors matter more than absolute errors\n",
    "# -----------------------------------------------------------------------------\n",
    "logarithmic_metric_functions = {\n",
    "    # MSLE: Mean Squared Logarithmic Error\n",
    "    # Penalizes underestimates more than overestimates\n",
    "    # Good for ETA: better to overestimate than underestimate travel time\n",
    "    \"mean_squared_log_error\": metrics.mean_squared_log_error,\n",
    "    \n",
    "    # RMSLE: Root Mean Squared Logarithmic Error\n",
    "    \"root_mean_squared_log_error\": metrics.root_mean_squared_log_error,\n",
    "}\n",
    "logarithmic_metric_args = {\n",
    "    \"mean_squared_log_error\": {},\n",
    "    \"root_mean_squared_log_error\": {},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# DEVIANCE METRICS - For generalized linear models / distributional analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "deviance_metric_functions = {\n",
    "    # Mean Poisson Deviance - assumes count/positive data\n",
    "    \"mean_poisson_deviance\": metrics.mean_poisson_deviance,\n",
    "    \n",
    "    # Mean Gamma Deviance - assumes positive continuous data\n",
    "    # Good for travel times which are always positive\n",
    "    \"mean_gamma_deviance\": metrics.mean_gamma_deviance,\n",
    "    \n",
    "    # Mean Tweedie Deviance - generalized (power parameter)\n",
    "    # power=0: Normal, power=1: Poisson, power=2: Gamma\n",
    "    \"mean_tweedie_deviance\": metrics.mean_tweedie_deviance,\n",
    "}\n",
    "deviance_metric_args = {\n",
    "    \"mean_poisson_deviance\": {},\n",
    "    \"mean_gamma_deviance\": {},\n",
    "    \"mean_tweedie_deviance\": {\"power\": 1.5},  # Between Poisson and Gamma\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# QUANTILE METRICS - For quantile regression / prediction intervals\n",
    "# -----------------------------------------------------------------------------\n",
    "quantile_metric_functions = {\n",
    "    # Mean Pinball Loss (quantile loss) - for prediction intervals\n",
    "    # alpha=0.5 is equivalent to MAE (median regression)\n",
    "    \"mean_pinball_loss\": metrics.mean_pinball_loss,\n",
    "}\n",
    "quantile_metric_args = {\n",
    "    \"mean_pinball_loss\": {\"alpha\": 0.5},  # Median (50th percentile)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# D2 SCORE METRICS - Fraction of deviance explained (like R2 for deviance)\n",
    "# These are \"coefficient of determination\" analogs for different loss functions\n",
    "# -----------------------------------------------------------------------------\n",
    "d2_metric_functions = {\n",
    "    # D2 Absolute Error Score - fraction of absolute error explained\n",
    "    # Similar to R2 but based on MAE instead of MSE\n",
    "    \"d2_absolute_error_score\": metrics.d2_absolute_error_score,\n",
    "    \n",
    "    # D2 Pinball Score - fraction of pinball loss explained\n",
    "    \"d2_pinball_score\": metrics.d2_pinball_score,\n",
    "    \n",
    "    # D2 Tweedie Score - fraction of Tweedie deviance explained\n",
    "    \"d2_tweedie_score\": metrics.d2_tweedie_score,\n",
    "}\n",
    "d2_metric_args = {\n",
    "    \"d2_absolute_error_score\": {},\n",
    "    \"d2_pinball_score\": {\"alpha\": 0.5},\n",
    "    \"d2_tweedie_score\": {\"power\": 1.5},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Metrics Evaluation\n",
    "\n",
    "Sklearn regression metrics organized by category:\n",
    "\n",
    "| Category | Metrics | Use Case |\n",
    "|----------|---------|----------|\n",
    "| **Primary** | MAE, RMSE, MAPE, R2 | Core ETA evaluation |\n",
    "| **Secondary** | MSE, MedianAE, MaxError, EVS | Additional insights |\n",
    "| **Logarithmic** | MSLE, RMSLE | Relative errors, positive targets |\n",
    "| **Deviance** | Poisson, Gamma, Tweedie | Distributional analysis |\n",
    "| **Quantile** | Pinball Loss | Prediction intervals |\n",
    "| **D2 Scores** | D2_AE, D2_Pinball, D2_Tweedie | Fraction explained |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COMPUTE ALL METRICS\n",
    "# =============================================================================\n",
    "metrics_to_log = {}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# PRIMARY METRICS\n",
    "# -----------------------------------------------------------------------------\n",
    "for name, func in primary_metric_functions.items():\n",
    "    try:\n",
    "        metrics_to_log[name] = func(y_test, y_pred, **primary_metric_args[name])\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing {name}: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SECONDARY METRICS\n",
    "# -----------------------------------------------------------------------------\n",
    "for name, func in secondary_metric_functions.items():\n",
    "    try:\n",
    "        metrics_to_log[name] = func(y_test, y_pred, **secondary_metric_args[name])\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing {name}: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# LOGARITHMIC METRICS (require positive values)\n",
    "# -----------------------------------------------------------------------------\n",
    "# Clip predictions to positive values for log metrics\n",
    "y_pred_positive = np.maximum(y_pred, 1e-10)\n",
    "y_test_positive = np.maximum(y_test.values, 1e-10)\n",
    "\n",
    "for name, func in logarithmic_metric_functions.items():\n",
    "    try:\n",
    "        metrics_to_log[name] = func(y_test_positive, y_pred_positive, **logarithmic_metric_args[name])\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing {name}: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# DEVIANCE METRICS (require strictly positive values)\n",
    "# -----------------------------------------------------------------------------\n",
    "for name, func in deviance_metric_functions.items():\n",
    "    try:\n",
    "        metrics_to_log[name] = func(y_test_positive, y_pred_positive, **deviance_metric_args[name])\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing {name}: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# QUANTILE METRICS\n",
    "# -----------------------------------------------------------------------------\n",
    "for name, func in quantile_metric_functions.items():\n",
    "    try:\n",
    "        metrics_to_log[name] = func(y_test, y_pred, **quantile_metric_args[name])\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing {name}: {e}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# D2 SCORE METRICS\n",
    "# -----------------------------------------------------------------------------\n",
    "for name, func in d2_metric_functions.items():\n",
    "    try:\n",
    "        if \"tweedie\" in name or \"pinball\" in name:\n",
    "            metrics_to_log[name] = func(y_test_positive, y_pred_positive, **d2_metric_args[name])\n",
    "        else:\n",
    "            metrics_to_log[name] = func(y_test, y_pred, **d2_metric_args[name])\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing {name}: {e}\")\n",
    "\n",
    "metrics_to_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metrics in a readable format\n",
    "print(\"=\" * 60)\n",
    "print(f\"ETA PREDICTION METRICS ({MODEL_TYPE.upper()})\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- PRIMARY METRICS (Industry Standard for ETA) ---\")\n",
    "print(f\"MAE:  {metrics_to_log['mean_absolute_error']:.2f} seconds ({metrics_to_log['mean_absolute_error']/60:.2f} minutes)\")\n",
    "print(f\"RMSE: {metrics_to_log['root_mean_squared_error']:.2f} seconds ({metrics_to_log['root_mean_squared_error']/60:.2f} minutes)\")\n",
    "print(f\"MAPE: {metrics_to_log['mean_absolute_percentage_error']*100:.2f}%\")\n",
    "print(f\"R2:   {metrics_to_log['r2_score']:.4f}\")\n",
    "\n",
    "print(\"\\n--- SECONDARY METRICS ---\")\n",
    "print(f\"MSE:               {metrics_to_log['mean_squared_error']:.2f}\")\n",
    "print(f\"Median AE:         {metrics_to_log['median_absolute_error']:.2f} seconds\")\n",
    "print(f\"Max Error:         {metrics_to_log['max_error']:.2f} seconds ({metrics_to_log['max_error']/60:.2f} minutes)\")\n",
    "print(f\"Explained Var:     {metrics_to_log['explained_variance_score']:.4f}\")\n",
    "\n",
    "print(\"\\n--- LOGARITHMIC METRICS ---\")\n",
    "print(f\"MSLE:  {metrics_to_log['mean_squared_log_error']:.6f}\")\n",
    "print(f\"RMSLE: {metrics_to_log['root_mean_squared_log_error']:.6f}\")\n",
    "\n",
    "print(\"\\n--- DEVIANCE METRICS ---\")\n",
    "print(f\"Poisson Deviance:  {metrics_to_log['mean_poisson_deviance']:.4f}\")\n",
    "print(f\"Gamma Deviance:    {metrics_to_log['mean_gamma_deviance']:.6f}\")\n",
    "print(f\"Tweedie Deviance:  {metrics_to_log['mean_tweedie_deviance']:.4f}\")\n",
    "\n",
    "print(\"\\n--- QUANTILE & D2 METRICS ---\")\n",
    "print(f\"Pinball Loss (50%): {metrics_to_log['mean_pinball_loss']:.2f}\")\n",
    "print(f\"D2 Absolute Error:  {metrics_to_log['d2_absolute_error_score']:.4f}\")\n",
    "print(f\"D2 Pinball:         {metrics_to_log['d2_pinball_score']:.4f}\")\n",
    "print(f\"D2 Tweedie:         {metrics_to_log['d2_tweedie_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all model parameters\n",
    "print(\"\\nModel parameters:\")\n",
    "if MODEL_TYPE == \"lightgbm\":\n",
    "    all_params = model.get_params()\n",
    "else:\n",
    "    all_params = model.get_params()\n",
    "    \n",
    "for param, value in all_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YellowBrick\n",
    "\n",
    "YellowBrick visualizations for regression will be added in the next phase:\n",
    "\n",
    "| Category | Visualizers |\n",
    "|----------|-------------|\n",
    "| **Regression** | ResidualsPlot, PredictionError |\n",
    "| **Feature Analysis** | Rank1D, Rank2D, PCA, Manifold, ParallelCoordinates |\n",
    "| **Target** | FeatureCorrelation |\n",
    "| **Model Selection** | FeatureImportances, LearningCurve, ValidationCurve |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
