{
  "project": "Transaction Fraud Detection",
  "model": "ARFClassifier",
  "task": "Binary Classification",
  "metrics": {
    "fbeta": {
      "name": "F-Beta Score",
      "formula": "$$F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}$$",
      "explanation": "Weighted harmonic mean of Precision and Recall. The \u03b2 parameter controls the trade-off: \u03b2 > 1 favors Recall, \u03b2 < 1 favors Precision.",
      "context": "With **\u03b2=2**, FBeta prioritizes **Recall 2x more than Precision**. This is ideal for fraud detection where **missing a fraud (False Negative) is more costly** than a false alarm (False Positive). A blocked legitimate transaction can be resolved, but undetected fraud causes direct financial loss.",
      "range": "0 to 1",
      "optimal": "Higher is better",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/FBeta/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html"
      }
    },
    "rocauc": {
      "name": "ROC AUC",
      "formula": "$$\\text{AUC} = \\int_0^1 \\text{TPR}(\\text{FPR}^{-1}(x)) \\, dx = P(\\text{score}_{\\text{fraud}} > \\text{score}_{\\text{legit}})$$",
      "explanation": "Area Under the Receiver Operating Characteristic curve. Measures the model's ability to distinguish between classes across all probability thresholds.",
      "context": "Shows how well the model **separates fraud from legitimate transactions** regardless of the decision threshold. A score of **0.5 = random guessing**, **1.0 = perfect separation**. Industry standard for fraud detection is **> 0.90**.",
      "range": "0 to 1",
      "optimal": "Higher is better",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/ROCAUC/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html"
      }
    },
    "precision": {
      "name": "Precision",
      "formula": "$$\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{\\text{True Positives}}{\\text{All Predicted Positives}}$$",
      "explanation": "Of all transactions flagged as fraud, what proportion were actually fraud? Measures the accuracy of positive predictions.",
      "context": "High precision means **fewer false alarms** - when the model says 'fraud', it's usually right. Important for **customer experience** since too many false positives lead to frustrated customers with blocked legitimate transactions.",
      "range": "0 to 1",
      "optimal": "Higher is better",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/Precision/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html"
      }
    },
    "recall": {
      "name": "Recall (Sensitivity)",
      "formula": "$$\\text{Recall} = \\frac{TP}{TP + FN} = \\frac{\\text{True Positives}}{\\text{All Actual Positives}}$$",
      "explanation": "Of all actual fraud cases, what proportion did the model catch? Also known as Sensitivity or True Positive Rate (TPR).",
      "context": "High recall means **catching more fraud** - the model misses fewer fraudulent transactions. **Critical for fraud detection** since a missed fraud (False Negative) results in direct financial loss to the company or customer.",
      "range": "0 to 1",
      "optimal": "Higher is better",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/Recall/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html"
      }
    },
    "rolling_rocauc": {
      "name": "Rolling ROC AUC",
      "formula": "$$\\text{Rolling AUC}_t = \\text{AUC}(\\{(y_i, \\hat{p}_i) : i \\in [t-w, t]\\})$$",
      "explanation": "ROC AUC calculated over a sliding window of the most recent samples. Used to detect concept drift - when the data distribution changes over time.",
      "context": "With **window_size=5000**, monitors the model's recent discrimination ability. A **sudden drop** in Rolling AUC indicates **concept drift** - fraud patterns may have changed and the model needs attention. Essential for **real-time fraud detection** where patterns evolve.",
      "range": "0 to 1",
      "optimal": "Higher and stable is better",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/ROCAUC/"
      }
    },
    "mcc": {
      "name": "Matthews Correlation Coefficient",
      "formula": "$$\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$",
      "explanation": "Correlation coefficient between observed and predicted classifications. Considered one of the best metrics for imbalanced datasets as it uses all four confusion matrix values.",
      "context": "**Best single metric for imbalanced fraud data**. Unlike accuracy, MCC is not fooled by class imbalance. A value of **+1 = perfect**, **0 = random**, **-1 = total disagreement**. Preferred by researchers for fraud detection evaluation.",
      "range": "-1 to 1",
      "optimal": "Higher is better (closer to +1)",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/MCC/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html"
      }
    },
    "balanced_accuracy": {
      "name": "Balanced Accuracy",
      "formula": "$$\\text{Balanced Accuracy} = \\frac{1}{2}\\left(\\frac{TP}{TP+FN} + \\frac{TN}{TN+FP}\\right) = \\frac{\\text{Sensitivity} + \\text{Specificity}}{2}$$",
      "explanation": "Average of Recall (Sensitivity) for each class. Accounts for class imbalance by giving equal weight to both classes.",
      "context": "Since fraud is rare (typically **1-5% of transactions**), regular accuracy can be misleading. Balanced Accuracy ensures both **fraud detection (Sensitivity)** and **legitimate transaction handling (Specificity)** are weighted equally.",
      "range": "0 to 1",
      "optimal": "Higher is better",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/BalancedAccuracy/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html"
      }
    },
    "f1": {
      "name": "F1 Score",
      "formula": "$$F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}$$",
      "explanation": "Harmonic mean of Precision and Recall with equal weight (\u03b2=1). Balances the trade-off between catching fraud and avoiding false alarms.",
      "context": "F1 gives **equal importance to Precision and Recall**. For fraud detection, we typically prefer **FBeta with \u03b2=2** since catching fraud is more important than avoiding false alarms. F1 is useful as a reference baseline.",
      "range": "0 to 1",
      "optimal": "Higher is better",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/F1/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
      }
    },
    "accuracy": {
      "name": "Accuracy",
      "formula": "$$\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{\\text{Correct Predictions}}{\\text{Total Predictions}}$$",
      "explanation": "Proportion of all predictions that are correct. Simple but can be misleading for imbalanced datasets.",
      "context": "**Caution:** With 99% legitimate transactions, a model predicting 'not fraud' always would have 99% accuracy but **catch zero fraud**. Use with other metrics like MCC, F1, or Balanced Accuracy for fraud detection.",
      "range": "0 to 1",
      "optimal": "Higher is better (but interpret carefully)",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/Accuracy/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"
      }
    },
    "geometric_mean": {
      "name": "Geometric Mean",
      "formula": "$$G\\text{-}Mean = \\sqrt{\\text{Sensitivity} \\cdot \\text{Specificity}} = \\sqrt{\\frac{TP}{TP+FN} \\cdot \\frac{TN}{TN+FP}}$$",
      "explanation": "Geometric mean of Sensitivity (Recall) and Specificity. Zero if either metric is zero, making it strict about balanced performance.",
      "context": "Ensures the model performs well on **both fraud and legitimate transactions**. A high G-Mean means the model doesn't sacrifice one class for the other - important when both fraud detection and customer experience matter.",
      "range": "0 to 1",
      "optimal": "Higher is better",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/GeometricMean/",
        "batch": "https://imbalanced-learn.org/stable/references/generated/imblearn.metrics.geometric_mean_score.html"
      }
    },
    "cohen_kappa": {
      "name": "Cohen's Kappa",
      "formula": "$$\\kappa = \\frac{p_o - p_e}{1 - p_e}$$",
      "explanation": "Measures agreement between predictions and actual values, adjusted for chance. p_o = observed agreement, p_e = expected agreement by chance.",
      "context": "Indicates how much better the model performs **compared to random chance**. Interpretation: **< 0 = worse than random**, **0-0.2 = slight**, **0.2-0.4 = fair**, **0.4-0.6 = moderate**, **0.6-0.8 = substantial**, **> 0.8 = almost perfect**.",
      "range": "-1 to 1",
      "optimal": "Higher is better (closer to +1)",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/CohenKappa/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.cohen_kappa_score.html"
      }
    },
    "jaccard": {
      "name": "Jaccard Index",
      "formula": "$$J = \\frac{TP}{TP + FP + FN} = \\frac{|\\text{Predicted Fraud} \\cap \\text{Actual Fraud}|}{|\\text{Predicted Fraud} \\cup \\text{Actual Fraud}|}$$",
      "explanation": "Intersection over Union (IoU) for the positive class. Measures overlap between predicted and actual fraud cases.",
      "context": "Also known as the **Jaccard Similarity Coefficient**. For fraud detection, it measures how well the predicted fraud set overlaps with actual fraud. More intuitive than F1 for understanding **set overlap**.",
      "range": "0 to 1",
      "optimal": "Higher is better",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/Jaccard/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html"
      }
    },
    "logloss": {
      "name": "Log Loss (Cross-Entropy)",
      "formula": "$$\\text{LogLoss} = -\\frac{1}{N}\\sum_{i=1}^{N}[y_i \\log(p_i) + (1-y_i)\\log(1-p_i)]$$",
      "explanation": "Measures the quality of probability predictions. Penalizes confident wrong predictions more heavily than uncertain ones.",
      "context": "Evaluates **probability calibration** - how well the predicted fraud probabilities match actual fraud rates. **Lower is better**. Random baseline = 0.693 (ln 2). Target: **< 0.1** for well-calibrated models. Important for **risk scoring** where probability thresholds matter.",
      "range": "0 to infinity",
      "optimal": "Lower is better",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/CrossEntropy/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html"
      }
    },
    "average_precision": {
      "name": "Average Precision (PR-AUC)",
      "formula": "$$\\text{AP} = \\sum_{n} (R_n - R_{n-1}) P_n = \\int_0^1 P(R) \\, dR$$",
      "explanation": "Area under the Precision-Recall curve. Summarizes the trade-off between precision and recall across all thresholds.",
      "context": "**Better than ROC-AUC for imbalanced data** like fraud detection. Focuses on the positive class (fraud) performance. Values: **0.5 = random** for balanced data, but **much lower baseline** for imbalanced data. Target: **> 0.5** is good for rare fraud.",
      "range": "0 to 1",
      "optimal": "Higher is better",
      "docs_url": {
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html"
      }
    },
    "brier": {
      "name": "Brier Score",
      "formula": "$$\\text{Brier} = \\frac{1}{N}\\sum_{i=1}^{N}(p_i - y_i)^2$$",
      "explanation": "Mean squared error between predicted probabilities and actual outcomes. Measures both calibration and discrimination.",
      "context": "Evaluates **probability accuracy** - how close predicted fraud probabilities are to 0/1 outcomes. **Lower is better**. Perfect = 0, worst = 1. For fraud detection, helps assess if **probability scores are reliable** for risk-based decisions.",
      "range": "0 to 1",
      "optimal": "Lower is better",
      "docs_url": {
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html"
      }
    },
    "d2_logloss": {
      "name": "D\u00b2 Log Loss Score",
      "formula": "$$D^2 = 1 - \\frac{\\text{LogLoss}_{\\text{model}}}{\\text{LogLoss}_{\\text{null}}}$$",
      "explanation": "Fraction of log loss explained by the model compared to a null model (predicting class frequencies). Similar to R\u00b2 but for log loss.",
      "context": "Shows **how much better** the model's probabilities are vs always predicting the fraud rate. **1.0 = perfect**, **0 = no better than null**, **negative = worse than null**. Useful for comparing models on the same dataset.",
      "range": "-infinity to 1",
      "optimal": "Higher is better (closer to 1)",
      "docs_url": {
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.d2_log_loss_score.html"
      }
    },
    "d2_brier": {
      "name": "D\u00b2 Brier Score",
      "formula": "$$D^2 = 1 - \\frac{\\text{Brier}_{\\text{model}}}{\\text{Brier}_{\\text{null}}}$$",
      "explanation": "Fraction of Brier score explained by the model compared to a null model. Similar to R\u00b2 but for Brier score.",
      "context": "Measures **improvement over baseline** probability predictions. **1.0 = perfect**, **0 = no better than predicting fraud rate**, **negative = worse**. More interpretable than raw Brier score for model comparison.",
      "range": "-infinity to 1",
      "optimal": "Higher is better (closer to 1)",
      "docs_url": {
        "batch": "https://scikit-learn.org/stable/modules/model_evaluation.html#d2-score-the-coefficient-of-determination"
      }
    },
    "confusion_matrix": {
      "name": "Confusion Matrix",
      "formula": "$$\\begin{bmatrix} TN & FP \\\\ FN & TP \\end{bmatrix} = \\begin{bmatrix} \\text{Correct Legit} & \\text{False Alarm} \\\\ \\text{Missed Fraud} & \\text{Caught Fraud} \\end{bmatrix}$$",
      "explanation": "2x2 matrix showing all prediction outcomes: True Negatives (TN), False Positives (FP), False Negatives (FN), True Positives (TP).",
      "context": "**Foundation of all classification metrics**. For fraud detection: **TP** = correctly caught fraud, **TN** = correctly approved legitimate, **FP** = false alarm (blocked legitimate), **FN** = missed fraud (most costly error).",
      "range": "Counts (0 to N)",
      "optimal": "High TP and TN, Low FP and FN",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/ConfusionMatrix/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html"
      }
    },
    "classification_report": {
      "name": "Classification Report",
      "formula": "Per-class: Precision, Recall, F1-Score, Support",
      "explanation": "Comprehensive per-class breakdown showing Precision, Recall, and F1 for each class (fraud and legitimate), plus support (sample count).",
      "context": "Shows **detailed performance per class**. For fraud detection, focus on **Class 1 (Fraud)** metrics since that's what we're trying to catch. Support shows **class imbalance** - typically 95-99% legitimate (Class 0) vs 1-5% fraud (Class 1).",
      "range": "0 to 1 for metrics, counts for support",
      "optimal": "High Precision/Recall/F1 for fraud class",
      "docs_url": {
        "incremental": "https://riverml.xyz/latest/api/metrics/ClassificationReport/",
        "batch": "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
      }
    }
  }
}
