{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e38bad0",
   "metadata": {},
   "source": [
    "# Rewriting the processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b13a4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'account_age_days': 370,\n",
      " 'amount': 302.69,\n",
      " 'billing_address_match': True,\n",
      " 'currency': 'BRL',\n",
      " 'cvv_provided': True,\n",
      " 'device_info': {'browser': 'Opera', 'os': 'Windows'},\n",
      " 'ip_address': '169.235.63.28',\n",
      " 'is_fraud': 0,\n",
      " 'location': {'lat': -68.4965105, 'lon': -153.515477},\n",
      " 'merchant_id': 'merchant_65',\n",
      " 'payment_method': 'debit_card',\n",
      " 'product_category': 'luxury_items',\n",
      " 'timestamp': '2025-04-17T19:52:06.994066+00:00',\n",
      " 'transaction_id': 'ffd3d366-06e4-4ddb-894e-03876e893079',\n",
      " 'transaction_type': 'deposit',\n",
      " 'user_agent': 'Mozilla/5.0 (Windows; U; Windows NT 11.0) AppleWebKit/532.16.3 '\n",
      "               '(KHTML, like Gecko) Version/4.1 Safari/532.16.3',\n",
      " 'user_id': '61fa227e-d309-4ed0-b513-3cffa5526463'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "x = {\n",
    "    'transaction_id': 'ffd3d366-06e4-4ddb-894e-03876e893079', \n",
    "    'user_id': '61fa227e-d309-4ed0-b513-3cffa5526463', \n",
    "    'timestamp': '2025-04-17T19:52:06.994066+00:00', \n",
    "    'amount': 302.69, \n",
    "    'currency': 'BRL', \n",
    "    'merchant_id': 'merchant_65', \n",
    "    'product_category': 'luxury_items', \n",
    "    'transaction_type': 'deposit', \n",
    "    'payment_method': 'debit_card', \n",
    "    'location': {'lat': -68.4965105, 'lon': -153.515477}, \n",
    "    'ip_address': '169.235.63.28', \n",
    "    'device_info': {'os': 'Windows', 'browser': 'Opera'}, \n",
    "    'user_agent': 'Mozilla/5.0 (Windows; U; Windows NT 11.0) AppleWebKit/532.16.3 (KHTML, like Gecko) Version/4.1 Safari/532.16.3', \n",
    "    'account_age_days': 370, \n",
    "    'cvv_provided': True, \n",
    "    'billing_address_match': True, \n",
    "    'is_fraud': 0}\n",
    "\n",
    "pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430b5fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial encoder state: CustomPicklableOrdinalEncoder(features=0 [])\n",
      "\n",
      "Learning from sample dictionaries:\n",
      "Learning sample: {'color': 'red', 'size': 'M', 'city': 'NY'}\n",
      "Learning sample: {'color': 'blue', 'size': 'L', 'city': 'London'}\n",
      "Learning sample: {'color': 'red', 'size': 'S', 'city': 'NY'}\n",
      "Learning sample: {'color': 'green', 'size': 'M', 'city': 'Paris'}\n",
      "Learning sample: {'color': 'blue', 'size': 'L', 'city': 'NY'}\n",
      "\n",
      "Encoder state after learning: CustomPicklableOrdinalEncoder(features=3 [color: 3 categories, size: 3 categories, city: 3 categories])\n",
      "Learned feature mappings: {'color': {'red': 0, 'blue': 1, 'green': 2}, 'size': {'M': 0, 'L': 1, 'S': 2}, 'city': {'NY': 0, 'London': 1, 'Paris': 2}}\n",
      "Next IDs per feature: {'color': 3, 'size': 3, 'city': 3}\n",
      "\n",
      "Transforming sample dictionaries:\n",
      "Original: {'color': 'red', 'size': 'M', 'city': 'NY'}\n",
      "Transformed: {'color': 0, 'size': 0, 'city': 0}\n",
      "Original: {'color': 'blue', 'size': 'L', 'city': 'London'}\n",
      "Transformed: {'color': 1, 'size': 1, 'city': 1}\n",
      "Original: {'color': 'red', 'size': 'S', 'city': 'NY'}\n",
      "Transformed: {'color': 0, 'size': 2, 'city': 0}\n",
      "Original: {'color': 'green', 'size': 'M', 'city': 'Paris'}\n",
      "Transformed: {'color': 2, 'size': 0, 'city': 2}\n",
      "Original: {'color': 'blue', 'size': 'L', 'city': 'NY'}\n",
      "Transformed: {'color': 1, 'size': 1, 'city': 0}\n",
      "\n",
      "Saving encoder to 'custom_ordinal_encoder.pkl'...\n",
      "Encoder saved successfully.\n",
      "\n",
      "Loading encoder from 'custom_ordinal_encoder.pkl'...\n",
      "Encoder loaded successfully.\n",
      "Loaded encoder state: CustomPicklableOrdinalEncoder(features=3 [color: 3 categories, size: 3 categories, city: 3 categories])\n",
      "Loaded feature mappings: {'color': {'red': 0, 'blue': 1, 'green': 2}, 'size': {'M': 0, 'L': 1, 'S': 2}, 'city': {'NY': 0, 'London': 1, 'Paris': 2}}\n",
      "\n",
      "Transforming data points using loaded encoder:\n",
      "Original: {'color': 'red', 'size': 'M', 'city': 'NY'}\n",
      "Transformed (loaded encoder): {'color': 0, 'size': 0, 'city': 0}\n",
      "Original: {'color': 'blue', 'size': 'L', 'city': 'London'}\n",
      "Transformed (loaded encoder): {'color': 1, 'size': 1, 'city': 1}\n",
      "Original: {'color': 'red', 'size': 'S', 'city': 'NY'}\n",
      "Transformed (loaded encoder): {'color': 0, 'size': 2, 'city': 0}\n",
      "Original: {'color': 'green', 'size': 'M', 'city': 'Paris'}\n",
      "Transformed (loaded encoder): {'color': 2, 'size': 0, 'city': 2}\n",
      "Original: {'color': 'blue', 'size': 'L', 'city': 'NY'}\n",
      "Transformed (loaded encoder): {'color': 1, 'size': 1, 'city': 0}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from typing import Any, Dict, Hashable\n",
    "\n",
    "# Define the path for saving/loading the encoder state\n",
    "ENCODER_FILE = \"custom_ordinal_encoder.pkl\"\n",
    "\n",
    "class CustomPicklableOrdinalEncoder:\n",
    "    \"\"\"\n",
    "    An incremental ordinal encoder that is picklable and processes dictionaries.\n",
    "    Assigns a unique integer ID to each unique category encountered for each feature.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Dictionary to store mappings for each feature.\n",
    "        # Keys are feature names (from input dictionary), values are dictionaries\n",
    "        # mapping category value to integer ID for that feature.\n",
    "        self._feature_mappings: Dict[Hashable, Dict[Any, int]] = {}\n",
    "        # Dictionary to store the next available integer ID for each feature.\n",
    "        # Keys are feature names, values are integers.\n",
    "        self._feature_next_ids: Dict[Hashable, int] = {}\n",
    "    def learn_one(self, x: Dict[Hashable, Any]):\n",
    "        \"\"\"\n",
    "        Learns categories from a single sample dictionary.\n",
    "        Iterates through the dictionary's items and learns each category value\n",
    "        for its corresponding feature.\n",
    "        Args:\n",
    "            x: A dictionary representing a single sample.\n",
    "               Keys are feature names, values are feature values.\n",
    "               Assumes categorical features are present in this dictionary.\n",
    "        \"\"\"\n",
    "        for feature_name, category_value in x.items():\n",
    "            # Ensure the category value is hashable (dictionaries/lists are not)\n",
    "            # You might need more sophisticated type checking or handling\n",
    "            # if your input dictionaries contain complex unhashable types\n",
    "            if not isinstance(category_value, Hashable):\n",
    "                 print(f\"Warning: Skipping unhashable value for feature '{feature_name}': {category_value}\")\n",
    "                 continue # Skip this feature for learning\n",
    "            # If this is the first time we see this feature, initialize its mapping and counter\n",
    "            if feature_name not in self._feature_mappings:\n",
    "                self._feature_mappings[feature_name] = {}\n",
    "                self._feature_next_ids[feature_name] = 0\n",
    "            # Get the mapping and counter for this specific feature\n",
    "            feature_map = self._feature_mappings[feature_name]\n",
    "            feature_next_id = self._feature_next_ids[feature_name]\n",
    "            # Check if the category value is already in the mapping for this feature\n",
    "            if category_value not in feature_map:\n",
    "                # If it's a new category for this feature, assign the next available ID\n",
    "                feature_map[category_value] = feature_next_id\n",
    "                # Increment the counter for the next new category for this feature\n",
    "                self._feature_next_ids[feature_name] += 1\n",
    "    def transform_one(self, x: Dict[Hashable, Any]) -> Dict[Hashable, int]:\n",
    "        \"\"\"\n",
    "        Transforms categorical features in a single sample dictionary into integer IDs.\n",
    "        Args:\n",
    "            x: A dictionary representing a single sample.\n",
    "               Keys are feature names, values are feature values.\n",
    "        Returns:\n",
    "            A new dictionary containing the transformed integer IDs for the\n",
    "            categorical features that the encoder has seen. Features not\n",
    "            seen by the encoder are excluded from the output dictionary.\n",
    "        Raises:\n",
    "            KeyError: If a feature is seen but a specific category value\n",
    "                      within that feature has not been seen during learning.\n",
    "                      You might want to add logic here to handle unseen categories\n",
    "                      (e.g., return a default value like -1 or NaN for that feature).\n",
    "        \"\"\"\n",
    "        transformed_sample: Dict[Hashable, int] = {}\n",
    "        for feature_name, category_value in x.items():\n",
    "            # Only attempt to transform features that the encoder has seen\n",
    "            if feature_name in self._feature_mappings:\n",
    "                feature_map = self._feature_mappings[feature_name]\n",
    "\n",
    "                # Check if the category value for this feature has been seen\n",
    "                if category_value in feature_map:\n",
    "                    # Transform the category value using the feature's mapping\n",
    "                    transformed_sample[feature_name] = feature_map[category_value]\n",
    "                else:\n",
    "                    # Handle unseen category values for a known feature\n",
    "                    # By default, this will raise a KeyError as per the docstring.\n",
    "                    # Example: return a placeholder value instead of raising error:\n",
    "                    # transformed_sample[feature_name] = -1 # Or some other indicator\n",
    "                    # print(f\"Warning: Unseen category '{category_value}' for feature '{feature_name}' during transform.\")\n",
    "                    # Or raise the error explicitly:\n",
    "                    raise KeyError(f\"Unseen category '{category_value}' for feature '{feature_name}' during transform.\")\n",
    "            # Features not in self._feature_mappings are ignored in the output.\n",
    "            # If you need to include them (e.g., original numerical features),\n",
    "            # you would copy them over here. This encoder only outputs encoded features.\n",
    "        return transformed_sample\n",
    "    def get_feature_mappings(self) -> Dict[Hashable, Dict[Any, int]]:\n",
    "        \"\"\"Returns the current mappings for all features.\"\"\"\n",
    "        return self._feature_mappings\n",
    "    def get_feature_next_ids(self) -> Dict[Hashable, int]:\n",
    "        \"\"\"Returns the next available IDs for all features.\"\"\"\n",
    "        return self._feature_next_ids\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"String representation of the encoder.\"\"\"\n",
    "        num_features = len(self._feature_mappings)\n",
    "        feature_details = \", \".join([f\"{name}: {len(mapping)} categories\" for name, mapping in self._feature_mappings.items()])\n",
    "        return f\"CustomPicklableOrdinalEncoder(features={num_features} [{feature_details}])\"\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# 1. Create an instance of the custom encoder\n",
    "custom_encoder = CustomPicklableOrdinalEncoder()\n",
    "print(f\"Initial encoder state: {custom_encoder}\")\n",
    "\n",
    "# 2. Learn from some sample dictionaries\n",
    "sample_data = [\n",
    "    {'color': 'red', 'size': 'M', 'city': 'NY'},\n",
    "    {'color': 'blue', 'size': 'L', 'city': 'London'},\n",
    "    {'color': 'red', 'size': 'S', 'city': 'NY'},\n",
    "    {'color': 'green', 'size': 'M', 'city': 'Paris'},\n",
    "    {'color': 'blue', 'size': 'L', 'city': 'NY'},\n",
    "]\n",
    "\n",
    "print(\"\\nLearning from sample dictionaries:\")\n",
    "for sample in sample_data:\n",
    "    print(f\"Learning sample: {sample}\")\n",
    "    custom_encoder.learn_one(sample)\n",
    "\n",
    "print(f\"\\nEncoder state after learning: {custom_encoder}\")\n",
    "print(f\"Learned feature mappings: {custom_encoder.get_feature_mappings()}\")\n",
    "print(f\"Next IDs per feature: {custom_encoder.get_feature_next_ids()}\")\n",
    "\n",
    "\n",
    "# 3. Transform data points\n",
    "print(\"\\nTransforming sample dictionaries:\")\n",
    "transformed_samples = [custom_encoder.transform_one(sample) for sample in sample_data]\n",
    "for i, sample in enumerate(sample_data):\n",
    "    print(f\"Original: {sample}\")\n",
    "    print(f\"Transformed: {transformed_samples[i]}\")\n",
    "\n",
    "# Example of transforming a sample with an unseen category for a known feature (will raise KeyError)\n",
    "# unseen_sample = {'color': 'yellow', 'size': 'S', 'city': 'NY'}\n",
    "# try:\n",
    "#     print(\"\\nAttempting to transform sample with unseen category 'yellow':\")\n",
    "#     custom_encoder.transform_one(unseen_sample)\n",
    "# except KeyError as e:\n",
    "#     print(f\"Caught expected error: {e}\")\n",
    "\n",
    "# Example of transforming a sample with a new feature (the new feature is ignored)\n",
    "# new_feature_sample = {'color': 'red', 'size': 'M', 'material': 'wood'}\n",
    "# print(\"\\nAttempting to transform sample with new feature 'material':\")\n",
    "# transformed_new_feature = custom_encoder.transform_one(new_feature_sample)\n",
    "# print(f\"Original: {new_feature_sample}\")\n",
    "# print(f\"Transformed: {transformed_new_feature}\") # 'material' is not in the output\n",
    "\n",
    "\n",
    "# 4. Save the encoder using pickle\n",
    "print(f\"\\nSaving encoder to '{ENCODER_FILE}'...\")\n",
    "try:\n",
    "    with open(ENCODER_FILE, \"wb\") as f:\n",
    "        pickle.dump(custom_encoder, f)\n",
    "    print(\"Encoder saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving encoder: {e}\")\n",
    "\n",
    "# 5. Load the encoder using pickle\n",
    "loaded_encoder = None\n",
    "if os.path.exists(ENCODER_FILE):\n",
    "    print(f\"\\nLoading encoder from '{ENCODER_FILE}'...\")\n",
    "    try:\n",
    "        with open(ENCODER_FILE, \"rb\") as f:\n",
    "            loaded_encoder = pickle.load(f)\n",
    "        print(\"Encoder loaded successfully.\")\n",
    "        print(f\"Loaded encoder state: {loaded_encoder}\")\n",
    "        print(f\"Loaded feature mappings: {loaded_encoder.get_feature_mappings()}\")\n",
    "\n",
    "        # Verify the loaded encoder works\n",
    "        print(\"\\nTransforming data points using loaded encoder:\")\n",
    "        transformed_data_loaded = [loaded_encoder.transform_one(sample) for sample in sample_data]\n",
    "        for i, sample in enumerate(sample_data):\n",
    "            print(f\"Original: {sample}\")\n",
    "            print(f\"Transformed (loaded encoder): {transformed_data_loaded[i]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading encoder: {e}\")\n",
    "    # finally:\n",
    "        # Clean up the saved file\n",
    "        # os.remove(ENCODER_FILE)\n",
    "        # print(f\"\\nCleaned up '{ENCODER_FILE}'.\")\n",
    "        # pass # Keep the file for inspection if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d402684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'account_age_days': 370,\n",
      " 'amount': 302.69,\n",
      " 'billing_address_match': True,\n",
      " 'cvv_provided': True}\n"
     ]
    }
   ],
   "source": [
    "from river import compose\n",
    "\n",
    "pipe1 = compose.Select(\n",
    "    \"amount\",\n",
    "    \"account_age_days\",\n",
    "    \"cvv_provided\",\n",
    "    \"billing_address_match\"\n",
    ")\n",
    "\n",
    "x_pipe_1 = pipe1.transform_one(x)\n",
    "pprint(x_pipe_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f75fc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'currency': 0,\n",
      " 'merchant_id': 0,\n",
      " 'payment_method': 0,\n",
      " 'product_category': 0,\n",
      " 'transaction_type': 0,\n",
      " 'user_agent': 0}\n"
     ]
    }
   ],
   "source": [
    "from river import preprocessing\n",
    "\n",
    "pipe2a = compose.Select(\n",
    "    \"currency\",\n",
    "    \"merchant_id\",\n",
    "    \"payment_method\",\n",
    "    \"product_category\",\n",
    "    \"transaction_type\",\n",
    "    \"user_agent\"\n",
    ")\n",
    "\n",
    "pipe2a.learn_one(x)\n",
    "x_pipe_2 = pipe2a.transform_one(x)\n",
    "\n",
    "#pipe2b = preprocessing.OrdinalEncoder()\n",
    "pipe2b = CustomPicklableOrdinalEncoder()\n",
    "\n",
    "pipe2b.learn_one(x_pipe_2)\n",
    "x_pipe_2 = pipe2b.transform_one(x_pipe_2)\n",
    "\n",
    "pprint(x_pipe_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c067d83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product_category': {'luxury_items': 0},\n",
       " 'user_agent': {'Mozilla/5.0 (Windows; U; Windows NT 11.0) AppleWebKit/532.16.3 (KHTML, like Gecko) Version/4.1 Safari/532.16.3': 0},\n",
       " 'merchant_id': {'merchant_65': 0},\n",
       " 'transaction_type': {'deposit': 0},\n",
       " 'currency': {'BRL': 0},\n",
       " 'payment_method': {'debit_card': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2b.get_feature_mappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5bc2694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "test = dt.datetime.strptime(\n",
    "        x['timestamp'],\n",
    "        \"%Y-%m-%dT%H:%M:%S.%f%z\")#.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "test.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7fc11d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_device_info(x):\n",
    "    x_ = x['device_info']\n",
    "    return {\n",
    "        'os': x_['os'],\n",
    "        'browser': x_['browser'],\n",
    "    }\n",
    "\n",
    "pipe3a = compose.Select(\n",
    "    \"device_info\",\n",
    ")\n",
    "\n",
    "pipe3a.learn_one(x)\n",
    "x_pipe_3 = pipe3a.transform_one(x)\n",
    "\n",
    "pipe3b = compose.FuncTransformer(\n",
    "    extract_device_info,\n",
    ")\n",
    "\n",
    "pipe3b.learn_one(x_pipe_3)\n",
    "x_pipe_3 = pipe3b.transform_one(x_pipe_3)\n",
    "\n",
    "#pipe3c = preprocessing.OrdinalEncoder()\n",
    "pipe3c = CustomPicklableOrdinalEncoder()\n",
    "\n",
    "pipe3c.learn_one(x_pipe_3)\n",
    "x_pipe_3 = pipe3c.transform_one(x_pipe_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5635bd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'os': {'Windows': 0}, 'browser': {'Opera': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe3c.get_feature_mappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67c6825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'day': 17, 'hour': 19, 'minute': 52, 'month': 4, 'second': 6, 'year': 2025}\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def extract_timestamp_info(x):\n",
    "    x_ = dt.datetime.strptime(\n",
    "        x['timestamp'],\n",
    "        \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "    return {\n",
    "        'year': x_.year,\n",
    "        'month': x_.month,\n",
    "        'day': x_.day,\n",
    "        'hour': x_.hour,\n",
    "        'minute': x_.minute,\n",
    "        'second': x_.second\n",
    "    }\n",
    "\n",
    "pipe4a = compose.Select(\n",
    "    \"timestamp\",\n",
    ")\n",
    "\n",
    "pipe4a.learn_one(x)\n",
    "x_pipe_4 = pipe4a.transform_one(x)\n",
    "\n",
    "pipe4b = compose.FuncTransformer(\n",
    "    extract_timestamp_info,\n",
    ")\n",
    "\n",
    "pipe4b.learn_one(x_pipe_4)\n",
    "x_pipe_4 = pipe4b.transform_one(x_pipe_4)\n",
    "\n",
    "pprint(x_pipe_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5a05ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'account_age_days': 370,\n",
      " 'amount': 302.69,\n",
      " 'billing_address_match': True,\n",
      " 'browser': 0,\n",
      " 'currency': 0,\n",
      " 'cvv_provided': True,\n",
      " 'day': 17,\n",
      " 'hour': 19,\n",
      " 'merchant_id': 0,\n",
      " 'minute': 52,\n",
      " 'month': 4,\n",
      " 'os': 0,\n",
      " 'payment_method': 0,\n",
      " 'product_category': 0,\n",
      " 'second': 6,\n",
      " 'transaction_type': 0,\n",
      " 'user_agent': 0,\n",
      " 'year': 2025}\n"
     ]
    }
   ],
   "source": [
    "x = x_pipe_1 | x_pipe_2 | x_pipe_3 | x_pipe_4\n",
    "\n",
    "pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f79e9",
   "metadata": {},
   "source": [
    "## Trying to serialize (pickle) some parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d11d6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"ordinal_encoder_1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipe2b, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1147d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ordinal_encoder_2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipe3c, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d89ba",
   "metadata": {},
   "source": [
    "## Now, let's try to retrieve the saved encoder and check if internal data was saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "90fa5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ordinal_encoder_1.pkl\", \"rb\") as f:\n",
    "    pipe2b = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8aaa3e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_agent': {'Mozilla/5.0 (Windows; U; Windows NT 11.0) AppleWebKit/532.16.3 (KHTML, like Gecko) Version/4.1 Safari/532.16.3': 0},\n",
       " 'payment_method': {'debit_card': 0},\n",
       " 'merchant_id': {'merchant_65': 0},\n",
       " 'product_category': {'luxury_items': 0},\n",
       " 'transaction_type': {'deposit': 0},\n",
       " 'currency': {'BRL': 0}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2b.get_feature_mappings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ddbd59",
   "metadata": {},
   "source": [
    "## Create a function to process each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e79b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "import datetime as dt\n",
    "\n",
    "def extract_device_info(x):\n",
    "    x_ = x['device_info']\n",
    "    return {\n",
    "        'os': x_['os'],\n",
    "        'browser': x_['browser'],\n",
    "    }\n",
    "\n",
    "def extract_timestamp_info(x):\n",
    "    x_ = dt.datetime.strptime(\n",
    "        x['timestamp'],\n",
    "        \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "    return {\n",
    "        'year': x_.year,\n",
    "        'month': x_.month,\n",
    "        'day': x_.day,\n",
    "        'hour': x_.hour,\n",
    "        'minute': x_.minute,\n",
    "        'second': x_.second\n",
    "    }\n",
    "\n",
    "def process_sample(x):\n",
    "    pipe1 = compose.Select(\n",
    "        \"amount\",\n",
    "        \"account_age_days\",\n",
    "        \"cvv_provided\",\n",
    "        \"billing_address_match\"\n",
    "    )\n",
    "    pipe1.learn_one(x)\n",
    "    x1 = pipe1.transform_one(x)\n",
    "    pipe2a = compose.Select(\n",
    "        \"currency\",\n",
    "        \"merchant_id\",\n",
    "        \"payment_method\",\n",
    "        \"product_category\",\n",
    "        \"transaction_type\",\n",
    "        \"user_agent\"\n",
    "    )\n",
    "    pipe2a.learn_one(x)\n",
    "    x_pipe_2 = pipe2a.transform_one(x)\n",
    "    pipe3a = compose.Select(\n",
    "        \"device_info\"\n",
    "    )\n",
    "    pipe3a.learn_one(x)\n",
    "    x_pipe_3 = pipe3a.transform_one(x)\n",
    "    pipe3b = compose.FuncTransformer(\n",
    "        extract_device_info,\n",
    "    )\n",
    "    pipe3b.learn_one(x_pipe_3)\n",
    "    x_pipe_3 = pipe3b.transform_one(x_pipe_3)\n",
    "    pipe4a = compose.Select(\n",
    "        \"timestamp\",\n",
    "    )\n",
    "    pipe4a.learn_one(x)\n",
    "    x_pipe_4 = pipe4a.transform_one(x)\n",
    "    pipe4b = compose.FuncTransformer(\n",
    "        extract_timestamp_info,\n",
    "    )\n",
    "    pipe4b.learn_one(x_pipe_4)\n",
    "    x_pipe_4 = pipe4b.transform_one(x_pipe_4)\n",
    "    ordinal_encoder = CustomPicklableOrdinalEncoder()\n",
    "    x_to_encode = x_pipe_2 | x_pipe_3 | x_pipe_4\n",
    "    ordinal_encoder.learn_one(x_to_encode)\n",
    "    x2 = ordinal_encoder.transform_one(x_to_encode)\n",
    "    return x1 | x2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7706f900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'account_age_days': 370,\n",
      " 'amount': 302.69,\n",
      " 'billing_address_match': True,\n",
      " 'currency': 'BRL',\n",
      " 'cvv_provided': True,\n",
      " 'device_info': {'browser': 'Opera', 'os': 'Windows'},\n",
      " 'ip_address': '169.235.63.28',\n",
      " 'is_fraud': 0,\n",
      " 'location': {'lat': -68.4965105, 'lon': -153.515477},\n",
      " 'merchant_id': 'merchant_65',\n",
      " 'payment_method': 'debit_card',\n",
      " 'product_category': 'luxury_items',\n",
      " 'timestamp': '2025-04-17T19:52:06.994066+00:00',\n",
      " 'transaction_id': 'ffd3d366-06e4-4ddb-894e-03876e893079',\n",
      " 'transaction_type': 'deposit',\n",
      " 'user_agent': 'Mozilla/5.0 (Windows; U; Windows NT 11.0) AppleWebKit/532.16.3 '\n",
      "               '(KHTML, like Gecko) Version/4.1 Safari/532.16.3',\n",
      " 'user_id': '61fa227e-d309-4ed0-b513-3cffa5526463'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "x = {\n",
    "    'transaction_id': 'ffd3d366-06e4-4ddb-894e-03876e893079', \n",
    "    'user_id': '61fa227e-d309-4ed0-b513-3cffa5526463', \n",
    "    'timestamp': '2025-04-17T19:52:06.994066+00:00', \n",
    "    'amount': 302.69, \n",
    "    'currency': 'BRL', \n",
    "    'merchant_id': 'merchant_65', \n",
    "    'product_category': 'luxury_items', \n",
    "    'transaction_type': 'deposit', \n",
    "    'payment_method': 'debit_card', \n",
    "    'location': {'lat': -68.4965105, 'lon': -153.515477}, \n",
    "    'ip_address': '169.235.63.28', \n",
    "    'device_info': {'os': 'Windows', 'browser': 'Opera'}, \n",
    "    'user_agent': 'Mozilla/5.0 (Windows; U; Windows NT 11.0) AppleWebKit/532.16.3 (KHTML, like Gecko) Version/4.1 Safari/532.16.3', \n",
    "    'account_age_days': 370, \n",
    "    'cvv_provided': True, \n",
    "    'billing_address_match': True, \n",
    "    'is_fraud': 0}\n",
    "\n",
    "pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33938672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'account_age_days': 370,\n",
       " 'billing_address_match': True,\n",
       " 'amount': 302.69,\n",
       " 'cvv_provided': True,\n",
       " 'currency': 0,\n",
       " 'merchant_id': 0,\n",
       " 'transaction_type': 0,\n",
       " 'user_agent': 0,\n",
       " 'product_category': 0,\n",
       " 'payment_method': 0,\n",
       " 'os': 0,\n",
       " 'browser': 0,\n",
       " 'year': 0,\n",
       " 'month': 0,\n",
       " 'day': 0,\n",
       " 'hour': 0,\n",
       " 'minute': 0,\n",
       " 'second': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_x = process_sample(x)\n",
    "processed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f671d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ordinal_encoder_1.pkl\", \"rb\") as f:\n",
    "    pipe2b = pickle.load(f)\n",
    "with open(\"ordinal_encoder_2.pkl\", \"rb\") as f:\n",
    "    pipe3c = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41554916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_agent': {'Mozilla/5.0 (Windows; U; Windows NT 11.0) AppleWebKit/532.16.3 (KHTML, like Gecko) Version/4.1 Safari/532.16.3': 0},\n",
       " 'payment_method': {'debit_card': 0},\n",
       " 'merchant_id': {'merchant_65': 0},\n",
       " 'product_category': {'luxury_items': 0},\n",
       " 'transaction_type': {'deposit': 0},\n",
       " 'currency': {'BRL': 0}}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2b.get_feature_mappings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "93addb27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'os': {'Windows': 0}, 'browser': {'Opera': 0}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe3c.get_feature_mappings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ffba4f",
   "metadata": {},
   "source": [
    "## CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a85dace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import base\n",
    "\n",
    "# --- Custom Transformer for Imputing missing values ---\n",
    "class DictImputer(base.Transformer):\n",
    "    \"\"\"\n",
    "    Imputes missing values (None or missing keys) for specified features in a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    on\n",
    "        List of feature names to impute.\n",
    "    fill_value\n",
    "        The value to use for imputation.\n",
    "    \"\"\"\n",
    "    def __init__(self, on: list, fill_value):\n",
    "        self.on = on\n",
    "        self.fill_value = fill_value\n",
    "    def transform_one(self, x: dict):\n",
    "        x_transformed = x.copy()\n",
    "        for feature in self.on:\n",
    "            if x_transformed.get(feature) is None:\n",
    "                x_transformed[feature] = self.fill_value\n",
    "        return x_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10a15e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose\n",
    "import datetime as dt\n",
    "\n",
    "def extract_device_info(x):\n",
    "    x_ = x['device_info']\n",
    "    return {\n",
    "        'os': x_['os'],\n",
    "        'browser': x_['browser'],\n",
    "    }\n",
    "\n",
    "def extract_timestamp_info(x):\n",
    "    x_ = dt.datetime.strptime(\n",
    "        x['timestamp'],\n",
    "        \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "    return {\n",
    "        'year': x_.year,\n",
    "        'month': x_.month,\n",
    "        'day': x_.day,\n",
    "        'hour': x_.hour,\n",
    "        'minute': x_.minute,\n",
    "        'second': x_.second,\n",
    "        'weekday': x_.weekday(),\n",
    "    }\n",
    "\n",
    "def extract_coordinates(x):\n",
    "    x_ = x['location']\n",
    "    return {\n",
    "        'lat': x_['lat'],\n",
    "        'lon': x_['lon'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FROM CLASSIFICATION, ERASE LATER\n",
    "def process_sample(x):\n",
    "    pipe1 = compose.Select(\n",
    "        \"amount\",\n",
    "        \"account_age_days\",\n",
    "        \"cvv_provided\",\n",
    "        \"billing_address_match\"\n",
    "    )\n",
    "    pipe1.learn_one(x)\n",
    "    x1 = pipe1.transform_one(x)\n",
    "    pipe2 = compose.Select(\n",
    "        \"currency\",\n",
    "        \"merchant_id\",\n",
    "        \"payment_method\",\n",
    "        \"product_category\",\n",
    "        \"transaction_type\",\n",
    "        #\"user_agent\"\n",
    "    )\n",
    "    pipe2.learn_one(x)\n",
    "    x_pipe_2 = pipe2.transform_one(x)\n",
    "    pipe3a = compose.Select(\n",
    "        \"device_info\"\n",
    "    )\n",
    "    pipe3a.learn_one(x)\n",
    "    x_pipe_3 = pipe3a.transform_one(x)\n",
    "    pipe3b = compose.FuncTransformer(\n",
    "        extract_device_info,\n",
    "    )\n",
    "    pipe3b.learn_one(x_pipe_3)\n",
    "    x_pipe_3 = pipe3b.transform_one(x_pipe_3)\n",
    "    pipe4a = compose.Select(\n",
    "        \"timestamp\",\n",
    "    )\n",
    "    pipe4a.learn_one(x)\n",
    "    x_pipe_4 = pipe4a.transform_one(x)\n",
    "    pipe4b = compose.FuncTransformer(\n",
    "        extract_timestamp_info,\n",
    "    )\n",
    "    pipe4b.learn_one(x_pipe_4)\n",
    "    x_pipe_4 = pipe4b.transform_one(x_pipe_4)\n",
    "    x_to_encode = x_pipe_2 | x_pipe_3 | x_pipe_4\n",
    "    ordinal_encoder.learn_one(x_to_encode)\n",
    "    x2 = ordinal_encoder.transform_one(x_to_encode)\n",
    "    return x1 | x2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bdc950fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'customer_id': '494ad3bb-7170-4286-bbbc-1486adec7a67',\n",
      " 'device_info': {'browser': 'Safari', 'device_type': 'Mobile', 'os': 'Windows'},\n",
      " 'event_id': '8ae9d883-dae1-4446-befc-db3ae2d4d599',\n",
      " 'event_type': 'page_view',\n",
      " 'location': {'lat': 30.010473, 'lon': -95.78408},\n",
      " 'page_url': 'https://example.com/grocery-gourmet-food/prod_69785',\n",
      " 'price': 1058.04,\n",
      " 'product_category': 'Grocery & Gourmet Food',\n",
      " 'product_id': 'prod_69785',\n",
      " 'quantity': None,\n",
      " 'referrer_url': None,\n",
      " 'search_query': None,\n",
      " 'session_event_sequence': 3,\n",
      " 'session_id': 'c89c8d69-d39a-4366-95a5-9b83f0ea9db5',\n",
      " 'time_on_page_seconds': 53,\n",
      " 'timestamp': '2025-05-04T15:18:32.284468+00:00'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "x = {'customer_id': '494ad3bb-7170-4286-bbbc-1486adec7a67',\n",
    "     'device_info': {'browser': 'Safari', 'device_type': 'Mobile', 'os': 'Windows'},\n",
    "     'event_id': '8ae9d883-dae1-4446-befc-db3ae2d4d599',\n",
    "     'event_type': 'page_view',\n",
    "     'location': {'lat': 30.010473, 'lon': -95.78408},\n",
    "     'page_url': 'https://example.com/grocery-gourmet-food/prod_69785',\n",
    "     'price': 1058.04,\n",
    "     'product_category': 'Grocery & Gourmet Food',\n",
    "     'product_id': 'prod_69785',\n",
    "     'quantity': None,\n",
    "     'referrer_url': None,\n",
    "     'search_query': None,\n",
    "     'session_event_sequence': 3,\n",
    "     'session_id': 'c89c8d69-d39a-4366-95a5-9b83f0ea9db5',\n",
    "     'time_on_page_seconds': 53,\n",
    "     'timestamp': '2025-05-04T15:18:32.284468+00:00'}\n",
    "\n",
    "pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7c0577d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import compose, preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "feature_hasher = preprocessing.FeatureHasher(n_features = 50, seed = 42)\n",
    "\n",
    "def process_sample(x):\n",
    "    pipe1 = compose.Select(\n",
    "        'price',\n",
    "        'quantity',\n",
    "        'session_event_sequence',\n",
    "        'time_on_page_seconds'\n",
    "    )\n",
    "    pipe1.learn_one(x)\n",
    "    x1 = pipe1.transform_one(x)\n",
    "    pipe2 = compose.Select(\n",
    "        'event_type',\n",
    "        'product_category',\n",
    "        'product_id',\n",
    "        'referrer_url',\n",
    "    )\n",
    "    pipe2.learn_one(x)\n",
    "    x_pipe_2 = pipe2.transform_one(x)\n",
    "    pipe3a = compose.Select(\n",
    "        \"device_info\"\n",
    "    )\n",
    "    pipe3a.learn_one(x)\n",
    "    x_pipe_3 = pipe3a.transform_one(x)\n",
    "    pipe3b = compose.FuncTransformer(\n",
    "        extract_device_info,\n",
    "    )\n",
    "    pipe3b.learn_one(x_pipe_3)\n",
    "    x_pipe_3 = pipe3b.transform_one(x_pipe_3)\n",
    "    pipe4a = compose.Select(\n",
    "        \"timestamp\",\n",
    "    )\n",
    "    pipe4a.learn_one(x)\n",
    "    x_pipe_4 = pipe4a.transform_one(x)\n",
    "    pipe4b = compose.FuncTransformer(\n",
    "        extract_timestamp_info,\n",
    "    )\n",
    "    pipe4b.learn_one(x_pipe_4)\n",
    "    x_pipe_4 = pipe4b.transform_one(x_pipe_4)\n",
    "    pipe5a = compose.Select(\n",
    "        \"location\",\n",
    "    )\n",
    "    pipe5a.learn_one(x)\n",
    "    x_pipe_5 = pipe5a.transform_one(x)\n",
    "    pipe5b = compose.FuncTransformer(\n",
    "        extract_coordinates,\n",
    "    )\n",
    "    pipe5b.learn_one(x_pipe_5)\n",
    "    x_pipe_5 = pipe5b.transform_one(x_pipe_5)\n",
    "    x_to_prep = x1 | x_pipe_2 | x_pipe_3 | x_pipe_4 | x_pipe_5\n",
    "    x_to_prep = DictImputer(\n",
    "        fill_value = False, \n",
    "        on = list(x_to_prep.keys())).transform_one(\n",
    "            x_to_prep)\n",
    "    numerical_features = [\n",
    "        'price',\n",
    "        'session_event_sequence',\n",
    "        'time_on_page_seconds',\n",
    "        'quantity'\n",
    "    ]\n",
    "    categorical_features = [\n",
    "        'event_type',\n",
    "        'product_category',\n",
    "        'product_id',\n",
    "        'referrer_url',\n",
    "        'os',\n",
    "        'browser',\n",
    "        'year',\n",
    "        'month',\n",
    "        'day',\n",
    "        'hour',\n",
    "        'minute',\n",
    "        'second',\n",
    "        'weekday'\n",
    "    ]\n",
    "    num_pipe = compose.Select(*numerical_features)\n",
    "    num_pipe.learn_one(x_to_prep)\n",
    "    x_num = num_pipe.transform_one(x_to_prep)\n",
    "    cat_pipe = compose.Select(*categorical_features)\n",
    "    cat_pipe.learn_one(x_to_prep)\n",
    "    x_cat = cat_pipe.transform_one(x_to_prep)\n",
    "    scaler.learn_one(x_num)\n",
    "    x_scaled = scaler.transform_one(x_num)\n",
    "    feature_hasher.learn_one(x_cat)\n",
    "    x_hashed = feature_hasher.transform_one(x_cat)\n",
    "    x = x_scaled | x_hashed\n",
    "    return x, scaler, feature_hasher\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "77bd8362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1,\n",
      " 2: 18,\n",
      " 14: 5,\n",
      " 20: 6,\n",
      " 27: 0,\n",
      " 29: 19,\n",
      " 31: 32,\n",
      " 34: 1,\n",
      " 37: 1,\n",
      " 42: 2025,\n",
      " 46: 1,\n",
      " 48: 1,\n",
      " 'price': 0.0,\n",
      " 'quantity': 0.0,\n",
      " 'session_event_sequence': 0.0,\n",
      " 'time_on_page_seconds': 0.0}\n"
     ]
    }
   ],
   "source": [
    "x, scaler, feature_hasher = process_sample(x)\n",
    "pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b493ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "os.makedirs('encoders', exist_ok=True)\n",
    "\n",
    "with open('encoders/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open('encoders/feature_hasher.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_hasher, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94d6e109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'price': 1,\n",
       "         'session_event_sequence': 1,\n",
       "         'time_on_page_seconds': 1,\n",
       "         'quantity': 1})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dd30d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import cluster\n",
    "\n",
    "model = cluster.DBSTREAM(\n",
    "                clustering_threshold = 1.0,\n",
    "                fading_factor = 0.01,\n",
    "                cleanup_interval = 2,\n",
    "            )\n",
    "#model = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f52f21a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn_one(x)\n",
    "y_pred = model.predict_one(x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3e996900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'price': 0.0,\n",
       "  'session_event_sequence': 0.0,\n",
       "  'time_on_page_seconds': 0.0,\n",
       "  'quantity': 0.0,\n",
       "  48: 1,\n",
       "  2: 18,\n",
       "  46: 1,\n",
       "  34: 1,\n",
       "  31: 32,\n",
       "  1: 1,\n",
       "  20: 6,\n",
       "  29: 19,\n",
       "  42: 2025,\n",
       "  37: 1,\n",
       "  14: 5,\n",
       "  27: 0}}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "06265e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'price': 0.0, 'session_event_sequence': 0.0, 'time_on_page_seconds': 0.0, 'quantity': 0.0, 48: 1, 2: 18, 46: 1, 34: 1, 31: 32, 1: 1, 20: 6, 29: 19, 42: 2025, 37: 1, 14: 5, 27: 0}\n",
      "0\n",
      "{0: {'price': 0.0, 'session_event_sequence': 0.0, 'time_on_page_seconds': 0.0, 'quantity': 0.0, 48: 1, 2: 18, 46: 1, 34: 1, 31: 32, 1: 1, 20: 6, 29: 19, 42: 2025, 37: 1, 14: 5, 27: 0}}\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y_pred)\n",
    "print(model.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dc86750d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clustering_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "525b4558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__ror__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_centers',\n",
       " '_cleanup',\n",
       " '_clusters',\n",
       " '_distance',\n",
       " '_find_fixed_radius_nn',\n",
       " '_gaussian_neighborhood',\n",
       " '_generate_clusters_from_labels',\n",
       " '_generate_labels',\n",
       " '_generate_weighted_adjacency_matrix',\n",
       " '_get_params',\n",
       " '_is_stochastic',\n",
       " '_memory_usage',\n",
       " '_micro_clusters',\n",
       " '_more_tags',\n",
       " '_mutable_attributes',\n",
       " '_n_clusters',\n",
       " '_raw_memory_usage',\n",
       " '_recluster',\n",
       " '_repr_html_',\n",
       " '_supervised',\n",
       " '_tags',\n",
       " '_time_stamp',\n",
       " '_unit_test_params',\n",
       " '_unit_test_skips',\n",
       " '_update',\n",
       " 'centers',\n",
       " 'cleanup_interval',\n",
       " 'clone',\n",
       " 'clustering_is_up_to_date',\n",
       " 'clustering_threshold',\n",
       " 'clusters',\n",
       " 'fading_factor',\n",
       " 'intersection_factor',\n",
       " 'learn_one',\n",
       " 'micro_clusters',\n",
       " 'minimum_weight',\n",
       " 'mutate',\n",
       " 'n_clusters',\n",
       " 'predict_one',\n",
       " 's',\n",
       " 's_t']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model) #DBSTREAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4b966eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -2, 1: 0}\n",
      "2\n",
      "{0: defaultdict(..., {0: 2.825146214041993, 1: -4.1897343141034495}), 1: defaultdict(..., {0: 3.2102057281313594, 1: 1.5694266433303334}), 2: defaultdict(..., {0: -2.22659408343387, 1: 1.201266225235274})}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Silhouette: 0.32145"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import cluster\n",
    "from river import stream\n",
    "from river import metrics\n",
    "\n",
    "X = [\n",
    "    [1, 2],\n",
    "    [1, 4],\n",
    "    [1, 0],\n",
    "    [4, 2],\n",
    "    [4, 4],\n",
    "    [4, 0],\n",
    "    [-2, 2],\n",
    "    [-2, 4],\n",
    "    [-2, 0]\n",
    "]\n",
    "\n",
    "k_means = cluster.KMeans(n_clusters=3, halflife=0.4, sigma=3, seed=0)\n",
    "metric = metrics.Silhouette()\n",
    "\n",
    "for x, _ in stream.iter_array(X):\n",
    "    k_means.learn_one(x)\n",
    "    y_pred = k_means.predict_one(x)\n",
    "    metric.update(x, y_pred, k_means.centers)\n",
    "\n",
    "print(x)\n",
    "print(y_pred)\n",
    "print(k_means.centers)\n",
    "\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce26395f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
