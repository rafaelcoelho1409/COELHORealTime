# Environment configuration
# 'local' = Skaffold local development (no registry prefix)
# 'production' = ArgoCD with k3d built-in registry
environment: local

# Registry configuration (only used when environment: production)
registry:
  url: master-registry:5000
  # ImagePullSecret not needed for k3d insecure registry
  imagePullSecret: ""

# Image configurations
# Note: Skaffold overrides these with local builds during 'skaffold dev'
# ArgoCD Image Updater updates these tags when new images are pushed to registry
# NOTE: fastapi service (port 8001) has been terminated and merged into river/sklearn services


# Bitnami Kafka chart configuration (https://github.com/bitnami/charts/tree/main/bitnami/kafka)
# KRaft mode enabled by default (no Zookeeper required)
# NOTE: Using bitnamilegacy registry since mainline bitnami images were restructured in Aug 2025
# See: https://github.com/bitnami/charts/issues/35164
kafka:
  enabled: true
  # Use legacy registry (mainline bitnami no longer has these images after Aug 2025)
  image:
    registry: docker.io
    repository: bitnamilegacy/kafka
    tag: "4.0.0-debian-12-r10"
  # Allow legacy images (required for bitnamilegacy registry)
  global:
    security:
      allowInsecureImages: true
  # Volume permissions init container (uses os-shell)
  defaultInitContainers:
    volumePermissions:
      image:
        registry: docker.io
        repository: bitnamilegacy/os-shell
        tag: "12-debian-12-r51"
  # Single-node KRaft mode for local development
  # Controller nodes act as both controller and broker
  controller:
    replicaCount: 1
    persistence:
      enabled: true
      size: 2Gi
      storageClass: local-path
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
  # No separate broker nodes - controller handles everything
  broker:
    replicaCount: 0
  # Listener configuration - PLAINTEXT for local development (no auth)
  listeners:
    client:
      protocol: PLAINTEXT
    controller:
      protocol: PLAINTEXT
    interbroker:
      protocol: PLAINTEXT
  # Disable SASL authentication for local development
  sasl:
    client:
      users: []
      passwords: ""
  # Kafka metrics configuration
  # JMX exporter disabled due to compatibility issues with Kafka 4.0
  # TODO: Re-enable when JMX exporter supports Kafka 4.0 or use alternative metrics
  metrics:
    jmx:
      enabled: false
  # Kafka service configuration
  service:
    type: ClusterIP
    ports:
      client: 9092
      controller: 9093
      interbroker: 9094
  # Extra environment variables
  extraEnvVars:
    - name: KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE
      value: "true"
    - name: KAFKA_CFG_NUM_PARTITIONS
      value: "3"
    - name: KAFKA_CFG_DEFAULT_REPLICATION_FACTOR
      value: "1"
  # Log configuration
  log:
    dirs: /bitnami/kafka/data
    retentionHours: 168
    retentionBytes: "1073741824"


# Kafka Producers - Python data generators (separate from Kafka broker)
kafka-producers:
  enabled: true
  image: coelho-realtime-kafka-producers:latest
  imagePullPolicy: IfNotPresent
  replicas: 1
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "250m"


# NOTE: Streamlit has been removed - fully replaced by Reflex


# MLflow subchart configuration (community-charts/mlflow)
# Connects to MinIO for artifacts, PostgreSQL for metadata
mlflow:
  # Service configuration
  service:
    type: ClusterIP
    port: 5000
    containerPort: 5000
  # Enable Prometheus metrics for MLflow tracking server
  tracking:
    metrics:
      enabled: true
      serviceMonitor:
        enabled: true
        namespace: "coelho-realtime"
        labels:
          release: "coelho-realtime"
        interval: 30s
  # Gunicorn: multiple workers supported with PostgreSQL (no lock contention)
  extraArgs:
    workers: "2"
    gunicornOpts: "--log-level=info --timeout=120"
  # Backend store using PostgreSQL (persisted via hostPath)
  backendStore:
    databaseMigration: true
    databaseConnectionCheck: true
    postgres:
      enabled: true
      host: "coelho-realtime-postgresql"
      port: 5432
      database: "mlflow"
      user: "mlflow"
      password: "mlflow123"
  # Artifact store pointing to MinIO S3-compatible storage
  artifactRoot:
    s3:
      enabled: true
      bucket: "mlflow-artifacts"
      awsAccessKeyId: "minioadmin"
      awsSecretAccessKey: "minioadmin123"
  # Resource limits
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"
  # Environment variables for MinIO/S3 connection
  extraEnvVars:
    # Allow external connections (required for Python MLflow client)
    MLFLOW_SERVER_ALLOWED_HOSTS: "*"
    # MinIO S3 endpoint (internal cluster service)
    MLFLOW_S3_ENDPOINT_URL: "http://coelho-realtime-minio:9000"
    MLFLOW_S3_IGNORE_TLS: "true"
    AWS_DEFAULT_REGION: "us-east-1"


reflex:
  image: coelho-realtime-reflex:latest
  imagePullPolicy: IfNotPresent
  replicas: 1
  resources:
    requests:
      memory: "2Gi"
      cpu: "1000m"
    limits:
      memory: "4Gi"
      cpu: "2000m"
  portsSettings:
    ports:
      - name: frontend
        port: 3000
        targetPort: 3000
        nodePort: 30004
        protocol: TCP
      - name: backend
        port: 8000
        targetPort: 8000
        nodePort: 30005
        protocol: TCP
    type: LoadBalancer
  livenessProbeSettings:
    livenessProbe:
      tcpSocket:
        port: 3000
      initialDelaySeconds: 120
      periodSeconds: 20
      failureThreshold: 5
  readinessProbeSettings:
    readinessProbe:
      tcpSocket:
        port: 3000
      initialDelaySeconds: 90
      periodSeconds: 10
      failureThreshold: 6


# River ML Training Service
# Handles incremental ML model training using River library
river:
  image: coelho-realtime-river:latest
  imagePullPolicy: IfNotPresent
  replicas: 1
  resources:
    requests:
      memory: "2Gi"
      cpu: "1000m"
    limits:
      memory: "4Gi"
      cpu: "2000m"
  portsSettings:
    ports:
      - name: http
        port: 8002
        targetPort: 8002
        nodePort: 30006
        protocol: TCP
    type: LoadBalancer
  livenessProbeSettings:
    livenessProbe:
      httpGet:
        path: /health
        port: 8002
      initialDelaySeconds: 180
      periodSeconds: 30
      failureThreshold: 5
  readinessProbeSettings:
    readinessProbe:
      httpGet:
        path: /health
        port: 8002
      initialDelaySeconds: 120
      periodSeconds: 10
      failureThreshold: 3


sklearn:
  image: coelho-realtime-sklearn:latest
  imagePullPolicy: IfNotPresent
  replicas: 1
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "4Gi"
      cpu: "2000m"
  portsSettings:
    ports:
      - name: http
        port: 8003
        targetPort: 8003
        nodePort: 30007
        protocol: TCP
    type: LoadBalancer
  livenessProbeSettings:
    livenessProbe:
      httpGet:
        path: /health
        port: 8003
      initialDelaySeconds: 120
      periodSeconds: 30
      failureThreshold: 5
  readinessProbeSettings:
    readinessProbe:
      httpGet:
        path: /health
        port: 8003
      initialDelaySeconds: 60
      periodSeconds: 10
      failureThreshold: 3


# Redis configuration (Bitnami Redis chart)
# Used by Reflex for state management in production
redis:
  enabled: true
  architecture: standalone  # Use 'replication' for high availability
  auth:
    enabled: false  # Disable authentication for simplicity in local k3d
  master:
    persistence:
      enabled: true
      size: 2Gi
      storageClass: local-path
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "250m"
  # Redis configuration for Reflex optimization
  commonConfiguration: |-
    # Enable AOF persistence for better durability
    appendonly yes
    # Disable RDB snapshots (we use AOF)
    save ""
    # Set max memory and eviction policy
    maxmemory 256mb
    maxmemory-policy allkeys-lru
  # Prometheus metrics exporter
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      namespace: coelho-realtime
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 128Mi


# MinIO Object Storage (S3-compatible) - Official MinIO Chart (https://helm.min.io/)
# Used for storing parquet files and MLflow artifacts
# Data persists in /data/minio on host via K3d volume mount
minio:
  enabled: true
  # Deployment mode
  mode: standalone
  # Root credentials
  rootUser: "minioadmin"
  rootPassword: "minioadmin123"
  # MinIO API Service (S3 endpoint)
  service:
    type: NodePort
    port: "9000"
    nodePort: 30900
  # MinIO Console Service (Web UI)
  consoleService:
    type: NodePort
    port: "9001"
    nodePort: 30901
  # Persistence using pre-created PVC with hostPath (survives skaffold dev Ctrl+C)
  persistence:
    enabled: true
    existingClaim: "minio-hostpath-pvc"
  # Security context - run as root to avoid permission issues with hostPath
  securityContext:
    enabled: false
  containerSecurityContext:
    enabled: false
  # Resource limits
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      memory: 512Mi
  # Replicas (standalone mode = 1)
  replicas: 1
  # Buckets to create on MinIO startup (native MinIO chart provisioning)
  buckets:
    - name: mlflow-artifacts
      policy: none
      purge: false
    - name: lakehouse
      policy: none
      purge: false
  users: []
  policies: []
  customCommands: []
  svcaccts: []
  # Service account
  serviceAccount:
    create: true
    name: "minio-sa"
  # Disable ingress for local development
  ingress:
    enabled: false
  consoleIngress:
    enabled: false
  # Prometheus metrics
  metrics:
    serviceMonitor:
      enabled: true
      public: true
      includeNode: true
      namespace: coelho-realtime


# PostgreSQL Database (Bitnami PostgreSQL chart)
# Used by MLflow for metadata storage (experiments, runs, metrics, parameters)
# Also used by Grafana for dashboard/user storage (created via Grafana init container)
# Data persists in /data/postgresql on host via K3d volume mount
postgresql:
  enabled: true
  # Authentication - primary user for MLflow
  auth:
    postgresPassword: "postgres123"
    username: "mlflow"
    password: "mlflow123"
    database: "mlflow"
  # Use pre-created PVC with hostPath (survives skaffold dev Ctrl+C)
  primary:
    persistence:
      enabled: true
      existingClaim: "postgresql-hostpath-pvc"
    # Resource limits
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
  # Enable volumePermissions init container to fix hostPath ownership
  volumePermissions:
    enabled: true
  # Disable replication for local development
  architecture: standalone
  # Prometheus metrics exporter
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
      namespace: coelho-realtime
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 100m
        memory: 128Mi


# kube-prometheus-stack configuration (Prometheus + Grafana + Alertmanager)
# Provides comprehensive monitoring and observability for the cluster
# Port mappings changed to avoid conflicts:
#   - Grafana: 3000 → 3001 (Reflex uses 3000)
#   - Alertmanager: 9093 → 9094 (Kafka controller uses 9093)
# Integrations:
#   - Grafana → PostgreSQL (shared with MLflow)
kube-prometheus-stack:
  enabled: true
  # Prometheus Operator configuration
  prometheusOperator:
    enabled: true
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi
  # Prometheus Server configuration
  prometheus:
    enabled: true
    prometheusSpec:
      # Add cluster label to all metrics (required for kube-prometheus-stack dashboards)
      externalLabels:
        cluster: coelho-realtime
      # Scrape interval and evaluation
      scrapeInterval: 30s
      evaluationInterval: 30s
      # Retention period (7 days for local dev)
      retention: 7d
      retentionSize: "5GB"
      # Resource limits for local k3d
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          cpu: 1000m
          memory: 2Gi
      # Storage for metrics
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: local-path
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
      # Enable service discovery for all namespaces
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
    # Prometheus service configuration
    service:
      type: NodePort
      port: 9090
      targetPort: 9090
      nodePort: 30090
  # Grafana configuration
  grafana:
    enabled: true
    # Admin credentials
    adminUser: admin
    adminPassword: admin123
    # Service configuration - Port 3001 to avoid conflict with Reflex (3000)
    service:
      type: NodePort
      port: 3001
      targetPort: 3000  # Grafana internal port stays 3000
      nodePort: 30031
    # Resource limits
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
    # Use PostgreSQL as database backend (shared with MLflow)
    persistence:
      enabled: false  # Disabled since we use PostgreSQL
    # Init container to create grafana database in PostgreSQL before Grafana starts
    extraInitContainers:
      - name: init-postgres-grafana-db
        image: bitnami/postgresql:latest
        command:
          - /bin/bash
          - -c
          - |
            echo "Waiting for PostgreSQL to be ready..."
            until PGPASSWORD=$POSTGRES_PASSWORD psql -h $PG_HOST -U postgres -c '\q' 2>/dev/null; do
              echo "PostgreSQL is unavailable - sleeping"
              sleep 2
            done
            echo "PostgreSQL is ready!"

            echo "Creating grafana user and database if not exists..."
            PGPASSWORD=$POSTGRES_PASSWORD psql -h $PG_HOST -U postgres <<-EOSQL
              DO \$\$
              BEGIN
                IF NOT EXISTS (SELECT FROM pg_catalog.pg_roles WHERE rolname = 'grafana') THEN
                  CREATE USER grafana WITH PASSWORD 'grafana123';
                END IF;
              END
              \$\$;

              SELECT 'CREATE DATABASE grafana OWNER grafana'
              WHERE NOT EXISTS (SELECT FROM pg_database WHERE datname = 'grafana')\gexec

              GRANT ALL PRIVILEGES ON DATABASE grafana TO grafana;
            EOSQL
            echo "Grafana database setup complete!"
        env:
          - name: PG_HOST
            value: "coelho-realtime-postgresql"
          - name: POSTGRES_PASSWORD
            value: "postgres123"
    # PostgreSQL database configuration for Grafana
    env:
      GF_DATABASE_TYPE: postgres
      GF_DATABASE_HOST: coelho-realtime-postgresql:5432
      GF_DATABASE_NAME: grafana
      GF_DATABASE_USER: grafana
      GF_DATABASE_PASSWORD: grafana123
      GF_DATABASE_SSL_MODE: disable
    # Default dashboards
    defaultDashboardsEnabled: true
    defaultDashboardsTimezone: browser
    # Sidecar for dashboard/datasource discovery
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
        searchNamespace: ALL
      datasources:
        enabled: true
        # Disable default datasource - we provision our own below
        defaultDatasourceEnabled: false
    # Datasources provisioned directly (not via sidecar) to avoid timing issues
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Prometheus
            type: prometheus
            uid: prometheus
            url: http://coelho-realtime-kube-prome-prometheus.coelho-realtime:9090/
            access: proxy
            isDefault: true
            jsonData:
              httpMethod: POST
              timeInterval: 30s
          - name: Alertmanager
            type: alertmanager
            uid: alertmanager
            url: http://coelho-realtime-kube-prome-alertmanager.coelho-realtime:9094/
            access: proxy
            jsonData:
              handleGrafanaManagedAlerts: false
              implementation: prometheus
    # Additional data sources
    additionalDataSources: []
    # Grafana.ini configuration
    grafana.ini:
      server:
        root_url: "%(protocol)s://%(domain)s:%(http_port)s/"
        serve_from_sub_path: false
      auth.anonymous:
        enabled: true
        org_role: Viewer
  # Alertmanager configuration
  alertmanager:
    enabled: true
    alertmanagerSpec:
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 100m
          memory: 128Mi
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: local-path
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 1Gi
    # Service configuration - Port 9094 to avoid conflict with Kafka controller (9093)
    service:
      type: NodePort
      port: 9094
      targetPort: 9093  # Alertmanager internal port stays 9093
      nodePort: 30094
    # Alertmanager notification configuration
    config:
      global:
        resolve_timeout: 5m
        # SMTP settings for email notifications (commented - not needed for now)
        # smtp_smarthost: 'smtp.gmail.com:587'
        # smtp_from: 'alertmanager@coelhorealtime.local'
        # smtp_auth_username: 'your-email@gmail.com'
        # smtp_auth_password: 'your-app-password'
        # smtp_require_tls: true

        # Slack API URL (commented - not needed for now)
        # slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

        # PagerDuty URL (commented - not needed for now)
        # pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

      # Templates for notification formatting
      templates:
        - '/etc/alertmanager/config/*.tmpl'
      # Route tree for alert routing
      route:
        # Default receiver when no other routes match
        receiver: 'null'
        # Group alerts by these labels
        group_by: ['alertname', 'namespace', 'severity']
        # Wait before sending initial notification (allows grouping)
        group_wait: 30s
        # Wait before sending updates to a group
        group_interval: 5m
        # Wait before resending if alert is still firing
        repeat_interval: 4h
        # Child routes for severity-based routing
        routes:
          # Critical alerts - immediate notification
          - receiver: 'null'  # Change to 'slack-critical' or 'pagerduty' when ready
            matchers:
              - severity = "critical"
            group_wait: 10s
            group_interval: 1m
            repeat_interval: 1h
            continue: true
          # Warning alerts - batched notifications
          - receiver: 'null'  # Change to 'slack-warnings' or 'email' when ready
            matchers:
              - severity = "warning"
            group_wait: 1m
            group_interval: 10m
            repeat_interval: 6h
            continue: true
          # Info alerts - low priority
          - receiver: 'null'
            matchers:
              - severity = "info"
            group_wait: 5m
            group_interval: 30m
            repeat_interval: 24h
          # Watchdog alert - silence it (used for "dead man's switch")
          - receiver: 'null'
            matchers:
              - alertname = "Watchdog"
          # InfoInhibitor alert - silence it
          - receiver: 'null'
            matchers:
              - alertname = "InfoInhibitor"
      # Receivers define where notifications are sent
      receivers:
        # Null receiver - discards all alerts (useful for silencing)
        - name: 'null'
        # Slack receiver for critical alerts (commented - not needed for now)
        # - name: 'slack-critical'
        #   slack_configs:
        #     - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        #       channel: '#alerts-critical'
        #       username: 'Alertmanager'
        #       icon_emoji: ':rotating_light:'
        #       send_resolved: true
        #       title: '{{ template "slack.default.title" . }}'
        #       text: >-
        #         {{ range .Alerts }}
        #         *Alert:* {{ .Annotations.summary }}
        #         *Description:* {{ .Annotations.description }}
        #         *Severity:* {{ .Labels.severity }}
        #         *Namespace:* {{ .Labels.namespace }}
        #         {{ end }}

        # Slack receiver for warning alerts (commented - not needed for now)
        # - name: 'slack-warnings'
        #   slack_configs:
        #     - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        #       channel: '#alerts-warnings'
        #       username: 'Alertmanager'
        #       icon_emoji: ':warning:'
        #       send_resolved: true
        #       title: '{{ template "slack.default.title" . }}'
        #       text: >-
        #         {{ range .Alerts }}
        #         *Alert:* {{ .Annotations.summary }}
        #         *Description:* {{ .Annotations.description }}
        #         *Severity:* {{ .Labels.severity }}
        #         {{ end }}

        # Discord receiver (commented - not needed for now)
        # - name: 'discord'
        #   discord_configs:
        #     - webhook_url: 'https://discord.com/api/webhooks/YOUR/DISCORD/WEBHOOK'
        #       title: '{{ template "discord.default.title" . }}'
        #       message: '{{ template "discord.default.message" . }}'
        #       send_resolved: true

        # Email receiver (commented - not needed for now)
        # - name: 'email'
        #   email_configs:
        #     - to: 'your-email@example.com'
        #       send_resolved: true
        #       headers:
        #         Subject: '[COELHORealTime] {{ .Status | toUpper }} - {{ .CommonLabels.alertname }}'

        # PagerDuty receiver for critical on-call alerts (commented - not needed for now)
        # - name: 'pagerduty'
        #   pagerduty_configs:
        #     - service_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
        #       send_resolved: true
        #       severity: '{{ .CommonLabels.severity }}'
        #       description: '{{ .CommonAnnotations.summary }}'

        # Webhook receiver for custom integrations (commented - not needed for now)
        # - name: 'webhook'
        #   webhook_configs:
        #     - url: 'http://your-webhook-endpoint:8080/alerts'
        #       send_resolved: true
      # Inhibition rules prevent certain alerts when others are firing
      inhibit_rules:
        # If a critical alert is firing, suppress warnings for the same alertname
        - source_matchers:
            - severity = "critical"
          target_matchers:
            - severity = "warning"
          equal: ['alertname', 'namespace']
        # If a node is down, suppress all other alerts for pods on that node
        - source_matchers:
            - alertname = "NodeDown"
          target_matchers:
            - severity =~ "warning|critical"
          equal: ['node']
        # If Kafka is down, suppress Kafka-related downstream alerts
        - source_matchers:
            - alertname = "KafkaDown"
          target_matchers:
            - alertname =~ "Kafka.*"
          equal: ['namespace']
        # If PostgreSQL is down, suppress MLflow alerts (depends on PostgreSQL)
        - source_matchers:
            - alertname = "PostgreSQLDown"
          target_matchers:
            - alertname =~ "MLflow.*"
          equal: ['namespace']
        # If MinIO is down, suppress MLflow and River alerts (they depend on MinIO)
        - source_matchers:
            - alertname = "MinIODown"
          target_matchers:
            - alertname =~ "MLflow.*|River.*"
          equal: ['namespace']
        # If InfoInhibitor is firing, suppress all info-level alerts
        - source_matchers:
            - alertname = "InfoInhibitor"
          target_matchers:
            - severity = "info"
          equal: ['namespace']
  # Node Exporter for node-level metrics
  nodeExporter:
    enabled: true
    resources:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 64Mi
  # kube-state-metrics for Kubernetes object metrics
  kubeStateMetrics:
    enabled: true
  # Disable components not needed for local k3d development
  kubeApiServer:
    enabled: true
  kubeControllerManager:
    enabled: false  # Not accessible in k3d
  kubeScheduler:
    enabled: false  # Not accessible in k3d
  kubeProxy:
    enabled: false  # k3d uses different networking
  kubeEtcd:
    enabled: false  # Not accessible in k3d
  # CoreDNS monitoring
  coreDns:
    enabled: true
  # Kubelet monitoring
  kubelet:
    enabled: true
  # Default rules for alerting
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: false  # Not accessible in k3d
      configReloaders: true
      general: true
      k8sContainerCpuUsageSecondsTotal: true
      k8sContainerMemoryCache: true
      k8sContainerMemoryRss: true
      k8sContainerMemorySwap: true
      k8sContainerResource: true
      k8sPodOwner: true
      kubeApiserverAvailability: true
      kubeApiserverBurnrate: true
      kubeApiserverHistogram: true
      kubeApiserverSlos: true
      kubeControllerManager: false  # Not accessible in k3d
      kubelet: true
      kubeProxy: false  # k3d uses different networking
      kubePrometheusGeneral: true
      kubePrometheusNodeRecording: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      kubeSchedulerAlerting: false  # Not accessible in k3d
      kubeSchedulerRecording: false  # Not accessible in k3d
      kubeStateMetrics: true
      network: true
      node: true
      nodeExporterAlerting: true
      nodeExporterRecording: true
      prometheus: true
      prometheusOperator: true
      windows: false


# Bitnami Spark configuration (https://github.com/bitnami/charts/tree/main/bitnami/spark)
# Used for Kafka → Delta Lake streaming pipeline
# Delta Lake tables stored on MinIO (S3-compatible)
# NOTE: Using bitnamilegacy due to Bitnami's August 2025 migration to paid model
spark:
  enabled: true
  # Spark 4.0.0 image (from bitnamilegacy - free legacy images)
  image:
    registry: docker.io
    repository: bitnamilegacy/spark
    tag: "4.0.0"
  # Master configuration
  master:
    # Resource limits for local k3d
    resources:
      requests:
        memory: "1Gi"
        cpu: "500m"
      limits:
        memory: "2Gi"
        cpu: "1000m"
    # Extra environment variables for Spark configuration
    extraEnvVars:
      # Delta Lake and S3 configuration
      - name: SPARK_EXTRA_CLASSPATH
        value: "/opt/bitnami/spark/jars/extra/*"
    # Init container to download Delta Lake, Kafka, and Hadoop-AWS JARs
    initContainers:
      - name: download-delta-jars
        image: curlimages/curl:8.11.1
        command: ["/bin/sh", "-c"]
        args:
          - |
            echo "Downloading Delta Lake JARs for Spark 4.0..."
            mkdir -p /jars

            # Delta Lake 4.0.0 (for Spark 4.0)
            curl -L -o /jars/delta-spark_2.13-4.0.0.jar \
              https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.0/delta-spark_2.13-4.0.0.jar
            curl -L -o /jars/delta-storage-4.0.0.jar \
              https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.0/delta-storage-4.0.0.jar

            # Kafka Structured Streaming (Spark 4.0)
            curl -L -o /jars/spark-sql-kafka-0-10_2.13-4.0.0.jar \
              https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/4.0.0/spark-sql-kafka-0-10_2.13-4.0.0.jar
            curl -L -o /jars/spark-token-provider-kafka-0-10_2.13-4.0.0.jar \
              https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.13/4.0.0/spark-token-provider-kafka-0-10_2.13-4.0.0.jar
            curl -L -o /jars/kafka-clients-3.9.0.jar \
              https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.9.0/kafka-clients-3.9.0.jar
            curl -L -o /jars/commons-pool2-2.12.0.jar \
              https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.12.0/commons-pool2-2.12.0.jar

            # Hadoop AWS (S3A) for MinIO connectivity
            curl -L -o /jars/hadoop-aws-3.4.2.jar \
              https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.2/hadoop-aws-3.4.2.jar
            curl -L -o /jars/aws-java-sdk-bundle-1.12.790.jar \
              https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.790/aws-java-sdk-bundle-1.12.790.jar

            echo "All JARs downloaded successfully!"
            ls -la /jars/
        volumeMounts:
          - name: spark-extra-jars
            mountPath: /jars
    extraVolumes:
      - name: spark-extra-jars
        emptyDir: {}
    extraVolumeMounts:
      - name: spark-extra-jars
        mountPath: /opt/bitnami/spark/jars/extra
  # Worker configuration
  worker:
    # Single worker for local development
    replicaCount: 1
    # Resource limits
    resources:
      requests:
        memory: "2Gi"
        cpu: "1000m"
      limits:
        memory: "4Gi"
        cpu: "2000m"
    # Same init container for workers
    extraEnvVars:
      - name: SPARK_EXTRA_CLASSPATH
        value: "/opt/bitnami/spark/jars/extra/*"
    initContainers:
      - name: download-delta-jars
        image: curlimages/curl:8.11.1
        command: ["/bin/sh", "-c"]
        args:
          - |
            echo "Downloading Delta Lake JARs for Spark 4.0..."
            mkdir -p /jars

            # Delta Lake 4.0.0 (for Spark 4.0)
            curl -L -o /jars/delta-spark_2.13-4.0.0.jar \
              https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.0/delta-spark_2.13-4.0.0.jar
            curl -L -o /jars/delta-storage-4.0.0.jar \
              https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.0/delta-storage-4.0.0.jar

            # Kafka Structured Streaming (Spark 4.0)
            curl -L -o /jars/spark-sql-kafka-0-10_2.13-4.0.0.jar \
              https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/4.0.0/spark-sql-kafka-0-10_2.13-4.0.0.jar
            curl -L -o /jars/spark-token-provider-kafka-0-10_2.13-4.0.0.jar \
              https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.13/4.0.0/spark-token-provider-kafka-0-10_2.13-4.0.0.jar
            curl -L -o /jars/kafka-clients-3.9.0.jar \
              https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.9.0/kafka-clients-3.9.0.jar
            curl -L -o /jars/commons-pool2-2.12.0.jar \
              https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.12.0/commons-pool2-2.12.0.jar

            # Hadoop AWS (S3A) for MinIO connectivity
            curl -L -o /jars/hadoop-aws-3.4.2.jar \
              https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.2/hadoop-aws-3.4.2.jar
            curl -L -o /jars/aws-java-sdk-bundle-1.12.780.jar \
              https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle-1.12.780/aws-java-sdk-bundle-1.12.780.jar

            echo "All JARs downloaded successfully!"
            ls -la /jars/
        volumeMounts:
          - name: spark-extra-jars
            mountPath: /jars
    extraVolumes:
      - name: spark-extra-jars
        emptyDir: {}
    extraVolumeMounts:
      - name: spark-extra-jars
        mountPath: /opt/bitnami/spark/jars/extra
  # Service configuration
  service:
    type: ClusterIP
  # Prometheus metrics
  metrics:
    enabled: true
    masterAnnotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8080"
      prometheus.io/path: "/metrics/master/prometheus"
    workerAnnotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8080"
      prometheus.io/path: "/metrics/worker/prometheus"
