# Environment configuration
# 'local' = Skaffold local development (no registry prefix)
# 'production' = ArgoCD with k3d built-in registry
environment: local

# Registry configuration (only used when environment: production)
registry:
  url: master-registry:5000
  # ImagePullSecret not needed for k3d insecure registry
  imagePullSecret: ""

# Image configurations
# Note: Skaffold overrides these with local builds during 'skaffold dev'
# ArgoCD Image Updater updates these tags when new images are pushed to registry
fastapi:
  image: coelho-realtime-fastapi:latest
  imagePullPolicy: IfNotPresent
  replicas: 1
  resources:
    requests:
      memory: "8Gi"
      cpu: "2000m"
    limits:
      memory: "16Gi"
      cpu: "4000m"
  portsSettings:
    ports:
      - name: http
        port: 8001
        targetPort: 8001
        nodePort: 30000
        protocol: TCP
    type: LoadBalancer #ClusterIP
  livenessProbeSettings:
    livenessProbe:
      httpGet:
        path: /health
        port: 8001
      initialDelaySeconds: 120  # Wait 2 minutes for startup (model loading)
      periodSeconds: 30         # Check every 30 seconds
      timeoutSeconds: 10        # Allow 10 seconds for response (was 1s default!)
      failureThreshold: 5       # Restart after 5 failures (150 seconds of unresponsiveness)
  readinessProbeSettings:
    readinessProbe:
      httpGet:
        path: /health
        port: 8001
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3


kafka:
  image: coelho-realtime-kafka:latest
  imagePullPolicy: IfNotPresent
  replicas: 1
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"
  portsSettings:
    ports:
      - name: external
        port: 9092
        targetPort: 9092
        nodePort: 30001
        protocol: TCP
      - name: internal
        port: 29092
        targetPort: 29092
        protocol: TCP
      - name: controller
        port: 9093
        targetPort: 9093
        protocol: TCP
    type: LoadBalancer #ClusterIP
  storageSize: 2Gi
  storageClassName: local-path
  livenessProbeSettings:
    livenessProbe:
      tcpSocket:
        port: 29092
      initialDelaySeconds: 90
      periodSeconds: 30
      failureThreshold: 5
  readinessProbeSettings:
    readinessProbe:
      tcpSocket:
        port: 29092
      initialDelaySeconds: 60
      periodSeconds: 10
      failureThreshold: 6


# DEPRECATED: Streamlit has been replaced by Reflex
# Set enabled: true to re-enable if needed for reference
streamlit:
  enabled: false
  image: coelho-realtime-streamlit:latest
  imagePullPolicy: IfNotPresent
  replicas: 1
  resources:
    requests:
      memory: "256Mi"
      cpu: "125m"
    limits:
      memory: "512Mi"
      cpu: "250m"
  portsSettings:
    ports:
      - name: http
        port: 8501
        targetPort: 8501
        nodePort: 30003
        protocol: TCP
    type: LoadBalancer #ClusterIP
  livenessProbeSettings:
    livenessProbe:
      httpGet:
        path: /_stcore/health
        port: 8501
      initialDelaySeconds: 30
      periodSeconds: 15
      failureThreshold: 3
  readinessProbeSettings:
    readinessProbe:
      httpGet:
        path: /_stcore/health
        port: 8501
      initialDelaySeconds: 10
      periodSeconds: 10
      failureThreshold: 3


# MLflow subchart configuration (community-charts/mlflow)
# Connects to MinIO for artifacts, PostgreSQL for metadata
mlflow:
  # Service configuration
  service:
    type: ClusterIP
    port: 5000
    containerPort: 5000
  # Gunicorn: multiple workers supported with PostgreSQL (no lock contention)
  extraArgs:
    workers: "2"
    gunicornOpts: "--log-level=info --timeout=120"
  # Backend store using PostgreSQL (persisted via hostPath)
  backendStore:
    databaseMigration: true
    databaseConnectionCheck: true
    postgres:
      enabled: true
      host: "coelho-realtime-postgresql"
      port: 5432
      database: "mlflow"
      user: "mlflow"
      password: "mlflow123"
  # Artifact store pointing to MinIO S3-compatible storage
  artifactRoot:
    s3:
      enabled: true
      bucket: "mlflow-artifacts"
      awsAccessKeyId: "minioadmin"
      awsSecretAccessKey: "minioadmin123"
  # Resource limits
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"
  # Environment variables for MinIO/S3 connection
  extraEnvVars:
    # Allow external connections (required for Python MLflow client)
    MLFLOW_SERVER_ALLOWED_HOSTS: "*"
    # MinIO S3 endpoint (internal cluster service)
    MLFLOW_S3_ENDPOINT_URL: "http://coelho-realtime-minio:9000"
    MLFLOW_S3_IGNORE_TLS: "true"
    AWS_DEFAULT_REGION: "us-east-1"


reflex:
  image: coelho-realtime-reflex:latest
  imagePullPolicy: IfNotPresent
  replicas: 1
  resources:
    requests:
      memory: "2Gi"
      cpu: "1000m"
    limits:
      memory: "4Gi"
      cpu: "2000m"
  portsSettings:
    ports:
      - name: frontend
        port: 3000
        targetPort: 3000
        nodePort: 30004
        protocol: TCP
      - name: backend
        port: 8000
        targetPort: 8000
        nodePort: 30005
        protocol: TCP
    type: LoadBalancer
  livenessProbeSettings:
    livenessProbe:
      tcpSocket:
        port: 3000
      initialDelaySeconds: 120
      periodSeconds: 20
      failureThreshold: 5
  readinessProbeSettings:
    readinessProbe:
      tcpSocket:
        port: 3000
      initialDelaySeconds: 90
      periodSeconds: 10
      failureThreshold: 6


# River ML Training Service
# Handles incremental ML model training using River library
river:
  image: coelho-realtime-river:latest
  imagePullPolicy: IfNotPresent
  replicas: 1
  resources:
    requests:
      memory: "1Gi"
      cpu: "500m"
    limits:
      memory: "2Gi"
      cpu: "1000m"
  portsSettings:
    ports:
      - name: http
        port: 8002
        targetPort: 8002
        nodePort: 30006
        protocol: TCP
    type: LoadBalancer
  livenessProbeSettings:
    livenessProbe:
      httpGet:
        path: /health
        port: 8002
      initialDelaySeconds: 60
      periodSeconds: 30
      failureThreshold: 5
  readinessProbeSettings:
    readinessProbe:
      httpGet:
        path: /health
        port: 8002
      initialDelaySeconds: 30
      periodSeconds: 10
      failureThreshold: 3


# Redis configuration (Bitnami Redis chart)
# Used by Reflex for state management in production
redis:
  enabled: true
  architecture: standalone  # Use 'replication' for high availability
  auth:
    enabled: false  # Disable authentication for simplicity in local k3d
  master:
    persistence:
      enabled: true
      size: 2Gi
      storageClass: local-path
    resources:
      requests:
        memory: "256Mi"
        cpu: "100m"
      limits:
        memory: "512Mi"
        cpu: "250m"
  # Redis configuration for Reflex optimization
  commonConfiguration: |-
    # Enable AOF persistence for better durability
    appendonly yes
    # Disable RDB snapshots (we use AOF)
    save ""
    # Set max memory and eviction policy
    maxmemory 256mb
    maxmemory-policy allkeys-lru


# MinIO Object Storage (S3-compatible) - Official MinIO Chart (https://helm.min.io/)
# Used for storing parquet files and MLflow artifacts
# Data persists in /data/minio on host via K3d volume mount
minio:
  enabled: true
  # Deployment mode
  mode: standalone
  # Root credentials
  rootUser: "minioadmin"
  rootPassword: "minioadmin123"
  # MinIO API Service (S3 endpoint)
  service:
    type: NodePort
    port: "9000"
    nodePort: 30900
  # MinIO Console Service (Web UI)
  consoleService:
    type: NodePort
    port: "9001"
    nodePort: 30901
  # Persistence using pre-created PVC with hostPath (survives skaffold dev Ctrl+C)
  persistence:
    enabled: true
    existingClaim: "minio-hostpath-pvc"
  # Security context - run as root to avoid permission issues with hostPath
  securityContext:
    enabled: false
  containerSecurityContext:
    enabled: false
  # Resource limits
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      memory: 512Mi
  # Replicas (standalone mode = 1)
  replicas: 1
  # Default buckets to create on startup
  buckets:
    - name: mlflow-artifacts
      policy: none
      purge: false
  # Service account
  serviceAccount:
    create: true
    name: "minio-sa"
  # Disable ingress for local development
  ingress:
    enabled: false
  consoleIngress:
    enabled: false


# PostgreSQL Database (Bitnami PostgreSQL chart)
# Used by MLflow for metadata storage (experiments, runs, metrics, parameters)
# Also used by Grafana for dashboard/user storage (created via Grafana init container)
# Data persists in /data/postgresql on host via K3d volume mount
postgresql:
  enabled: true
  # Authentication - primary user for MLflow
  auth:
    postgresPassword: "postgres123"
    username: "mlflow"
    password: "mlflow123"
    database: "mlflow"
  # Use pre-created PVC with hostPath (survives skaffold dev Ctrl+C)
  primary:
    persistence:
      enabled: true
      existingClaim: "postgresql-hostpath-pvc"
    # Resource limits
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
  # Enable volumePermissions init container to fix hostPath ownership
  volumePermissions:
    enabled: true
  # Disable replication for local development
  architecture: standalone


# kube-prometheus-stack configuration (Prometheus + Grafana + Alertmanager)
# Provides comprehensive monitoring and observability for the cluster
# Port mappings changed to avoid conflicts:
#   - Grafana: 3000 → 3001 (Reflex uses 3000)
#   - Alertmanager: 9093 → 9094 (Kafka controller uses 9093)
# Integrations:
#   - Grafana → PostgreSQL (shared with MLflow)
kube-prometheus-stack:
  enabled: true

  # Prometheus Operator configuration
  prometheusOperator:
    enabled: true
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi

  # Prometheus Server configuration
  prometheus:
    enabled: true
    prometheusSpec:
      # Scrape interval and evaluation
      scrapeInterval: 30s
      evaluationInterval: 30s
      # Retention period (7 days for local dev)
      retention: 7d
      retentionSize: "5GB"
      # Resource limits for local k3d
      resources:
        requests:
          cpu: 200m
          memory: 512Mi
        limits:
          cpu: 1000m
          memory: 2Gi
      # Storage for metrics
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: local-path
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
      # Enable service discovery for all namespaces
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
    # Prometheus service configuration
    service:
      type: NodePort
      port: 9090
      targetPort: 9090
      nodePort: 30090

  # Grafana configuration
  grafana:
    enabled: true
    # Admin credentials
    adminUser: admin
    adminPassword: admin123
    # Service configuration - Port 3001 to avoid conflict with Reflex (3000)
    service:
      type: NodePort
      port: 3001
      targetPort: 3000  # Grafana internal port stays 3000
      nodePort: 30031
    # Resource limits
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 512Mi
    # Use PostgreSQL as database backend (shared with MLflow)
    persistence:
      enabled: false  # Disabled since we use PostgreSQL
    # Init container to create grafana database in PostgreSQL before Grafana starts
    extraInitContainers:
      - name: init-postgres-grafana-db
        image: bitnami/postgresql:latest
        command:
          - /bin/bash
          - -c
          - |
            echo "Waiting for PostgreSQL to be ready..."
            until PGPASSWORD=$POSTGRES_PASSWORD psql -h $PG_HOST -U postgres -c '\q' 2>/dev/null; do
              echo "PostgreSQL is unavailable - sleeping"
              sleep 2
            done
            echo "PostgreSQL is ready!"

            echo "Creating grafana user and database if not exists..."
            PGPASSWORD=$POSTGRES_PASSWORD psql -h $PG_HOST -U postgres <<-EOSQL
              DO \$\$
              BEGIN
                IF NOT EXISTS (SELECT FROM pg_catalog.pg_roles WHERE rolname = 'grafana') THEN
                  CREATE USER grafana WITH PASSWORD 'grafana123';
                END IF;
              END
              \$\$;

              SELECT 'CREATE DATABASE grafana OWNER grafana'
              WHERE NOT EXISTS (SELECT FROM pg_database WHERE datname = 'grafana')\gexec

              GRANT ALL PRIVILEGES ON DATABASE grafana TO grafana;
            EOSQL
            echo "Grafana database setup complete!"
        env:
          - name: PG_HOST
            value: "coelho-realtime-postgresql"
          - name: POSTGRES_PASSWORD
            value: "postgres123"
    # PostgreSQL database configuration for Grafana
    env:
      GF_DATABASE_TYPE: postgres
      GF_DATABASE_HOST: coelho-realtime-postgresql:5432
      GF_DATABASE_NAME: grafana
      GF_DATABASE_USER: grafana
      GF_DATABASE_PASSWORD: grafana123
      GF_DATABASE_SSL_MODE: disable
    # Default dashboards
    defaultDashboardsEnabled: true
    defaultDashboardsTimezone: browser
    # Sidecar for dashboard/datasource discovery
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
        labelValue: "1"
        searchNamespace: ALL
      datasources:
        enabled: true
        defaultDatasourceEnabled: true
    # Datasources provisioned directly (not via sidecar) to avoid timing issues
    datasources:
      datasources.yaml:
        apiVersion: 1
        datasources:
          - name: Prometheus
            type: prometheus
            uid: prometheus
            url: http://coelho-realtime-kube-prome-prometheus.coelho-realtime:9090/
            access: proxy
            isDefault: true
            jsonData:
              httpMethod: POST
              timeInterval: 30s
          - name: Alertmanager
            type: alertmanager
            uid: alertmanager
            url: http://coelho-realtime-kube-prome-alertmanager.coelho-realtime:9094/
            access: proxy
            jsonData:
              handleGrafanaManagedAlerts: false
              implementation: prometheus
    # Additional data sources
    additionalDataSources: []
    # Grafana.ini configuration
    grafana.ini:
      server:
        root_url: "%(protocol)s://%(domain)s:%(http_port)s/"
        serve_from_sub_path: false
      auth.anonymous:
        enabled: true
        org_role: Viewer

  # Alertmanager configuration
  alertmanager:
    enabled: true
    alertmanagerSpec:
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          cpu: 100m
          memory: 128Mi
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: local-path
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 1Gi
    # Service configuration - Port 9094 to avoid conflict with Kafka controller (9093)
    service:
      type: NodePort
      port: 9094
      targetPort: 9093  # Alertmanager internal port stays 9093
      nodePort: 30094

  # Node Exporter for node-level metrics
  nodeExporter:
    enabled: true
    resources:
      requests:
        cpu: 50m
        memory: 32Mi
      limits:
        cpu: 100m
        memory: 64Mi

  # kube-state-metrics for Kubernetes object metrics
  kubeStateMetrics:
    enabled: true

  # Disable components not needed for local k3d development
  kubeApiServer:
    enabled: true
  kubeControllerManager:
    enabled: false  # Not accessible in k3d
  kubeScheduler:
    enabled: false  # Not accessible in k3d
  kubeProxy:
    enabled: false  # k3d uses different networking
  kubeEtcd:
    enabled: false  # Not accessible in k3d

  # CoreDNS monitoring
  coreDns:
    enabled: true

  # Kubelet monitoring
  kubelet:
    enabled: true

  # Default rules for alerting
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: false  # Not accessible in k3d
      configReloaders: true
      general: true
      k8sContainerCpuUsageSecondsTotal: true
      k8sContainerMemoryCache: true
      k8sContainerMemoryRss: true
      k8sContainerMemorySwap: true
      k8sContainerResource: true
      k8sPodOwner: true
      kubeApiserverAvailability: true
      kubeApiserverBurnrate: true
      kubeApiserverHistogram: true
      kubeApiserverSlos: true
      kubeControllerManager: false  # Not accessible in k3d
      kubelet: true
      kubeProxy: false  # k3d uses different networking
      kubePrometheusGeneral: true
      kubePrometheusNodeRecording: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      kubeSchedulerAlerting: false  # Not accessible in k3d
      kubeSchedulerRecording: false  # Not accessible in k3d
      kubeStateMetrics: true
      network: true
      node: true
      nodeExporterAlerting: true
      nodeExporterRecording: true
      prometheus: true
      prometheusOperator: true
      windows: false
